{
    "status": "OK",
    "request_id": "40b0b6d0-d8d3-4fdc-9270-06a5ec8856d3",
    "parameters": {
        "query": "data engineer in dallas,tx",
        "page": 1,
        "num_pages": 10
    },
    "data": [
        {
            "employer_name": "TEKsystems",
            "employer_logo": "https://www.teksystems.com/-/media/teksystems_com/Images/Logos/TEKsystems_logotype_RGB.png",
            "employer_website": "http://www.teksystems.com",
            "employer_company_type": "Staffing",
            "job_publisher": "TEKsystems Careers",
            "job_id": "HmRFsCRF1UsAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Data Engineer",
            "job_apply_link": "https://careers.teksystems.com/ca/fr/job/JP-003930154/Data-Engineer",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.8267,
            "job_description": "We are looking for a Data Engineer to join our team!\n\nTop Skills' Details\n\n1) Experience writing stored procedures to ingest data from a spreadsheet (Excel) or SQL or other data sources\n\n2) Experience summarizing data and building those reporting tables and reviews, Tableaus and PowerBI. Experience building database views to summarize the data and pull the data. They mainly work with SQL databases so that is the main database they can work with but they do work with multiple databases so need to be able to come in and understand each database and the data that is in these databases. Writing queries or packages to query the data and summarize it. Building database views to summarize the data and pull the data correctly\n\n3) Experience looking over the multiple databases they work with, main one being SQL but looking into the databases and the data that is in each database and understanding that well\n\nDay to day:\n\n- Data modeling designs / custom development of data models so the data flows into Tableau and PowerBI correctly\n\n- Building database views to summarize the data and pull the data correctly\n\n- Making enhancements to the dashboards within Tableau and PowerBI\n\n- They will receive requests for another view or metrics to add to the table or the column so need to be able to take those requests and implement them.\n\n- Will be sourcing from Hadoop platform as well and that work will continue to grow so ideally have experience within Hadoop.\n\n- Working with APIs, ingesting data from APIs, writing scripts within this, Shell or Python scripts\n\nAbout TEKsystems:\n\nWe're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.\n\nThe company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690392102,
            "job_posted_at_datetime_utc": "2023-07-26T17:21:42.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=HmRFsCRF1UsAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience writing stored procedures to ingest data from a spreadsheet (Excel) or SQL or other data sources",
                    "Experience summarizing data and building those reporting tables and reviews, Tableaus and PowerBI",
                    "Experience looking over the multiple databases they work with, main one being SQL but looking into the databases and the data that is in each database and understanding that well",
                    "Working with APIs, ingesting data from APIs, writing scripts within this, Shell or Python scripts"
                ],
                "Responsibilities": [
                    "Experience building database views to summarize the data and pull the data",
                    "Writing queries or packages to query the data and summarize it",
                    "Data modeling designs / custom development of data models so the data flows into Tableau and PowerBI correctly",
                    "Making enhancements to the dashboards within Tableau and PowerBI",
                    "They will receive requests for another view or metrics to add to the table or the column so need to be able to take those requests and implement them",
                    "Will be sourcing from Hadoop platform as well and that work will continue to grow so ideally have experience within Hadoop"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "Other"
            ],
            "job_naics_code": "561311",
            "job_naics_name": "Employment Placement Agencies"
        },
        {
            "employer_name": "DISYS",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSsPrzieb2HrugmWoiAtj4wAAw4sYD5BVV8zr69&s=0",
            "employer_website": "http://disys.com",
            "employer_company_type": null,
            "job_publisher": "LinkedIn",
            "job_id": "Qsa1ierlUMkAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-disys-3673557460",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5856,
            "job_description": "Overview:\n\u2022 The team is looking for a versatile Data Engineer who will provide data and report development services or technical support.\n\u2022 You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages.\n\u2022 You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality.\n\u2022 As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members.\n\nRequirements:\n\u2022 An associate degree, a bachelor's degree in computer science or equivalent courses\n\u2022 At least 4 years of experience in Data Engineering with SQL, Python\n\u2022 Experience with relational SQL and NoSQL databases\n\u2022 Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\n\u2022 At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\n\u2022 At least 2 years of experience with AWS cloud (with focus on Data services)\n\nDesired Skills and Experience\n\nOverview:\n\u2022 The team is looking for a versatile Data Engineer who will provide data and report development services or technical support.\n\u2022 You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages.\n\u2022 You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality.\n\u2022 As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members.\n\nRequirements:\n\u2022 An associate degree, a bachelor's degree in computer science or equivalent courses\n\u2022 At least 4 years of experience in Data Engineering with SQL, Python\n\u2022 Experience with relational SQL and NoSQL databases\n\u2022 Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\n\u2022 At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\n\u2022 At least 2 years of experience with AWS cloud (with focus on Data services)\n\nDexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals.\n\nDexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit www.dexian.com to learn more.\n\nDexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690461639,
            "job_posted_at_datetime_utc": "2023-07-27T12:40:39.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Qsa1ierlUMkAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T12:40:39.000Z",
            "job_offer_expiration_timestamp": 1693053639,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 48,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "An associate degree, a bachelor's degree in computer science or equivalent courses",
                    "At least 4 years of experience in Data Engineering with SQL, Python",
                    "Experience with relational SQL and NoSQL databases",
                    "Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)",
                    "At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)",
                    "At least 2 years of experience with AWS cloud (with focus on Data services)"
                ],
                "Responsibilities": [
                    "The team is looking for a versatile Data Engineer who will provide data and report development services or technical support",
                    "You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages",
                    "You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality",
                    "As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Trigger IT LLC",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Dice",
            "job_id": "oAmKqPWYe9QAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.dice.com/job-detail/fbf4ceaa-f4b9-4873-bb34-2c1ed7b97228",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5684,
            "job_description": "Hi,\n\nHope you are doing well!\n\nJob Title: Data Engineer | (Python, Hive, Spark) | Hybrid | Dallas, TX\n\nLocation: Dallas, TX( look for Locals or Nearby)\n\nPrimary Skills: Tableau, Python, SQL\n\nDescription:\n\u2022 Designs, develops, and implements Hadoop eco-system based applications to support business requirements.\n\u2022 Follows approved life cycle methodologies, creates design documents, and performs program coding and testing.\n\u2022 Resolves technical issues through debugging, research, and investigation\n\nRequired Qualifications \u2013\n\nOption 1: Bachelor\u2019s degree in Computer Science and 4 years' experience in software engineering or related field.\n\nOption 2: 6 years\u2019 experience in software engineering or related field.\n\nOption 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nNice to have soft skills \u2013\n\u2022 7+ years of experience with 3+ years of Big data development experience\n\u2022 Experience in HDFS, Hive, Hive UDF\u2019s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.\n\u2022 Demonstrates expertise in writing complex, highly optimized queries across large data sets\n\u2022 Retail experience and knowledge of commercial data is a huge plus\n\u2022 Experience with BI Tool Tableau or Looker is a plus.\n\nTHANKS & REGARDS,\nCh.Nikhil Reddy\nDirect: +1 )\nTRIGGER IT | Getting it Done\nIT Services | Consulting-Development-Staffing\n5000 Centre Green Way, Suite 500 Cary, NC-27513, United States Web: | Email:",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690311787,
            "job_posted_at_datetime_utc": "2023-07-25T19:03:07.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=oAmKqPWYe9QAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-25T19:03:07.000Z",
            "job_offer_expiration_timestamp": 1692990187,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": [
                "Python",
                "Big data",
                "Tableau"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 40,
            "job_max_salary": 45,
            "job_salary_currency": "USD",
            "job_salary_period": "HOUR",
            "job_highlights": {
                "Qualifications": [
                    "Option 1: Bachelor\u2019s degree in Computer Science and 4 years' experience in software engineering or related field",
                    "Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field",
                    "3 years' experience in data engineering, database engineering, business intelligence, or business analytics",
                    "Nice to have soft skills \u2013",
                    "7+ years of experience with 3+ years of Big data development experience",
                    "Experience in HDFS, Hive, Hive UDF\u2019s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix",
                    "Demonstrates expertise in writing complex, highly optimized queries across large data sets",
                    "Retail experience and knowledge of commercial data is a huge plus"
                ],
                "Responsibilities": [
                    "Designs, develops, and implements Hadoop eco-system based applications to support business requirements",
                    "Follows approved life cycle methodologies, creates design documents, and performs program coding and testing",
                    "Resolves technical issues through debugging, research, and investigation"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Skiltrek",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "ZipRecruiter",
            "job_id": "P7IfQYjEDhsAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "data engineer",
            "job_apply_link": "https://www.ziprecruiter.com/c/Skiltrek/Job/Data-Engineer/-in-Dallas,TX?jid=80b3c66c3cb0a5ed",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.629,
            "job_description": "job summary:\nRequired Qualifications -\n\u2022 Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field.\n\u2022 Option 2: 6 years' experience in software engineering or related field.\n\u2022 Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics.\nNice to have soft skills -\n\u2022 7+ years of experience with 3+ years of Big data development experience\n\u2022 Experience in HDFS, Hive, Hive UDF's, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.\n\u2022 Demonstrates expertise in writing complex, highly optimized queries across large data sets\n\u2022 Retail experience and knowledge of commercial data is a huge plus\n\u2022 Experience with BI Tool Tableau or Looker is a plus\n\nlocation: Dallas, Texas\njob type: Contract\nsalary: $62 - 72 per hour\nwork hours: 8am to 5pm\neducation: Bachelors\n\nresponsibilities:\nDesigns, develops, and implements Hadoop eco-system based applications to support business requirements.\n\nFollows approved life cycle methodologies, creates design documents, and performs program coding and testing.\n\nResolves technical issues through debugging, research, and investigation\n\nqualifications:\n\u2022 Experience level: Experienced\n\u2022 Minimum 4 years of experience\n\u2022 Education: Bachelors\n\nskills:\n\u2022 SQL\n\u2022 Python\n\u2022 Tableau\nAbout Us\n\nSkiltrek is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US.\nAt Skiltrek, we promise you the perfect opportunity of building technical excellence, understand business performance and nuances,\nbe abreast with the latest happenings in technology world and enjoy a satisfying work life balance.\nSkiltrek is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender,\nrace, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law.\nSkiltrek is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690277800,
            "job_posted_at_datetime_utc": "2023-07-25T09:36:40.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=P7IfQYjEDhsAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-27T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693094400,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field",
                    "Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field",
                    "3 years' experience in data engineering, database engineering, business intelligence, or business analytics",
                    "Nice to have soft skills -",
                    "7+ years of experience with 3+ years of Big data development experience",
                    "Experience in HDFS, Hive, Hive UDF's, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix",
                    "Demonstrates expertise in writing complex, highly optimized queries across large data sets",
                    "Retail experience and knowledge of commercial data is a huge plus",
                    "Experience level: Experienced",
                    "Minimum 4 years of experience",
                    "Education: Bachelors",
                    "SQL"
                ],
                "Responsibilities": [
                    "work hours: 8am to 5pm",
                    "Designs, develops, and implements Hadoop eco-system based applications to support business requirements",
                    "Follows approved life cycle methodologies, creates design documents, and performs program coding and testing",
                    "Resolves technical issues through debugging, research, and investigation"
                ],
                "Benefits": [
                    "salary: $62 - 72 per hour"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-2041.00: Statisticians"
            ]
        },
        {
            "employer_name": "Softworld, a Kelly Company",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "WAVY Jobs",
            "job_id": "QwDdBZfCBAUAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://jobs.wavy.com/jobs/data-engineer-dallas-texas/1074871948-2/",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5741,
            "job_description": "Job Number: 240856\n\nData Engineer\n\nJob Description\n\nThis role will be part of a team focused on cloud transformation, modernizing analytics platforms and improving agility. The role requires hands-on experience in building and managing analytics solutions in SnowFlake, Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of Data Engineering architecture and Development.\n\nPrimary duties and responsibilities:\n\n- Establish Data Engineering architecture strategy, best practices, standards, and roadmap\n\n- Experience developing ETL Pipeline using Python, Snowflake and IDMC.\n\n- Experience with loading batch data and streaming data via Kafka\n\n- Build Data Flows mapping Source systems and Process flows.\n\n- Assemble large, complex data sets that meet non-functional and functional business requirements\n\n- Mentor team members on best practices, efficient implementations and delivering high quality data products\n\n- Lead onshore and offshore teams\n\n- Perform code reviews and assist developers in optimization and troubleshooting.\n\n- Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features\n\n- Knowledge in AWS and management technologies such as S3.\n\n- Strong written communication skills\n\n- Is effective and persuasive in both written and oral communication\n\nMust Haves:\n\n- 5-10 years of experience within data engineering\n\n- Python experience\n\n- Snowflake experience (developing ETL pipelines)\n\n- IDMC (intelligent data management cloud) experience\n\n- Knowledge of AWS\n\nPlusses:\n\n- Kafka experience (loading batch and streaming data)\n\u2022 Hybrid position - 2 days a week on-site at client office in downtown Dallas, TX\n\nDesired Skills and Experience\n\nData Engineer, Python, IDMC, Snowflake, AWS, Dallas, ETL, ETL Pipeline, Kafka, RBAC controls, S3, ETL, AWS, Snowflake, IDMC, intelligent data management cloud, data engineer, hybrid",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690431118,
            "job_posted_at_datetime_utc": "2023-07-27T04:11:58.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=QwDdBZfCBAUAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Knowledge in AWS and management technologies such as S3",
                    "Strong written communication skills",
                    "5-10 years of experience within data engineering",
                    "Python experience",
                    "Snowflake experience (developing ETL pipelines)",
                    "Kafka experience (loading batch and streaming data)",
                    "Hybrid position - 2 days a week on-site at client office in downtown Dallas, TX",
                    "Data Engineer, Python, IDMC, Snowflake, AWS, Dallas, ETL, ETL Pipeline, Kafka, RBAC controls, S3, ETL, AWS, Snowflake, IDMC, intelligent data management cloud, data engineer, hybrid"
                ],
                "Responsibilities": [
                    "This role will be part of a team focused on cloud transformation, modernizing analytics platforms and improving agility",
                    "The role requires hands-on experience in building and managing analytics solutions in SnowFlake, Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of Data Engineering architecture and Development",
                    "Establish Data Engineering architecture strategy, best practices, standards, and roadmap",
                    "Experience developing ETL Pipeline using Python, Snowflake and IDMC",
                    "Experience with loading batch data and streaming data via Kafka",
                    "Build Data Flows mapping Source systems and Process flows",
                    "Assemble large, complex data sets that meet non-functional and functional business requirements",
                    "Mentor team members on best practices, efficient implementations and delivering high quality data products",
                    "Lead onshore and offshore teams",
                    "Perform code reviews and assist developers in optimization and troubleshooting",
                    "Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Compunnel Inc.",
            "employer_logo": "https://images.comparably.com/companies/compunnel-software-group",
            "employer_website": "http://www.compunnel.com",
            "employer_company_type": null,
            "job_publisher": "LinkedIn",
            "job_id": "v6KgOCdJjGgAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-compunnel-inc-3677796668",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5598,
            "job_description": "Data Engineer (W2 only)\n\nThe Expertise You Have\n\u2022 Bachelor\u2019s degree required and 5+ years of related experience in Data Engineering.\n\u2022 Solid experience in writing complex SQL queries on Snowflake (AWS) or Oracle and performance optimization for large data volumes.\n\u2022 Proficiency working with Python specifically as it relates to data processing.\n\u2022 Proven technical leadership in definition/support of standards and best practices\n\u2022 Superior Data Modeling skills and experience performing deep data analysis on multiple database platforms\n\u2022 Deep understanding of data warehousing techniques and methodologies.\n\u2022 Exposure and/or understanding Client\u2019s enterprise data lake strategy.\n\u2022 Experience in Linux commands and basic shell scripting.\n\u2022 Experience in working with devops tools for code migrations.\n\u2022 Ability to develop ELT/ETL pipelines to move data to and from Snowflake data store using combination of Python and Snowflake SnowSQL.\n\u2022 Proven track record of working in collaborative teams to deliver high quality data solutions in a multi-developer agile environment following design & coding best practices.\n\u2022 Excellent communicator to both technical and non-technical data players to ensure common understanding of design to streamline and optimize data enablement.\n\u2022 Prior experience working in Agile software development environments, with proven ability to convert user stories into delivery work that provides incremental, iterative delivery of business value.\n\u2022 Financial services experience preferred.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690474929,
            "job_posted_at_datetime_utc": "2023-07-27T16:22:09.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=v6KgOCdJjGgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T16:22:09.000Z",
            "job_offer_expiration_timestamp": 1693066929,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor\u2019s degree required and 5+ years of related experience in Data Engineering",
                    "Solid experience in writing complex SQL queries on Snowflake (AWS) or Oracle and performance optimization for large data volumes",
                    "Proficiency working with Python specifically as it relates to data processing",
                    "Proven technical leadership in definition/support of standards and best practices",
                    "Superior Data Modeling skills and experience performing deep data analysis on multiple database platforms",
                    "Deep understanding of data warehousing techniques and methodologies",
                    "Exposure and/or understanding Client\u2019s enterprise data lake strategy",
                    "Experience in Linux commands and basic shell scripting",
                    "Experience in working with devops tools for code migrations",
                    "Ability to develop ELT/ETL pipelines to move data to and from Snowflake data store using combination of Python and Snowflake SnowSQL",
                    "Proven track record of working in collaborative teams to deliver high quality data solutions in a multi-developer agile environment following design & coding best practices",
                    "Excellent communicator to both technical and non-technical data players to ensure common understanding of design to streamline and optimize data enablement",
                    "Prior experience working in Agile software development environments, with proven ability to convert user stories into delivery work that provides incremental, iterative delivery of business value"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Ascendion Inc.",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS500weGU8Dx9ZPZosQM_sMearUR_pwAnRmX7Ms&s=0",
            "employer_website": "http://www.collabera.com",
            "employer_company_type": null,
            "job_publisher": "Professional Diversity Network",
            "job_id": "gbf_tKAAwsoAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.prodivnet.com/job/data-engineer-dallas-texas-13317454",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5781,
            "job_description": "About Ascendion\n\nAscendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next. Ascendion | Engineering to elevate life\n\nWe have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:\n\u2022 Build the coolest tech for world's leading brands\n\u2022 Solve complex problems - and learn new skills\n\u2022 Experience the power of transforming digital engineering for Fortune 500 clients\n\u2022 Master your craft with leading training programs and hands-on experience\n\nExperience a community of change makers!\n\nJoin a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.\n\nAbout the Role: We are looking for an experienced Data Engineer with 6+ years of experience with Snowflake and Informatica cloud. The data engineer should be somewhere between an engineer and an architect.\n\nJob Title: Data Engineer\n\nLocation: Dallas, TX(Hybrid)\n\nEducation:\n\nBachelor's degree/University degree or equivalent experience\n\nResponsibilities:\n\nDay-To-Day:\n\u2022 Manage & deliver solutions by liaising with business users, architects and understanding the business requirements\n\u2022 Help design and develop technical and business solutions\n\u2022 Should be able to coordinate & work with offshore developers\n\u2022 Develop technical solutions which may involve a variety of data development tools, preferably SQL Server or Informatica\n\u2022 Design, develop, test and support from SQL stored procedures, views, Informatica workflows, Control-M flows or script objects\n\u2022 Executes unit and system test scripts, analyzes, captures and publishes test results\n\u2022 Mentors junior staff members in all areas of database engineering. Guide team members in setting expectations and monitor their progress\n\u2022 Identifies opportunities to create efficiencies in technical work streams as well as in operational procedures of team\n\nMust haves:\n\u2022 Experience in Informatica cloud, SQL Server\n\u2022 Snowflake, creating pipes\n\u2022 SQL & stored procedure development in MS SQL Server or Oracle\n\nPlusses:\n\u2022 Implementation experience in scripting/coding for automation (Windows Batch, Unix Shell, PowerShell Python or similar scripting languages)\n\u2022 PowerShell, Python scripting\n\u2022 Big Data technologies\n\nSalary Range: The salary for this position is between $1,28,900- $1,41,400 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience and other qualifications of the successful candidate.\n\nThis position is eligible for commissions in accordance with the terms of the Company's plan. Commissions for this position are estimated to be based on individual performance. Additionally, this role is also eligible for bonus based on achievement of mutually agreed KRAs.\n\nBenefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [12-15 days (about 2 weeks) of paid vacation time] [6-8 weeks (about 2 months) of paid parental leave after a year of service] [9 paid holidays and 2 floating holidays per calendar year] [Ascendion Learning Management System] [Tuition Reimbursement Program]\n\nWant to change the world? Let us know.\n\nTell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let's talk!\n\nPDN-99bc1c43-05af-4728-8e2f-a41a46b908d3",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690352408,
            "job_posted_at_datetime_utc": "2023-07-26T06:20:08.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "paid_time_off",
                "dental_coverage",
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=gbf_tKAAwsoAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-10-24T06:20:08.000Z",
            "job_offer_expiration_timestamp": 1698128408,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "About the Role: We are looking for an experienced Data Engineer with 6+ years of experience with Snowflake and Informatica cloud",
                    "The data engineer should be somewhere between an engineer and an architect",
                    "Bachelor's degree/University degree or equivalent experience",
                    "Experience in Informatica cloud, SQL Server",
                    "Snowflake, creating pipes",
                    "SQL & stored procedure development in MS SQL Server or Oracle",
                    "Implementation experience in scripting/coding for automation (Windows Batch, Unix Shell, PowerShell Python or similar scripting languages)",
                    "Bring your knowledge, unique viewpoint, and creativity to the table"
                ],
                "Responsibilities": [
                    "Build the coolest tech for world's leading brands",
                    "Solve complex problems - and learn new skills",
                    "Experience the power of transforming digital engineering for Fortune 500 clients",
                    "Master your craft with leading training programs and hands-on experience",
                    "Manage & deliver solutions by liaising with business users, architects and understanding the business requirements",
                    "Help design and develop technical and business solutions",
                    "Should be able to coordinate & work with offshore developers",
                    "Design, develop, test and support from SQL stored procedures, views, Informatica workflows, Control-M flows or script objects",
                    "Executes unit and system test scripts, analyzes, captures and publishes test results",
                    "Mentors junior staff members in all areas of database engineering",
                    "Guide team members in setting expectations and monitor their progress",
                    "Identifies opportunities to create efficiencies in technical work streams as well as in operational procedures of team"
                ],
                "Benefits": [
                    "Salary Range: The salary for this position is between $1,28,900- $1,41,400 annually",
                    "Factors which may affect pay within this range may include geography/market, skills, education, experience and other qualifications of the successful candidate",
                    "This position is eligible for commissions in accordance with the terms of the Company's plan",
                    "Additionally, this role is also eligible for bonus based on achievement of mutually agreed KRAs",
                    "Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [personal days accrued each calendar year",
                    "The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [12-15 days (about 2 weeks) of paid vacation time] [6-8 weeks (about 2 months) of paid parental leave after a year of service] [9 paid holidays and 2 floating holidays per calendar year] [Ascendion Learning Management System] [Tuition Reimbursement Program]"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Cloud BC Labs",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTGe7j9FVPJr-tQbbE7BvSD45V4WMr__2bM7Hd-&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "LinkedIn",
            "job_id": "NgXphZWo5UwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-cloud-bc-labs-3676042142",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5773,
            "job_description": "Job Description\n\nMust Have Skills\n\nDatabricks, Scala, Python\n\nCloud BC Labs Inc is a digital transformation organization aimed at creating seamless solutions for clients to effectively manage their business operations. The company specializes in Business and Management Consulting, AI/ML, Data Analytics & Visualization, Cloud Data Warehouse Migration, Snowflake Implementation, Informatica Implementation & Upgrade, Staffing Services and Data Management Solutions",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416391,
            "job_posted_at_datetime_utc": "2023-07-27T00:06:31.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=NgXphZWo5UwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:06:31.000Z",
            "job_offer_expiration_timestamp": 1693008391,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {},
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Stefanini",
            "employer_logo": "https://d1yjjnpx0p53s8.cloudfront.net/styles/logo-thumbnail/s3/0020/4960/brand.gif?itok=3Cs8keFV",
            "employer_website": "http://stefanini.com",
            "employer_company_type": "Computer Services",
            "job_publisher": "Salary.com",
            "job_id": "ICjkXTgkKKgAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.salary.com/job/stefanini/data-engineer/j202307260256561284218",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5621,
            "job_description": "Stefanini is looking for a Data Engineer in Dallas, TX (hybrid role). W2 only. US Citizens only\nFor quick apply, please reach out to Vishal Sharma- Vishal.sharma@stefanini.com / 248.263.5616\n\nThe team is is looking for a versatile Data Engineer who will provide data and report development services or technical support. You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages. You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality. As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members.\n\nResponsibilities:\n\u2022 Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns\n\u2022 Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity\n\u2022 Work under general guidance and clear framework of accountability with substantial autonomy\n\u2022 Use best practices and knowledge of internal or external business issues to improve products or services\n\u2022 Solve complex problems; takes a new perspective using existing solutions\n\u2022 Required Skills:\n\u2022 An associate degree, a bachelor's degree in computer science or equivalent courses\n\u2022 At least 4 years of experience in Data Engineering with SQL, Python\n\u2022 Experience with relational SQL and NoSQL databases\n\u2022 Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\n\u2022 At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\n\u2022 At least 2 years of experience with AWS cloud (with focus on Data services)\n\u2022 Experience with Databricks a plus\n\u2022 Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs\n\u2022 Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix/Linux shell scripting\n\u2022 Able to interpret user requirements and identify additional information needed in user requirements\n\u2022 Able to see effects of current design with future requirements and see possible coding solutions to meet the requirements\n\u2022 Detailed understanding of logical and physical data structures\n\u2022 Highly skilled in tools, evaluates the need for various tools for continuous integration, testing, automation, deployment etc. and discuss with the team\n\u2022 Highly skilled at designing tests for unfamiliar designs; Evaluates tests for weaknesses and continuously improves them\n\u2022 Detailed understanding of how to effectively test against multiple tools/software\n\u2022 Equivalent education and/or experience may be substituted for any of the above requirements\n\u2022 ***Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives.\n\nStefanini takes pride in hiring top talent and developing relationships with our future employees. Our talent acquisition teams will never make an offer of employment without having a phone conversation with you. Those face-to-face conversations will involve a description of the job for which you have applied. We also speak with you about the process including interviews and job offers.\n\nAbout the Company:\nStefanini\n\nStefanini is a global IT services company with over 24,000 employees across 77 offices in 40 countries across the Americas, Europe, Africa, Australia, and Asia. Since 1987, Stefanini has been providing offshore, onshore and nearshore IT services, including application development and outsourcing services, IT infrastructure outsourcing (help desk support and desktop services), systems integration, consulting and strategic staffing to Fortune 1000 enterprises around the world.\n\nWith a base of over 500 active clients, including more than 300 multinationals, Stefanini maintains a strong presence in industries such as financial services, manufacturing, telecommunications, chemical, services, technology, public sector, and utilities. Clients benefit from Stefanini's financial stability, sustained year-over-year growth, and zero net debt. The corporate global headquarters is located in Sao Paulo, Brazil with European headquarters in Brussels and North American headquarters in metropolitan Detroit.\n\nCompany Size:\n10,000 employees or more\n\nIndustry:\nBanking\n\nFounded:\n1987\n\nWebsite:\nhttp://www.stefanini.com",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416000,
            "job_posted_at_datetime_utc": "2023-07-27T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ICjkXTgkKKgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-24T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1692835200,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 48,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "An associate degree, a bachelor's degree in computer science or equivalent courses",
                    "At least 4 years of experience in Data Engineering with SQL, Python",
                    "Experience with relational SQL and NoSQL databases",
                    "Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)",
                    "At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)",
                    "At least 2 years of experience with AWS cloud (with focus on Data services)",
                    "Equivalent education and/or experience may be substituted for any of the above requirements"
                ],
                "Responsibilities": [
                    "The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support",
                    "You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages",
                    "You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality",
                    "As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members",
                    "Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns",
                    "Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity",
                    "Work under general guidance and clear framework of accountability with substantial autonomy",
                    "Use best practices and knowledge of internal or external business issues to improve products or services",
                    "Solve complex problems; takes a new perspective using existing solutions",
                    "Able to interpret user requirements and identify additional information needed in user requirements",
                    "Able to see effects of current design with future requirements and see possible coding solutions to meet the requirements",
                    "Detailed understanding of logical and physical data structures",
                    "and discuss with the team",
                    "Highly skilled at designing tests for unfamiliar designs; Evaluates tests for weaknesses and continuously improves them"
                ],
                "Benefits": [
                    "Also, some positions may include bonuses or other incentives"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541512",
            "job_naics_name": "Computer Systems Design Services"
        },
        {
            "employer_name": "Disney Media & Entertainment Distribution",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Disney_Media_and_Entertainment_Distribution_logo.svg/1200px-Disney_Media_and_Entertainment_Distribution_logo.svg.png",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "JobServe",
            "job_id": "HCShKDrrNT8AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Lead Data Engineer",
            "job_apply_link": "https://www.jobserve.com/us/en/extjob/LEAD-DATA-ENGINEER-in-Dallas-Texas-USA-9B3F79DD177BEEF34C/",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5575,
            "job_description": "Job Overview\n\nOur Data and Analytics team for Disney Streaming Services (DSS), a segment under the Disney Entertainment & ESPN Technology (DE&ET) is looking for a Lead Data Engineer. Data is essential for all our decision-making needs whether it's related to product design, measuring advertising effectiveness, helping users discover new content or building new businesses in emerging markets. This data is deeply valuable and gives us insights into how we can continue improving our service for our users, advertisers and our content partners. Our Engagement and Retention Data Engineering team is seeking a highly motivated Data Engineer with a strong technical background and passionate about diving deeper into Big Data to develop state of the art Data Solutions. What You'll Do\n\u2022 Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science\n\u2022 Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)\n\u2022 Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala\n\u2022 Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts\n\u2022 Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions\n\u2022 Maintain detailed documentation of your work and changes to support data quality and data governance\n\u2022 Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)\n\u2022 Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team Key Qualifications\n\u2022 At least 7 years of data engineering experience developing large data pipelines\n\u2022 Strong SQL skills and ability to create queries to extract data and build performant datasets\n\u2022 Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data\n\u2022 Strong programming skills in Python\n\u2022 Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)\n\u2022 Solid experience with data integration toolsets (ie Airflow) and writing and maintaining Data Pipelines\n\u2022 Strong in Data Modeling techniques and Data Warehousing standard methodologies and practices\n\u2022 Familiar with Scrum and Agile methodologies\n\u2022 You are a problem solver with strong attention to detail and excellent analytical and communication skills\n\u2022 Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2) Required Education\n\u2022 Bachelor's or Master's Degree in Computer Science, Information Systems or related field Additional Information\n\nThe hiring range for this position in California is $149,240 - $200,200 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate's geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416120,
            "job_posted_at_datetime_utc": "2023-07-27T00:02:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=HCShKDrrNT8AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:02:00.000Z",
            "job_offer_expiration_timestamp": 1693008120,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 149240,
            "job_max_salary": 200200,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "At least 7 years of data engineering experience developing large data pipelines",
                    "Strong SQL skills and ability to create queries to extract data and build performant datasets",
                    "Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data",
                    "Strong programming skills in Python",
                    "Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)",
                    "Solid experience with data integration toolsets (ie Airflow) and writing and maintaining Data Pipelines",
                    "Strong in Data Modeling techniques and Data Warehousing standard methodologies and practices",
                    "Familiar with Scrum and Agile methodologies",
                    "You are a problem solver with strong attention to detail and excellent analytical and communication skills",
                    "Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2) Required Education",
                    "Bachelor's or Master's Degree in Computer Science, Information Systems or related field Additional Information"
                ],
                "Responsibilities": [
                    "Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science",
                    "Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)",
                    "Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala",
                    "Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts",
                    "Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions",
                    "Maintain detailed documentation of your work and changes to support data quality and data governance",
                    "Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)",
                    "Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team Key Qualifications"
                ],
                "Benefits": [
                    "The base pay actually offered will take into account internal equity and also may vary depending on the candidate's geographic region, job-related knowledge, skills, and experience among other factors",
                    "A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Stark Dev, LLC",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT5BSMIt7m2uiId_3142LNElhU-kvf9h48rIEOM&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Trabajo.org",
            "job_id": "aV5T0382CTgAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Sr. Azure Data Engineer/Dallas,TX/ W2",
            "job_apply_link": "https://us.trabajo.org/job-1275-20230724-aa3f02cd58ce867110e1b6bcb7663628",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.3783,
            "job_description": "This position is open for *United States Citizens/ GC/GC EAD/H4 EAD/TN VISA HOLDERS.*\n\u2022 CONTRACT with the candidate directly with an extension option after initial 6 months of employment.*\n\nJob Title\n\nData Devops engineer\n\nSKILLS:\n\nAzure DevOps\n\nCI/CD pipeline\n\nAzure SQL\n\nAzure Synapse\n\nETL\n\nWith your expertise in SQL Server databases, Azure SQL databases, ETL processes, automation, and hybrid cloud architectures, you will play a pivotal role in our organization's technical progress. As a Senior Data/DevOps Engineer, your primary responsibility will be providing robust and reliable database administration support for a diverse range of environments, including on-prem SQL Servers and Azure SQL Databases. Alongside the hands-on technical work, we are looking for a professional who can drive process enhancements, implement automation to improve efficiency, and manage the entire life cycle of ETL solutions for both internal and external systems. This position requires a strong data analytics mindset to design, develop, and maintain our data analytics solutions.\n\nJob Type: Contract\n\nPay: From $65.00 per hour\n\nCompensation package:\n\u2022 Hourly pay\n\nExperience level:\n\u2022 4 years\n\u2022 5 years\n\nSchedule:\n\u2022 Monday to Friday\n\nAbility to commute/relocate:\n\u2022 Dallas, TX: Reliably commute or planning to relocate before starting work (Required)\n\nExperience:\n\u2022 CI/CD: 3 years (Required)\n\u2022 Azure DevOps: 4 years (Required)\n\u2022 Azure SQL: 4 years (Required)\n\u2022 Azure Synapse: 3 years (Required)\n\nWork Location: In person",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690175599,
            "job_posted_at_datetime_utc": "2023-07-24T05:13:19.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=aV5T0382CTgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-07-31T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1690761600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "5 years",
                    "Dallas, TX: Reliably commute or planning to relocate before starting work (Required)",
                    "CI/CD: 3 years (Required)",
                    "Azure DevOps: 4 years (Required)",
                    "Azure Synapse: 3 years (Required)"
                ],
                "Benefits": [
                    "CONTRACT with the candidate directly with an extension option after initial 6 months of employment.*",
                    "Pay: From $65.00 per hour",
                    "Compensation package:",
                    "Monday to Friday"
                ]
            },
            "job_job_title": null,
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Alto Pharmacy",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "ZipRecruiter",
            "job_id": "uTkg0mApiRgAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer",
            "job_apply_link": "https://www.ziprecruiter.com/c/Alto-Pharmacy/Job/Senior-Data-Engineer/-in-Dallas,TX?jid=a78db4a3205d1126",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.6175,
            "job_description": "Alto is America's leading digital pharmacy, transforming a $500 billion industry. Founded in 2015, Alto's better pharmacy model is centered on the critical role of pharmacists as the final link in a person's health journey. Alto combines expert pharmacist care with purpose-built technology to deliver a more convenient and affordable experience for those who need medication. To date, Alto has fulfilled more than three million prescriptions, expanded to twelve markets, and built a mobile app experience that makes it easier than ever to manage medications and chat with a pharmacist. As Alto continues its rapid growth, it remains customer obsessed, with an industry-leading NPS score of 86.\n\nAbout the Role\n\nThe Alto Data Engineering team is responsible for several key business areas:\n\u2022 ETL, data visualization, and metadata platforms (fivetran, snowflake, dbt, looker, datahub) that power business analytics and decision making\n\u2022 ML platform (kubeflow) powering the models that automate and optimize pharmacy operations\n\u2022 Data integrations with healthcare networks, drug manufacturers, and other partnerships\n\nOur goal is to make high quality data accessible to everyone at Alto to accelerate decision making and improve our products. We're looking for an experienced, customer minded engineer with a strong sense of ownership to join our team.\n\nAccelerate Your Career as You\n\u2022 Build a world class self service data platform that makes it easy to quickly answer business questions, trace lineage, and monitor data accuracy and latency\n\u2022 Scale out our machine learning platform and collaborate with our Data Science team to integrate ML/AI applications with our pharmacy operations platform\n\u2022 Identify core problems we can solve for our customer teams and build products that can support and scale data at Alto\n\nA Bit About You\n\nMinimum Qualifications:\n\u2022 7+ years of production data engineering experience\n\u2022 Strong technical skills in python, SQL, and data modeling\n\u2022 Experience with data warehouses, E(LT/TL) tools, and cloud services\n\u2022 Strong sense of ownership over your work\n\u2022 Comfortable working at startup pace and focus\n\nPreferred Qualifications:\n\u2022 Demonstrated experience improving data governance, accuracy, and latency\n\u2022 Familiarity defining and implementing security and data access policies\n\u2022 Experience and opinions on how to best leverage our core technologies\n\u2022 AWS expertise\n\nAdditional Physical Job Requirements\n\u2022 Read English, comprehend, and follow simple oral and written instructions. The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading. Assessing the accuracy, neatness and thoroughness of the work assigned.\n\u2022 Communicating with others to exchange information. Expressing or exchanging ideas by means of the spoken word; those activities where detailed or important spoken instructions must be conveyed to other workers accurately, loudly, or quickly.\n\u2022 Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound.\n\u2022 Frequent repeating motions required to operate a computer that may include the wrists, hands and/or fingers.\n\u2022 Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body. Walking & standing are required occasionally.\n\nSalary and Benefits\n\nSalary Range: $159,000 - $199,000\n\nCommission Eligible: No\n\nEquity Eligible: Yes\n\nTravel: No\n\nBenefits: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD/LTD, generous PTO and parental leave.\n\n#LI-Remote\n\nAlto Pharmacy is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.\n\nPursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.",
            "job_is_remote": true,
            "job_posted_at_timestamp": 1689934451,
            "job_posted_at_datetime_utc": "2023-07-21T10:14:11.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "dental_coverage",
                "paid_time_off",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=uTkg0mApiRgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693008000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "7+ years of production data engineering experience",
                    "Strong technical skills in python, SQL, and data modeling",
                    "Experience with data warehouses, E(LT/TL) tools, and cloud services",
                    "Strong sense of ownership over your work",
                    "Comfortable working at startup pace and focus",
                    "Read English, comprehend, and follow simple oral and written instructions",
                    "The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading",
                    "Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body",
                    "Walking & standing are required occasionally"
                ],
                "Responsibilities": [
                    "ETL, data visualization, and metadata platforms (fivetran, snowflake, dbt, looker, datahub) that power business analytics and decision making",
                    "ML platform (kubeflow) powering the models that automate and optimize pharmacy operations",
                    "Data integrations with healthcare networks, drug manufacturers, and other partnerships",
                    "Assessing the accuracy, neatness and thoroughness of the work assigned",
                    "Communicating with others to exchange information",
                    "Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound",
                    "Frequent repeating motions required to operate a computer that may include the wrists, hands and/or fingers"
                ],
                "Benefits": [
                    "Salary and Benefits",
                    "Salary Range: $159,000 - $199,000",
                    "Commission Eligible: No",
                    "Benefits: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD/LTD, generous PTO and parental leave"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06: Database Architects"
            ]
        },
        {
            "employer_name": "Jobot",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNwo2ypQevbabADKl8PjDknSLh_Ru7W8jKoYoG&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Dice.com",
            "job_id": "eMdLLHNlwWsAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Azure Data Engineer",
            "job_apply_link": "https://www.dice.com/job-detail/1fd6150a-43b5-46d8-8217-ee965a24970a",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6698,
            "job_description": "Great collaborative company that is focused on utilizing and investing in the latest and greatest technology!\n\nThis Jobot Job is hosted by: Jasmine Robinson\nAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.\nSalary: $125,000 - $150,000 per year\n\nA bit about us:\n\nWe focus on simplifying business transformation. We apply thought leadership and innovation to bring our customer's digital agenda to reality. We partner with customers in their journey from vision to adoption, and across the plethora of technology options available today.\n\nWhy join us?\n\nWe offer a flexible work schedule, collaborative environment, and ability to work with the latest and greatest technology!\n\nJob Details\n\nWe are seeking an experienced Senior Data Engineer who will work with clients and members of the consulting team on the architecture, design, and development of highly scalable data integration and data engineering processes. The Senior Consultant must have a strong understanding and experience with data & analytics solution architecture, including data warehousing, data lakes, ETL/ELT workload patterns, and related BI & analytics systems.\n\nAdditional responsibilities of this role will include the following:\n\nDeliver consulting projects/work on-time, on-budget, and in a way that accomplishes client goals\nDevelop and implement technical best practices for data ingestion, data quality, data cleansing, and other data integration/ETL/Engineering-related activities\nUnderstand and experience maintaining a multi-terabyte enterprise data warehouse with accompanying incremental data pipelines\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and modern cloud technologies.\nConduct or participate in meetings with owners of key system components to fully understand current data and systems environments\nResolve source data issues and refine transformation rules\nAnalyze source system data to assess transformation logic and data quality through data profiling\nLeverage data quality processes to assist with data cleansing requirements\nWork with technical and business representatives to determine strategies for handling data anomalies that are identified\nDesign ETL processes and develop source-to-target data mappings, integration workflows, and load processes\nDevelop, test, integrate, and deploy data pipelines using a variety of tools and external programming/scripting languages as necessary\nProvide technical documentation and other artifacts for data pipelines, ingestion, integration or other data solutions\nIdentify problems, develop ideas and propose solutions within differing situations requiring analytical, evaluative or constructive thinking in daily work\nApply creative thinking to identify possible reporting solution alternatives\nOther duties assigned as needed\nRequirements and Qualifications\n\n3+ years hands-on experience with one or more of these data integration/ETL tools:\nAzure Data Factory\nDatabricks/Spark\nExperience building on-prem data warehousing solutions\nExperience with designing and developing ETL's, Data Marts, Star Schema's\nExperience with building data warehousing solutions in Azure\nMoving data from on-prem to cloud\nDesigning a data warehouse solution using Synapse or Azure SQL DB\nExperience building pipelines using Synapse or Azure Data Factory to ingest data from various sources\nUnderstanding of integration run times available in Azure\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases\nKnowledge of scripting languages like Python, Scala.\nMicrosoft Azure Cloud platform certifications (nice to have)\nMust be able to travel to client locations based on project needs\n\nThis position is contract-to-hire. You will be eligible for full-time benefits during the contract period.\n\nInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689423709,
            "job_posted_at_datetime_utc": "2023-07-15T12:21:49.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=eMdLLHNlwWsAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-27T12:23:25.000Z",
            "job_offer_expiration_timestamp": 1693139005,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Design",
                "Transformation",
                "Business transformation",
                "Data cleansing",
                "Data warehouse",
                "Technical writing",
                "ELT",
                "Reporting",
                "Scripting",
                "Budget",
                "Cloud computing",
                "Extract",
                "transform",
                "load",
                "Innovation",
                "Database",
                "Extraction",
                "Data",
                "Microsoft Windows Azure",
                "Data profiling",
                "Business intelligence",
                "Star schema",
                "Relational databases",
                "Jasmine",
                "Workflow",
                "SQL",
                "Data marts",
                "Software development",
                "Analytical skill",
                "Scala",
                "Creativity",
                "Apache Spark",
                "SQL Azure",
                "FOCUS",
                "Thought leadership",
                "Data quality",
                "Data integration",
                "Data engineering",
                "Solution architecture",
                "Analytics",
                "Python"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 125000,
            "job_max_salary": 150000,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "We are seeking an experienced Senior Data Engineer who will work with clients and members of the consulting team on the architecture, design, and development of highly scalable data integration and data engineering processes",
                    "The Senior Consultant must have a strong understanding and experience with data & analytics solution architecture, including data warehousing, data lakes, ETL/ELT workload patterns, and related BI & analytics systems",
                    "3+ years hands-on experience with one or more of these data integration/ETL tools:",
                    "Experience building on-prem data warehousing solutions",
                    "Experience with designing and developing ETL's, Data Marts, Star Schema's",
                    "Moving data from on-prem to cloud",
                    "Designing a data warehouse solution using Synapse or Azure SQL DB",
                    "Experience building pipelines using Synapse or Azure Data Factory to ingest data from various sources",
                    "Understanding of integration run times available in Azure",
                    "Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases",
                    "Knowledge of scripting languages like Python, Scala",
                    "Microsoft Azure Cloud platform certifications (nice to have)",
                    "Must be able to travel to client locations based on project needs"
                ],
                "Responsibilities": [
                    "Deliver consulting projects/work on-time, on-budget, and in a way that accomplishes client goals",
                    "Develop and implement technical best practices for data ingestion, data quality, data cleansing, and other data integration/ETL/Engineering-related activities",
                    "Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and modern cloud technologies",
                    "Conduct or participate in meetings with owners of key system components to fully understand current data and systems environments",
                    "Resolve source data issues and refine transformation rules",
                    "Analyze source system data to assess transformation logic and data quality through data profiling",
                    "Leverage data quality processes to assist with data cleansing requirements",
                    "Work with technical and business representatives to determine strategies for handling data anomalies that are identified",
                    "Design ETL processes and develop source-to-target data mappings, integration workflows, and load processes",
                    "Develop, test, integrate, and deploy data pipelines using a variety of tools and external programming/scripting languages as necessary",
                    "Provide technical documentation and other artifacts for data pipelines, ingestion, integration or other data solutions",
                    "Identify problems, develop ideas and propose solutions within differing situations requiring analytical, evaluative or constructive thinking in daily work",
                    "Apply creative thinking to identify possible reporting solution alternatives",
                    "Other duties assigned as needed"
                ],
                "Benefits": [
                    "Salary: $125,000 - $150,000 per year",
                    "You will be eligible for full-time benefits during the contract period"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "V-Soft Consulting Group, Inc.",
            "employer_logo": "https://www.vsoftconsulting.com/wp-content/uploads/2018/09/VSoft-Logo.png",
            "employer_website": "http://www.vsoftconsulting.com",
            "employer_company_type": null,
            "job_publisher": "LinkedIn",
            "job_id": "g9Rv4Q-2xh0AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "AWS Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/aws-data-engineer-at-v-soft-consulting-group-inc-3673779665",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5829,
            "job_description": "Job Title: AWS Data Engineer\n\nLocation: HYBRID\n\nBoston, MA; Raleigh, NC or Westlake, TX (Need Locals)\n\nTop skills:\n\nMust-Haves (Concepts & Tools):\n\u2022 AWS cloud\u2014KMS, S3, Glue, Lambda etc.\n\u2022 Deployed data pipelines\n\u2022 Java, Python or PySpark hands on development experience\n\nNice-to-Haves (Concepts & Tools):\n\u2022 Prior ETL migration experience from on prem. To cloud\n\u2022 Exposure to even driven streaming\n\nThanks,\n\nMounika,\n\nmvajja@vsoftconsulting.com",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690213278,
            "job_posted_at_datetime_utc": "2023-07-24T15:41:18.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=g9Rv4Q-2xh0AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-23T15:41:18.000Z",
            "job_offer_expiration_timestamp": 1692805278,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Java, Python or PySpark hands on development experience"
                ],
                "Responsibilities": [
                    "AWS cloud\u2014KMS, S3, Glue, Lambda etc",
                    "Deployed data pipelines",
                    "Exposure to even driven streaming"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "NTT DATA Services",
            "employer_logo": "https://www.nttdata.com/jp/ja/-/media/nttdatajapan/images/news/release/2022/082500/082500-01.png?la=ja-jp&hash=5325A3858BA7CD912B3FA5B285C60638647319CC",
            "employer_website": "http://www.nttdata.com",
            "employer_company_type": null,
            "job_publisher": "Built In",
            "job_id": "Bd66No41SbcAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer (Python(Expert)/Cloud/AWS/GCP) (Dallas, TX)",
            "job_apply_link": "https://builtin.com/job/data/senior-data-engineer-pythonexpertcloudawsgcp/1933468",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5966,
            "job_description": "Req ID: 247008\n\nNTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\n\nWe are currently seeking a Senior Data Engineer (Python(Expert)/Cloud/AWS/GCP) to join our team in Dallas, Texas (US-TX), United States (US).\n\nJob Duties and Responsibilities:\n\u2022 Understand client requirements and understand case studies or current implementation for Predictive models, able to understand business domain needs and implement data pipelines & predictive models, Experience in Agile projects, work with multiple stakeholders like business, project team and deployment teams, ability to work independently and switch technical skills based on the project needs.\n\u2022 Detail oriented self-starter capable of working independently.\n\nExperience in ETL and ETL cloud services like AWS Data Pipeline Product Details, AWS Glue\n\u2022 Experience with private or public cloud technology.\n\u2022 Excellent written and verbal communication skills with ability to document and design proposals.\n\u2022 Expert in writing software packaging and deploying into a fully automated environment.\n\u2022 Experience in Service Now.\n\nBasic Qualifications:\n\u2022 5+ years of Release or Automation or Software Engineering experience, or equivalent.\n\u2022 5+ years Linux experience.\n\u2022 5+ years programming experience with Java or Python and scripting\n\u2022 3+ years of experience in DevSecOps toolchains/automation to achieve CICD, Blue-Green deployments, feature toggles (Git, Jenkins, uDeploy).\n\u2022 3+ years with Agile Scrum (Daily Standup, Sprint Planning and Sprint Retrospective meetings) and Kanban.\n\u2022 CADM-Cloud Apps-AWS (Amazon)- 3-5 years\n\u2022 Data and Intelligence-ETL-Architecture-ETL Tools - 3-5 years\n\n#INDFSINS\n\n#INDAPPS\n\nAbout NTT DATA Services\n\nNTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.\n\nNTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690049211,
            "job_posted_at_datetime_utc": "2023-07-22T18:06:51.000Z",
            "job_city": "Plano",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 33.019844,
            "job_longitude": -96.69888,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Bd66No41SbcAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience in ETL and ETL cloud services like AWS Data Pipeline Product Details, AWS Glue",
                    "Experience with private or public cloud technology",
                    "Excellent written and verbal communication skills with ability to document and design proposals",
                    "Expert in writing software packaging and deploying into a fully automated environment",
                    "Experience in Service Now",
                    "5+ years of Release or Automation or Software Engineering experience, or equivalent",
                    "5+ years Linux experience",
                    "5+ years programming experience with Java or Python and scripting",
                    "3+ years of experience in DevSecOps toolchains/automation to achieve CICD, Blue-Green deployments, feature toggles (Git, Jenkins, uDeploy)",
                    "3+ years with Agile Scrum (Daily Standup, Sprint Planning and Sprint Retrospective meetings) and Kanban",
                    "CADM-Cloud Apps-AWS (Amazon)- 3-5 years",
                    "Data and Intelligence-ETL-Architecture-ETL Tools - 3-5 years"
                ],
                "Responsibilities": [
                    "Understand client requirements and understand case studies or current implementation for Predictive models, able to understand business domain needs and implement data pipelines & predictive models, Experience in Agile projects, work with multiple stakeholders like business, project team and deployment teams, ability to work independently and switch technical skills based on the project needs",
                    "Detail oriented self-starter capable of working independently"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "ringcentral",
            "employer_logo": "https://www.ringcentral.com/content/dam/rc-www/en_us/images/content/whyringcentral/pressimages/2022/img-1.png",
            "employer_website": "http://www.ringcentral.com",
            "employer_company_type": "Information",
            "job_publisher": "Salary.com",
            "job_id": "NTqfpUM9qagAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer (Full Time; Multiple Openings",
            "job_apply_link": "https://www.salary.com/job/ringcentral/data-engineer-full-time-multiple-openings/j202305260616257193727",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6457,
            "job_description": "Say hello to possibilities. It\u2019s not everyday that you consider starting a new career. We\u2019re RingCentral, and we\u2019re happy that someone as talented as you is considering this role. First, a little about us, we\u2019re the $2 billion global leader in cloud-based communications and collaboration software. We are fundamentally changing the nature of human interaction\u2014giving people the freedom to connect powerfully and personally from anywhere, at any time, on any device. This is where you and your skills come in. We\u2019re currently looking for: Responsible for building and maintaining solutions around the various RingCentral distributed database technologies. To succeed in this role you must have experience in: Develop and enhance RingCentral\u2019s distributed databases and data infrastructure; Develop high-quality, high-performance distributed systems in Python, SQL and Java. Build robust, reliable, automated data pipelines using Kafka and Spark streaming. Develop upon and integrate with other services within the RingCentral application and development stacks. Work closely with other teams to understand and mitigate issues and improve performance. Work closely with RingCentral\u2019s operations teams to help develop and optimize solutions. Work with large data volumes, including processing, transforming and transporting large-scale data using big data stacks. Design, build and launch new data models in production. Design, build and launch new data extraction, transformation and loading processes in production. Desired Qualifications: U.S. Master\u2019s degree or foreign equivalent in Computer Science, Information Systems or a related field. Experience with Hadoop, HDFS, Hive, HBase, Python, SQL, Kafka, Spark, MapReduce, Hive SQL, and Linux is required Exerience with Programming Languages: C/C , Java, C, Python, Unix is required. Experience with Databases: MongoDB, ElasticSearch, Vertica, Amazon Redshift, Oracle is required. What we offer: Comprehensive medical, dental, vision, disability, life insurance Health Savings Account (HSA), Flexible Spending Account (FSAs) and Commuter benefits 401K match and ESPP Flexible vacation Wellness programs including 1:1 coaching and meditation guidance Paid parental and pregnancy leave and new parent gift boxes Family-forming benefits (IVF, Preservation, Adoption etc.) Emergency backup care (Child/Adult/Pets) Parental support for children with developmental and learning disabilities Pet insurance Employee Assistance Program (EAP) with counseling sessions available 24/7 Free legal services that provide legal advice, document creation and estate planning Employee bonus referral program Student loan refinancing assistance Employee perks and discounts program RingCentral\u2019s IT team ensures company data is accessible, secure, and optimized in ways that provide maximum competitive advantage. We are constantly discovering, developing and deploying innovations that power productivity and drive better decisions for our customers. Our IT professionals are talented, ambitious, out-of-the-box thinkers who love to learn on the job\u2014planning, deploying and maintaining state-of-the-art technology to deliver flawless performance 24/7/365. RingCentral\u2019s work culture is the backbone of our success. And don\u2019t just take our word for it: we are recognized as a Best Place to Work by Glassdoor, the Top Work Culture by Comparably and hold local BPTW awards in every major location. Bottom line: We are committed to hiring and retaining great people because we know you power our success. RingCentral offers on-site, remote and hybrid work options optimized for the ways we work and live now. About RingCentral RingCentral, Inc. (NYSE: RNG) is a leading provider of business cloud communications and contact center solutions based on its powerful Message Video Phone\u2122\u200a\u200a(MVP\u2122) global platform. More flexible and cost effective than legacy on-premises PBX and video conferencing systems that it replaces, RingCentral\u00ae empowers modern mobile and distributed workforces to communicate, collaborate, and connect via any mode, any device, and any location. RingCentral is headquartered in Belmont, California, and has offices around the world. RingCentral is an equal opportunity employer that truly values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. If you are hired in Dallas, Texas, the compensation range for this position is $153, 000 for full-time employees, in addition to eligibility for variable pay, equity, and benefits. Benefits may include, but are not limited to, health and wellness, 401k, ESPP, vacation, parental leave, and more! The salary may vary depending on your location, skills, and experience. RingCentral, Inc. (NYSE: RNG) is a leading provider of global enterprise cloud communications, collaboration, and contact center solutions. More flexible and cost-effective than legacy on-premises systems, the RingCentral platform empowers employees to Work as OneTM from any location, on any device, and via any mode to better serve customers, improving business efficiency and customer satisfaction. The company provides unified voice, video meetings, team messaging, digital customer engagement, and integrated contact center solutions for enterprises globally. RingCentral\u2019s open platform integrates with leading business apps and enables customers to easily customize business workflows. RingCentral is headquartered in Belmont, California, and has offices around the world. RingCentral is an EEO/AA employer.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1685145600,
            "job_posted_at_datetime_utc": "2023-05-27T00:00:00.000Z",
            "job_city": null,
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.707874,
            "job_longitude": -96.92091,
            "job_benefits": [
                "health_insurance",
                "paid_time_off",
                "dental_coverage",
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=NTqfpUM9qagAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-11-26T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1700956800,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {},
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "511210",
            "job_naics_name": "Software Publishers"
        },
        {
            "employer_name": "RIT Solutions, Inc.",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/4/49/Rochester_Institute_of_Technology_Seal_%282018%29.svg",
            "employer_website": "http://www.rit.edu",
            "employer_company_type": "Education",
            "job_publisher": "Lensa",
            "job_id": "fgEVfs0XL3QAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://lensa.com/data-engineer-jobs/dallas/jd/d8a402729fcfee1b3630e62a8405127f",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4732,
            "job_description": "Job Description\n\nDuration: 6+ month\nVisa: Only GC/USC -will need GC copy /citizenship proof\nMust have a valid LinkedIn profile\nMust be local and go onsite 2-3 days/week - Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ\nMust haves-\n\nTerraform -5+ years\n\nAzure -4+ years\n\nData Lake- 4+ years\n\nData Architecture -2+ years\n\u2022 * We are seeking Cloud DB Platform Engineers with Cloud and Data Engineering experience.\n\nNeed some of the following skills:\nTerraform or Kubernetes, Azure/GCP, DevOps Engineering, ETL , Client/AI, Data lake, Data architecture",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689276160,
            "job_posted_at_datetime_utc": "2023-07-13T19:22:40.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=fgEVfs0XL3QAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-12T19:22:40.000Z",
            "job_offer_expiration_timestamp": 1691868160,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Reading Comprehension",
                "Active Listening",
                "Writing",
                "Speaking",
                "Critical Thinking",
                "Active Learning",
                "Monitoring",
                "Social Perceptiveness",
                "Coordination",
                "Complex Problem Solving",
                "Programming",
                "Judgment and Decision Making",
                "Systems Analysis",
                "Systems Evaluation"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Visa: Only GC/USC -will need GC copy /citizenship proof",
                    "Must have a valid LinkedIn profile",
                    "Must be local and go onsite 2-3 days/week - Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ",
                    "Terraform -5+ years",
                    "Azure -4+ years",
                    "Data Lake- 4+ years",
                    "Data Architecture -2+ years",
                    "We are seeking Cloud DB Platform Engineers with Cloud and Data Engineering experience",
                    "Terraform or Kubernetes, Azure/GCP, DevOps Engineering, ETL , Client/AI, Data lake, Data architecture"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06 Database Architects"
            ],
            "job_naics_code": "61",
            "job_naics_name": "Education"
        },
        {
            "employer_name": "Evergreen Residential Holdings, LLC",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTzXy9DKoUcVC1d9HG1Ia4RfWx6MNgGfVUdEl1r&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Glassdoor",
            "job_id": "uGwOKTzWK8YAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Sr. Data Engineer",
            "job_apply_link": "https://www.glassdoor.com/job-listing/sr-data-engineer-evergreen-residential-JV_IC1139977_KO0,16_KE17,38.htm?jl=1008718815422",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5623,
            "job_description": "We are Evergreen Residential, a high growth early-stage institutional investment platform in the single-family residential sector. Our team is collaborative, open-minded and curious. Transparency is a core value, we speak our minds, are responsible for our actions and celebrate our wins. We are serious about the business without taking ourselves too seriously. We look for people who thrive in an entrepreneurial and fast paced environment. If you are self-motivated and mission driven with a 'can do' mindset and see solutions where others may see problems, come and grow with us!\n\nWe offer a flexible, empowering culture, competitive compensation and benefits, and potential for career growth through working closely with, and learning from, our experienced leadership team.\n\nThis is not a consulting position. Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time for this position.\n\nThis position will be (Full-time/Permanent employment) based in our downtown Dallas offices, and we will consider a hybrid work schedule.\n\nAs a technical/engineering expert, you also pride yourself on being able to quickly build strong business relationships both internally and externally e.g., with the leadership team, current and potential investors. With a passion for keeping current with advancements of the field, you deploy technology and data resources to provide innovative solutions to business needs. You will design and support data warehouse systems, perform data extraction and ensure data accuracy, enabling real-time insights from both internal and market data that will drive revenue growth and capital efficiency. This position plays a critical role in working with our analytics and reporting specialists to help Evergreen Residential make the best investment decisions.\n\nThe Role: Priorities can often change in a fast-paced environment like ours. Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool. Overall ensure there is one source of truth.\n\nThe role includes, but is not limited to, the following responsibilities:\n\u2022 Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository\n\u2022 Developing and maintaining data processing and storage infrastructure\n\u2022 Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes\n\u2022 Collaborating with data analysts to ensure that data is readily available for analysis and modeling\n\u2022 Optimizing database performance and troubleshooting issues as they arise\n\u2022 Implementing data security and access controls to protect sensitive data\n\u2022 Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization\n\u2022 Staying up-to-date with emerging trends and technologies in data engineering\n\u2022 Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance\n\u2022 Ensure scientific method and research are key drivers of the product roadmap\n\nWhat You Will Bring to the Table:\n\u2022 Bachelor's Degree in a relevant field required\n\u2022 Min 5 years of experience in data engineering or a related field\n\u2022 Proficiency in one or more programming languages such as Python, Java, or Scala\n\u2022 Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases\n\u2022 Experience in real estate investment and/or rental sector highly desirable\n\u2022 Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry\n\u2022 Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function\n\u2022 Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level\n\u2022 A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk\n\u2022 Collaborative team player, you are truly a \"do-er\", happy to be a hands-on problem-solver to move the data program forward\n\u2022 Excellent communication skills \u2013 verbal and written\n\nAbout Evergreen Residential\n\nFounded in 2021, Evergreen Residential is a full-service SFR platform leveraging proven operational practices and the latest technological advances to optimize investor returns and achieve positive outcomes for our residents and the communities in which we operate. We offer a full suite of services, including Investment Management, Asset Origination, and Advisory Services. The firm is headquartered in Dallas with offices in New York City.\n\nThe leadership team has extensive experience dating back to the early institutionalization of SFR and unrivaled depth of experience in the complete asset life cycle. We are built to withstand changing market conditions, and our business produces resilient, predictable cash flows and margins. We are committed to charting new paths and using data to achieve best-in-class results. Our business is evergreen.\n\nBeyond financial returns, the Company is committed to measurable impact objectives. We believe that inclusive and equitable management, environmentally sustainable long-term strategies, and resident-focused policies are good business - for our residents, our investors, and our team. We are committed to using environmentally sustainable practices and empowering our residents to improve their financial health.\n\nOur cornerstone values - Accountability, Transparency and Partnership - are built on a foundation of Integrity and provide the roadmap for our daily actions, interactions and decisions.\n\nEqual Opportunities and Other Employment Statements\n\nWe are deeply committed to building a workplace and community where inclusion is not only valued, but prioritized. We take pride in being an equal opportunity employer and seek to create a welcoming environment based on mutual respect, and to recruit, develop and retain the most talented people from a diverse candidate pool. All employment decisions shall be made without regard to race, color, religion, gender, gender identity or expression, family status, marital status, sexual orientation, national origin, genetics, neuro-diversity, disability, age, or veteran status, or any other basis as protected by federal, state, or local law.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1687392000,
            "job_posted_at_datetime_utc": "2023-06-22T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=uGwOKTzWK8YAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's Degree in a relevant field required",
                    "Min 5 years of experience in data engineering or a related field",
                    "Proficiency in one or more programming languages such as Python, Java, or Scala",
                    "Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases",
                    "Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry",
                    "Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function",
                    "Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level",
                    "A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk",
                    "Collaborative team player, you are truly a \"do-er\", happy to be a hands-on problem-solver to move the data program forward",
                    "Excellent communication skills \u2013 verbal and written"
                ],
                "Responsibilities": [
                    "You will design and support data warehouse systems, perform data extraction and ensure data accuracy, enabling real-time insights from both internal and market data that will drive revenue growth and capital efficiency",
                    "Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool",
                    "Overall ensure there is one source of truth",
                    "Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository",
                    "Developing and maintaining data processing and storage infrastructure",
                    "Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes",
                    "Collaborating with data analysts to ensure that data is readily available for analysis and modeling",
                    "Optimizing database performance and troubleshooting issues as they arise",
                    "Implementing data security and access controls to protect sensitive data",
                    "Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization",
                    "Staying up-to-date with emerging trends and technologies in data engineering",
                    "Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance",
                    "Ensure scientific method and research are key drivers of the product roadmap"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "LTIMindtree",
            "employer_logo": "https://www.ltimindtree.com/wp-content/uploads/2022/10/LTIMindtree_Linear_2-1-LT-Blue-1-1.png",
            "employer_website": "http://www.lntinfotech.com",
            "employer_company_type": "Computer Services",
            "job_publisher": "LTIMindtree",
            "job_id": "VoQSDjkz-2QAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Specialist - Data Engineering",
            "job_apply_link": "https://careers.ltimindtree.com/job/Dallas-Senior-Specialist-Data-Engineering-TX-75201/956028601/",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7696,
            "job_description": "A little about us...\n\nLTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 750 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree \u2014 a Larsen & Toubro Group company \u2014 combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com\n\nJob Title : Data Engineer (MSBI \u2013 SSIS)\n\nJob Location : Remote / Dallas, TX\n\nRequired Experience : 06 - 12 Years\n\nJob Description:\n\u2022 We are looking for an experienced Data Engineer (MSBI-SSIS) to join our team.\n\u2022 The ideal candidate is highly skilled in MSBI-SSIS and has experience in dealing with large-scale data extraction and manipulation.\n\u2022 The Data Engineer will be responsible for designing and developing SSIS packages to extract and manipulate data from multiple sources.\n\u2022 The Engineer will also be responsible for managing and troubleshooting data flows, ensuring the data is correct and within the required formats.\n\nResponsibilities:\n\u2022 Design, develop, and maintain ETL packages using MSBI-SSIS.\n\u2022 Develop data dictionary for the data sources and target systems.\n\u2022 Analyze data requirements and develop solutions to meet them.\n\u2022 Troubleshoot and debug data integration issues.\n\u2022 Proficient in creating jobs, packages, and stored procedures in MSBI-SSIS.\n\u2022 Design and develop data models to support the data analysis process.\n\u2022 Ensure data consistency, integrity, and security.\n\u2022 Develop and maintain data related documentation.\n\u2022 Develop and maintain SSIS packages for data integration and migration.\n\u2022 Develop and maintain SSRS reports.\n\u2022 Perform data profiling and data cleansing.\n\u2022 Monitor performance and optimize ETL processes.\n\u2022 Prepare data for reporting and analysis.\n\u2022 Work with BI teams to ensure data accuracy.\n\u2022 Assist with other data related tasks as needed.\n\nRequirements:\n\u2022 Bachelor\u2019s degree in Computer Science or related field.\n\u2022 6+ years of experience in MSBI-SSIS development.\n\u2022 Expertise in data integration and data manipulation.\n\u2022 Experience with SQL databases and query optimization.\n\u2022 Knowledge of data warehousing concepts and ETL design patterns.\n\u2022 Knowledge of data profiling and data cleansing.\n\u2022 Experience with SSRS reporting.\n\u2022 Excellent problem-solving, communication, and analytical skills.\n\u2022 Ability to work independently and in a team environment.\n\nHow will you grow:\n\u2022 Role-based Training programs\n\u2022 Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional\n\u2022 We encourage you to acquire various beneficial international certifications, with costs s reimbursed\n\u2022 Our role-based workshop helps us groom future leaders for LTI\n\nWhat's in it for you:\n\u2022 Excellent benefits plan: medical, dental, vision, life, FSA, & PTO\n\u2022 Roll over vacation days\n\u2022 Commuter benefits\n\u2022 Excellent growth and advancement opportunities\n\u2022 Certification reimbursement\n\u2022 Rewards and recognition programs\n\u2022 Innovative and collaborative company culture\n\nLTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688947200,
            "job_posted_at_datetime_utc": "2023-07-10T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "paid_time_off",
                "dental_coverage",
                "retirement_savings",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=VoQSDjkz-2QAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Required Experience : 06 - 12 Years",
                    "The ideal candidate is highly skilled in MSBI-SSIS and has experience in dealing with large-scale data extraction and manipulation",
                    "Bachelor\u2019s degree in Computer Science or related field",
                    "6+ years of experience in MSBI-SSIS development",
                    "Expertise in data integration and data manipulation",
                    "Experience with SQL databases and query optimization",
                    "Knowledge of data warehousing concepts and ETL design patterns",
                    "Knowledge of data profiling and data cleansing",
                    "Experience with SSRS reporting",
                    "Excellent problem-solving, communication, and analytical skills",
                    "Ability to work independently and in a team environment"
                ],
                "Responsibilities": [
                    "The Data Engineer will be responsible for designing and developing SSIS packages to extract and manipulate data from multiple sources",
                    "The Engineer will also be responsible for managing and troubleshooting data flows, ensuring the data is correct and within the required formats",
                    "Design, develop, and maintain ETL packages using MSBI-SSIS",
                    "Develop data dictionary for the data sources and target systems",
                    "Analyze data requirements and develop solutions to meet them",
                    "Troubleshoot and debug data integration issues",
                    "Proficient in creating jobs, packages, and stored procedures in MSBI-SSIS",
                    "Design and develop data models to support the data analysis process",
                    "Ensure data consistency, integrity, and security",
                    "Develop and maintain data related documentation",
                    "Develop and maintain SSIS packages for data integration and migration",
                    "Develop and maintain SSRS reports",
                    "Perform data profiling and data cleansing",
                    "Monitor performance and optimize ETL processes",
                    "Prepare data for reporting and analysis",
                    "Work with BI teams to ensure data accuracy",
                    "Assist with other data related tasks as needed"
                ],
                "Benefits": [
                    "Role-based Training programs",
                    "Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional",
                    "We encourage you to acquire various beneficial international certifications, with costs s reimbursed",
                    "Excellent benefits plan: medical, dental, vision, life, FSA, & PTO",
                    "Roll over vacation days",
                    "Commuter benefits",
                    "Excellent growth and advancement opportunities",
                    "Rewards and recognition programs",
                    "Innovative and collaborative company culture"
                ]
            },
            "job_job_title": "Senior specialist",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541511",
            "job_naics_name": "Custom Computer Programming Services"
        },
        {
            "employer_name": "Alcority",
            "employer_logo": null,
            "employer_website": "http://www.alcority.com",
            "employer_company_type": null,
            "job_publisher": "ZipRecruiter",
            "job_id": "K392nkfmd28AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineering Manager",
            "job_apply_link": "https://www.ziprecruiter.com/c/Alcority/Job/Data-Engineering-Manager/-in-Dallas,TX?jid=36ea57b473f139e6",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.7286,
            "job_description": "Job Summary:\n\nAlcority, a global professional services firm, is looking for an experienced Data Engineering Manager. The Data Engineering Manager, reporting to the Data Solutions Architect, expand and lead our data engineering team. As a strategic partner to a number of investments and operating companies around the world, Alcority is responsible for delivering technology solutions that help our partners create a competitive advantage. The ideal candidate will have strong leadership skills, technical expertise in data engineering, and excellent communication skills. The Data Engineering Manager will be responsible for overseeing the development and maintenance of our data engineering architecture, data pipelines, and data platforms, and must have experience in managing data engineering teams and working with the latest cloud based and Informatica SaaS technologies. He/she will have a passion for data with a keen interest to not only understand the technical components, but also understand the business context and impact of technical solutions that are provided.\n\nThe candidate can prioritize competing requirements, strategize development, guide in developing scalable solutions, and leverage AI for development.\n\nKey Responsibilities:\n\u2022 Lead and manage the internal Data Engineering Team, including hiring, training, coaching/mentoring, and performance management.\n\u2022 Develop resource plans to determine the right mix of internal and external resources; engage and manage 3rd party consultants/contractors as needed.\n\u2022 Develop and maintain the data engineering architecture and design to support the company's business objectives and strategy.\n\u2022 Develop and maintain data pipelines and data platforms using cloud-based technologies such as Informatica Data Management Cloud and Azure Data Factory ensuring data quality and reliability.\n\u2022 Work collaboratively with cross-functional teams and businesses including product management, data science, and analytics teams, to deliver data solutions that meet business needs.\n\u2022 Develop and maintain standards, policies, and procedures for data engineering best practices.\n\u2022 Develop and maintain documentation for data engineering processes and procedures.\n\u2022 Implement and manage best SDLC practices including agile CI/CD, automation.\n\u2022 Manage budgets, timelines, and resources for data engineering projects.\n\u2022 Clearly understand priority under pressure, strategize, guide, and develop scalable solutions.\n\u2022 Have knowledge to leveraging AI for development, using machine learning and other AI technologies to improve data quality, automate processes, and enhance the data engineering pipeline.\n\nRequirements:\n\u2022 Bachelor's degree in computer science, Information Technology, or a related field\n\u2022 At least 10 years of experience in data engineering, with at least 5 years in a leadership role managing teams that have spanned across multiple international Time zones.\n\u2022 Strong technical skills in data engineering, including experience with data architecture, data pipelines, data platforms, and data integration tools, with a focus on cloud-based technologies such as Informatica Cloud, Azure Data Factory with a Strong mindset on Building the softwares as Scalable micro services or micro components.\n\u2022 Experience with Informatica SaaS and other relevant cloud-based technologies such as Snowflake, Databricks, and Apache Kafka\n\u2022 Strong leadership skills, with experience in managing large data sets and engineering teams. Should be a master in delegating, guiding, and completing.\n\u2022 Excellent communication and interpersonal skills, with the ability to work effectively with cross-functional teams.\n\u2022 Strong problem-solving and analytical skills\n\u2022 Ability to clearly understand priority under pressure, strategize, guide, and develop scalable solutions.\n\u2022 Experience leveraging AI for development, using machine learning and other AI technologies to improve data quality, automate processes, and enhance the data engineering pipeline.\n\u2022 Ability to work under pressure and navigate multiple priorities in a fast-paced environment.\n\u2022 Experience with Agile development methodologies is a plus. (Eg: Devops, Jira or GitHub)\n\nIt is impossible to list every requirement for, or responsibility of, any position. Similarly, we cannot identify all the skills a position may require since job responsibilities and the Company's needs may change over time. Therefore, the above job description is not comprehensive or exhaustive. The Company reserves the right to adjust, add to or eliminate any aspect of the above description. The Company also retains the right to require all employees to undertake additional or different job responsibilities when necessary to meet business needs.\n\nMust be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future.\n\nBenefits & Perks:\n\u2022 Time Off: 20 days of PTO for full-time employees and 12 company holidays.\n\u2022 Summer Fridays: July 4th through Labor Day, the office is completely closed/offline every other Friday.\n\u2022 Company Paid Benefits: Life insurance, Short-term disability, Long-term disability, Paid parental leave, Employee Assistance Program, and medical insurance in our high deductible health plan.\n\u2022 Optional Employee Paid Benefits:Medical insurance in our EPO plan, Dental benefits, and Vision benefits. We also offer Health Savings Accounts, Flexible Spending Accounts, Supplemental Life insurance, and more.\n\u2022 401(k): Eligible after 60 days. Discretionary company match of 50% up to the first 6% of contributions.\n\nEQUAL OPPORTUNITY EMPLOYER\n\nALCORITY IS AN EQUAL EMPLOYMENT OPPORTUNITY EMPLOYER. THE COMPANY'S POLICY IS NOT TO DISCRIMINATE AGAINST ANY APPLICANT OR EMPLOYEE BASED ON RACE, COLOR, RELIGION, NATIONAL ORIGIN, GENDER, AGE, SEXUAL ORIENTATION, GENDER IDENTITY OR EXPRESSION, MARITAL STATUS, MENTAL OR PHYSICAL DISABILITY, AND GENETIC INFORMATION, OR ANY OTHER BASIS PROTECTED BY APPLICABLE LAW. THE FIRM ALSO PROHIBITS HARASSMENT OF APPLICANTS OR EMPLOYEES BASED ON ANY OF THESE PROTECTED CATEGORIES.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690271308,
            "job_posted_at_datetime_utc": "2023-07-25T07:48:28.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "paid_time_off",
                "dental_coverage",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=K392nkfmd28AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-24T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1692835200,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 120,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's degree in computer science, Information Technology, or a related field",
                    "At least 10 years of experience in data engineering, with at least 5 years in a leadership role managing teams that have spanned across multiple international Time zones",
                    "Strong technical skills in data engineering, including experience with data architecture, data pipelines, data platforms, and data integration tools, with a focus on cloud-based technologies such as Informatica Cloud, Azure Data Factory with a Strong mindset on Building the softwares as Scalable micro services or micro components",
                    "Experience with Informatica SaaS and other relevant cloud-based technologies such as Snowflake, Databricks, and Apache Kafka",
                    "Strong leadership skills, with experience in managing large data sets and engineering teams",
                    "Should be a master in delegating, guiding, and completing",
                    "Excellent communication and interpersonal skills, with the ability to work effectively with cross-functional teams",
                    "Strong problem-solving and analytical skills",
                    "Ability to clearly understand priority under pressure, strategize, guide, and develop scalable solutions",
                    "Experience leveraging AI for development, using machine learning and other AI technologies to improve data quality, automate processes, and enhance the data engineering pipeline",
                    "Ability to work under pressure and navigate multiple priorities in a fast-paced environment",
                    "(Eg: Devops, Jira or GitHub)",
                    "Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future"
                ],
                "Responsibilities": [
                    "The Data Engineering Manager will be responsible for overseeing the development and maintenance of our data engineering architecture, data pipelines, and data platforms, and must have experience in managing data engineering teams and working with the latest cloud based and Informatica SaaS technologies",
                    "He/she will have a passion for data with a keen interest to not only understand the technical components, but also understand the business context and impact of technical solutions that are provided",
                    "The candidate can prioritize competing requirements, strategize development, guide in developing scalable solutions, and leverage AI for development",
                    "Lead and manage the internal Data Engineering Team, including hiring, training, coaching/mentoring, and performance management",
                    "Develop resource plans to determine the right mix of internal and external resources; engage and manage 3rd party consultants/contractors as needed",
                    "Develop and maintain the data engineering architecture and design to support the company's business objectives and strategy",
                    "Develop and maintain data pipelines and data platforms using cloud-based technologies such as Informatica Data Management Cloud and Azure Data Factory ensuring data quality and reliability",
                    "Work collaboratively with cross-functional teams and businesses including product management, data science, and analytics teams, to deliver data solutions that meet business needs",
                    "Develop and maintain standards, policies, and procedures for data engineering best practices",
                    "Develop and maintain documentation for data engineering processes and procedures",
                    "Implement and manage best SDLC practices including agile CI/CD, automation",
                    "Manage budgets, timelines, and resources for data engineering projects",
                    "Clearly understand priority under pressure, strategize, guide, and develop scalable solutions",
                    "Have knowledge to leveraging AI for development, using machine learning and other AI technologies to improve data quality, automate processes, and enhance the data engineering pipeline"
                ],
                "Benefits": [
                    "Time Off: 20 days of PTO for full-time employees and 12 company holidays",
                    "Summer Fridays: July 4th through Labor Day, the office is completely closed/offline every other Friday",
                    "Company Paid Benefits: Life insurance, Short-term disability, Long-term disability, Paid parental leave, Employee Assistance Program, and medical insurance in our high deductible health plan",
                    "Optional Employee Paid Benefits:Medical insurance in our EPO plan, Dental benefits, and Vision benefits",
                    "We also offer Health Savings Accounts, Flexible Spending Accounts, Supplemental Life insurance, and more",
                    "401(k): Eligible after 60 days",
                    "Discretionary company match of 50% up to the first 6% of contributions"
                ]
            },
            "job_job_title": "Engineering manager",
            "job_posting_language": "en",
            "job_onet_soc": "11302100",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1141.00: Database Administrators"
            ]
        },
        {
            "employer_name": "Disney Media & Entertainment Distribution",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Disney_Media_and_Entertainment_Distribution_logo.svg/1200px-Disney_Media_and_Entertainment_Distribution_logo.svg.png",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "MyArklaMiss Jobs",
            "job_id": "xWoztRVZBtcAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Lead Data Engineer",
            "job_apply_link": "https://jobs.myarklamiss.com/jobs/lead-data-engineer-dallas-texas/1074864542-2/",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5492,
            "job_description": "Job Overview\n\nOur Data and Analytics team for Disney Streaming Services (DSS), a segment under the Disney Entertainment & ESPN Technology (DE&ET) is looking for a Lead Data Engineer. Data is essential for all our decision-making needs whether it's related to product design, measuring advertising effectiveness, helping users discover new content or building new businesses in emerging markets. This data is deeply valuable and gives us insights into how we can continue improving our service for our users, advertisers and our content partners. Our Engagement and Retention Data Engineering team is seeking a highly motivated Data Engineer with a strong technical background and passionate about diving deeper into Big Data to develop state of the art Data Solutions.\n\nWhat You'll Do\n\u2022 Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science\n\u2022 Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)\n\u2022 Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala\n\u2022 Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts\n\u2022 Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions\n\u2022 Maintain detailed documentation of your work and changes to support data quality and data governance\n\u2022 Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)\n\u2022 Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team\nKey Qualifications\n\u2022 At least 7 years of data engineering experience developing large data pipelines\n\u2022 Strong SQL skills and ability to create queries to extract data and build performant datasets\n\u2022 Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data\n\u2022 Strong programming skills in Python\n\u2022 Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)\n\u2022 Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines\n\u2022 Strong in Data Modeling techniques and Data Warehousing standard methodologies and practices\n\u2022 Familiar with Scrum and Agile methodologies\n\u2022 You are a problem solver with strong attention to detail and excellent analytical and communication skills\n\u2022 Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)\nRequired Education\n\u2022 Bachelor's or Master's Degree in Computer Science, Information Systems or related field\nAdditional Information\n\nThe hiring range for this position in California is $149,240 - $200,200 per year. The base pay actually offered will take into account internal equity and also may vary depending on the candidate's geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690429495,
            "job_posted_at_datetime_utc": "2023-07-27T03:44:55.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=xWoztRVZBtcAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 149240,
            "job_max_salary": 200200,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "At least 7 years of data engineering experience developing large data pipelines",
                    "Strong SQL skills and ability to create queries to extract data and build performant datasets",
                    "Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data",
                    "Strong programming skills in Python",
                    "Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)",
                    "Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines",
                    "Strong in Data Modeling techniques and Data Warehousing standard methodologies and practices",
                    "Familiar with Scrum and Agile methodologies",
                    "You are a problem solver with strong attention to detail and excellent analytical and communication skills",
                    "Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)",
                    "Bachelor's or Master's Degree in Computer Science, Information Systems or related field"
                ],
                "Responsibilities": [
                    "Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science",
                    "Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)",
                    "Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala",
                    "Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts",
                    "Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions",
                    "Maintain detailed documentation of your work and changes to support data quality and data governance",
                    "Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)",
                    "Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team"
                ],
                "Benefits": [
                    "The base pay actually offered will take into account internal equity and also may vary depending on the candidate's geographic region, job-related knowledge, skills, and experience among other factors",
                    "A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Fidelity Investments",
            "employer_logo": "https://1000logos.net/wp-content/uploads/2017/11/Fidelity-Logo.png",
            "employer_website": "http://www.fidelity.com",
            "employer_company_type": "Finance",
            "job_publisher": "Fidelity Investments",
            "job_id": "ailo5hpdYyEAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://jobiak.fidelity.com/jobdetails/dallas-tx-data-engineer-64be49cf4d21b70a8cf28959",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5711,
            "job_description": "Job Description:\n\nFidelity Investments Workplace Solutions (WS) organization is looking for an Principal Data Engineer. This person will be playing a key role in designing and crafting a modern Data and Information Delivery and Analytics platform in the cloud to support Equity Compensation products for the Global markets. This person will be working closely with architects and engineers and business SMEs to build and release solutions that help customers get the information they need fast and intuitively.\n\nThe Expertise and Skills You Bring\n\u2022 Bachelor\u2019s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required\n\u2022 10+ years of hands-on experience in Data engineering, data warehousing and analytics technologies\n\u2022 Proven ability with modern Object-Oriented Programming Languages (Python, Scala, Java)\n\u2022 Advanced skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform\n\u2022 Strong knowledge of designing data engineering solutions and platforms\n\u2022 Working experience with Relational Databases like Oracle\n\u2022 Experience with business intelligence and analytics tools such as OBIEE, PowerBI or Tableau\n\u2022 Strong working knowledge of cloud native data warehousing and data lake solutions using Snowflake, Redshift etc.\n\u2022 Experience building scalable patterns for data consumption from cloud based data-lakes.\n\u2022 Advanced experience with PL/SQL and complex queries\n\u2022 Solid understanding of Cloud Computing and DevOps concepts including CI/CD pipelines\n\u2022 Significant experience with ELT data integration and data movement design patterns\n\u2022 Working experience of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc.\n\u2022 Ability and passion for leading tech teams and mentor junior engineers\n\u2022 Your ability to collaborate with other technical and business minds in the organization\n\u2022 Ability to learn and experiment with new technologies and patterns\n\u2022 Your penchant for modern test driven and automation driven software development methodologies\n\u2022 Expertise in converting technology goals into achievable initiatives and Epics and stories\n\u2022 Experience in executing projects in an Agile environment\n\nThe Team\n\nYou will be part of the technology team in Stock Plan Services business unit that administers equity compensation programs on behalf of public and private companies offering various compensations programs such as: Employee Stock Purchase Plans, Restricted Stock Awards/Units, Stock Option Plans, Stock Appreciation Rights and Performance based Awards. Currently SPS services approx. 3M participants employed by over 500 clients and spread out in 150 countries. The business is looking to further expand in the international equity compensation markets and servicing. As part of that we are modernizing our legacy reporting and business intelligence platform to provide data and analytics in real-time running in the cloud.\n\nPlease see below for the salary range for work locations in Colorado only:\nN/A\n\nPlease see below for the salary range for work locations in New York City, Westchester County, NY and Jersey City, NJ only:\nN/A\n\nPlease see below for the salary range for work locations in California only:\nN/A\n\nPlease see below for the salary range for work locations in Washington only:\nN/A\n\nCertifications:\n\nCompany Overview\n\nFidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients\u2019 money.\n\nJoin Us\n\nAt Fidelity, you\u2019ll find endless opportunities to build a meaningful career that positively impacts peoples\u2019 lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees\u2019 Choice Award, we have been recognized by our employees as a Best Place to Work in 2023. And you don\u2019t need a finance background to succeed at Fidelity\u2014we offer a range of opportunities for learning so you can build the career you\u2019ve always imagined.\n\nAt Fidelity, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we\u2019re calling \u201cDynamic Working\u201d. Most associates will have a hybrid schedule with a requirement to work onsite at a Fidelity work location for at least one week, 5 consecutive days, every four weeks. These requirements are subject to change.\n\nWe invite you to Find Your Fidelity at fidelitycareers.com.\n\nFidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.\n\nFidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 3.\n\nAt Fidelity, we value honesty, integrity, and the safety of our associates and customers within a heavily regulated industry. Certain roles may require candidates to go through a preliminary credit check during the screening process. Candidates who are presented with a Fidelity offer will need to go through a background investigation and may be asked to provide additional documentation as requested. This investigation includes but is not limited to a criminal, civil litigations and regulatory review, employment, education, and credit review (role dependent). These investigations will account for 7 years or more of history, depending on the role. Where permitted by federal or state law, Fidelity will also conduct a pre-employment drug screen, which will review for the following substances: Amphetamines, THC (marijuana), cocaine, opiates, phencyclidine.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690329600,
            "job_posted_at_datetime_utc": "2023-07-26T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ailo5hpdYyEAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-23T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1692748800,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 120,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor\u2019s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required",
                    "10+ years of hands-on experience in Data engineering, data warehousing and analytics technologies",
                    "Proven ability with modern Object-Oriented Programming Languages (Python, Scala, Java)",
                    "Advanced skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform",
                    "Strong knowledge of designing data engineering solutions and platforms",
                    "Working experience with Relational Databases like Oracle",
                    "Experience with business intelligence and analytics tools such as OBIEE, PowerBI or Tableau",
                    "Strong working knowledge of cloud native data warehousing and data lake solutions using Snowflake, Redshift etc",
                    "Experience building scalable patterns for data consumption from cloud based data-lakes",
                    "Advanced experience with PL/SQL and complex queries",
                    "Solid understanding of Cloud Computing and DevOps concepts including CI/CD pipelines",
                    "Significant experience with ELT data integration and data movement design patterns",
                    "Working experience of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc",
                    "Ability and passion for leading tech teams and mentor junior engineers",
                    "Your ability to collaborate with other technical and business minds in the organization",
                    "Ability to learn and experiment with new technologies and patterns",
                    "Your penchant for modern test driven and automation driven software development methodologies",
                    "Expertise in converting technology goals into achievable initiatives and Epics and stories",
                    "Experience in executing projects in an Agile environment"
                ],
                "Responsibilities": [
                    "This person will be playing a key role in designing and crafting a modern Data and Information Delivery and Analytics platform in the cloud to support Equity Compensation products for the Global markets",
                    "This person will be working closely with architects and engineers and business SMEs to build and release solutions that help customers get the information they need fast and intuitively"
                ],
                "Benefits": [
                    "You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1132.00 -- Software Developers  Applications"
            ],
            "job_naics_code": "523920",
            "job_naics_name": "Portfolio Management"
        },
        {
            "employer_name": "AT&T",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFKAkrzoWdCGuLo1gJMq5U0N2cfsey1Z7ztAI0&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Career Cast Diversity - CareerCast.com",
            "job_id": "LuuL5FOvfNkAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Principal-Big Data Engineer",
            "job_apply_link": "https://diversity.careercast.com/jobs/principal-big-data-engineer-dallas-tx-75219-135026790-d",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5555,
            "job_description": "JOB LOCATION: 208 S. Akard Street, Dallas, TX 75202 [and various unanticipated locations throughout the U.S.; may work from home]\n\nDUTIES: Interpret the requirements of various big data analytic use cases and scenarios, and drive the design and implementation of specific data models to ultimately help drive better business decisions through insights from a combination of external and AT&T's data assets. Develop necessary enablers and data platform in the big data lake environment and has the responsibility of maintaining its integrity during the life cycle phases. Define data requirements, gather and mine large scale of structured and unstructured data, and validate data by running various data tools in the big data environment. Support the standardization, customization and ad-hoc data analysis, and develop the mechanisms in ingest, analyze, validate, normalize and clean data. Implement statistical data quality procedures on new data sources, and apply rigorous iterative data analytics. Support data scientists in data sourcing and preparation to visualize data and synthesize insights of commercial value. Work with big data policy and security teams and legal to create data policy and develop interfaces and retention models which requires synthesizing or anonymizing data. Develop and maintain data engineering best practices and contribute to insights on data analytics and visualization concepts, methods and techniques. Technical design to ingest data into Palantir Foundry platform on Azure. Participate in creating ingestion strategy and technology patterns. Provide technical direction (architecture and design) for projects ingesting data into Palantir Foundry. Conduct design, architecture and code reviews. Engage with the vendor to meet AT&T requirements and deliverables. Create tasks for Data Replication and Data Synchronization. Utilize Oracle, Teradata, Vertica, Azure DataLake, Databricks and Snowflake. Utilize Hbase and Hbase Shell. Develop UNIX shell scripts. Develop database load scripts: VSQL for Vertica. Utilize BTEQ, Mload, Fastload and fast export scripts for Teradata. Utilize SnowSQL for Snowflake and PySpark for Databricks. Develop schedules using workload scheduling tools: TWS.\n\nREQUIREMENTS: Requires a Masters Degree, or foreign equivalent degree, in Electrical and Electronic Engineering, Computer Science, or Computer Engineering and three (3) years of experience in the job offered or three (3) years of experience in a related occupation creating tasks for Data Replication and Data Synchronization; utilizing Oracle, Teradata, Vertica, Azure DataLake, Databricks, Snowflake and Palantir Foundry; utilizing Hbase and Hbase Shell; developing UNIX shell scripts; developing database load scripts: VSQL for Vertica; utilizing BTEQ, Mload, Fastload and fast export scripts for Teradata; utilizing SnowSQL for Snowflake and PySpark for Databricks; and developing schedules using workload scheduling tools: TWS.\n\nOur Principal-Big Data Engineers earn between $158,200 - $254,300 yearly. Not to mention all the other amazing rewards that working at AT&T offers.\n\nJoining our team comes with amazing perks and benefits:\n\nMedical/Dental/Vision coverage\n\n401(k) plan\n\nTuition reimbursement program\n\nPaid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)\n\nPaid Parental Leave\n\nPaid Caregiver Leave\n\nAdditional sick leave beyond what state and local law require may be available but is unprotected\n\nAdoption Reimbursement\n\nDisability Benefits (short term and long term)\n\nLife and Accidental Death Insurance\n\nSupplemental benefit programs: critical illness/accident hospital indemnity/group legal\n\nEmployee Assistance Programs (EAP)\n\nExtensive employee wellness programs\n\nEmployee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone\n\nAT&T is an Affirmative Action/Equal Opportunity Employer, and we are committed to hiring a diverse and talented workforce. EOE/AA/M/F/D/V\n\u2022 np*\nAT&T will consider for employment qualified applicants in a manner consistent with the requirements of federal, state and local laws\n\nWe expect employees to be honest, trustworthy, and operate with integrity. Discrimination and all unlawful harassment (including sexual harassment) in employment is not tolerated. We encourage success based on our individual merits and abilities without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, disability, marital status, citizenship status, military status, protected veteran status or employment status",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689897600,
            "job_posted_at_datetime_utc": "2023-07-21T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "dental_coverage",
                "health_insurance",
                "paid_time_off"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=LuuL5FOvfNkAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-20T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1692489600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "REQUIREMENTS: Requires a Masters Degree, or foreign equivalent degree, in Electrical and Electronic Engineering, Computer Science, or Computer Engineering and three (3) years of experience in the job offered or three (3) years of experience in a related occupation creating tasks for Data Replication and Data Synchronization; utilizing Oracle, Teradata, Vertica, Azure DataLake, Databricks, Snowflake and Palantir Foundry; utilizing Hbase and Hbase Shell; developing UNIX shell scripts; developing database load scripts: VSQL for Vertica; utilizing BTEQ, Mload, Fastload and fast export scripts for Teradata; utilizing SnowSQL for Snowflake and PySpark for Databricks; and developing schedules using workload scheduling tools: TWS"
                ],
                "Responsibilities": [
                    "DUTIES: Interpret the requirements of various big data analytic use cases and scenarios, and drive the design and implementation of specific data models to ultimately help drive better business decisions through insights from a combination of external and AT&T's data assets",
                    "Develop necessary enablers and data platform in the big data lake environment and has the responsibility of maintaining its integrity during the life cycle phases",
                    "Define data requirements, gather and mine large scale of structured and unstructured data, and validate data by running various data tools in the big data environment",
                    "Support the standardization, customization and ad-hoc data analysis, and develop the mechanisms in ingest, analyze, validate, normalize and clean data",
                    "Implement statistical data quality procedures on new data sources, and apply rigorous iterative data analytics",
                    "Support data scientists in data sourcing and preparation to visualize data and synthesize insights of commercial value",
                    "Work with big data policy and security teams and legal to create data policy and develop interfaces and retention models which requires synthesizing or anonymizing data",
                    "Develop and maintain data engineering best practices and contribute to insights on data analytics and visualization concepts, methods and techniques",
                    "Technical design to ingest data into Palantir Foundry platform on Azure",
                    "Participate in creating ingestion strategy and technology patterns",
                    "Provide technical direction (architecture and design) for projects ingesting data into Palantir Foundry",
                    "Conduct design, architecture and code reviews",
                    "Engage with the vendor to meet AT&T requirements and deliverables",
                    "Create tasks for Data Replication and Data Synchronization",
                    "Develop database load scripts: VSQL for Vertica",
                    "Utilize BTEQ, Mload, Fastload and fast export scripts for Teradata",
                    "Utilize SnowSQL for Snowflake and PySpark for Databricks",
                    "Develop schedules using workload scheduling tools: TWS"
                ],
                "Benefits": [
                    "Our Principal-Big Data Engineers earn between $158,200 - $254,300 yearly",
                    "Medical/Dental/Vision coverage",
                    "401(k) plan",
                    "Tuition reimbursement program",
                    "Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)",
                    "Paid Parental Leave",
                    "Paid Caregiver Leave",
                    "Additional sick leave beyond what state and local law require may be available but is unprotected",
                    "Adoption Reimbursement",
                    "Disability Benefits (short term and long term)",
                    "Life and Accidental Death Insurance",
                    "Supplemental benefit programs: critical illness/accident hospital indemnity/group legal",
                    "Extensive employee wellness programs",
                    "Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "Telecommunications"
            ]
        },
        {
            "employer_name": "PRIMUS Global Services, Inc",
            "employer_logo": null,
            "employer_website": "http://www.primusglobal.com",
            "employer_company_type": null,
            "job_publisher": "Indeed",
            "job_id": "epgFodzuXh8AAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Software Developer/Data Engineer \u2013 Apache Spark, SQL, Kafka Connect \u2013 Dallas, TX (Hybrid) 44700",
            "job_apply_link": "https://www.indeed.com/viewjob?jk=f14fb7a2b85d5796",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5816,
            "job_description": "We have an immediate long-term opportunity with one of our prime clients for a position of Software Developer/Data Engineer to work in Dallas, TX on a hybrid basis.\n\nAs a Software Developer/Data Engineer, you will be responsible for developing and implementing data engineering solutions, with a focus on utilizing Apache Spark, GCP (Google Cloud Platform), Azure, Databricks (batch processing), SQL, Kafka Connect (Java), and data structures in Java.\n\nRequirements:\n\nProven experience as a Software Developer/Data Engineer with expertise in Apache Spark, GCP, Azure, Databricks, SQL, Kafka Connect (Java), and Java data structures. Strong knowledge of data engineering concepts, data integration, and data processing. Experience with ETL development and data pipelines. Familiarity with cloud platforms like GCP and Azure for data storage and processing.\n\u2022 *ALL successful candidates for this position are required to work directly for PRIMUS. No agencies please**\n\nFor immediate consideration, please contact:\n\nPavan\nPRIMUS Global Services\nDirect: 972-798-2661\nDesk: 972-753-6500 Ext: 203\nEmail: jobs@primusglobal.com",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690497100,
            "job_posted_at_datetime_utc": "2023-07-27T22:31:40.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=epgFodzuXh8AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Proven experience as a Software Developer/Data Engineer with expertise in Apache Spark, GCP, Azure, Databricks, SQL, Kafka Connect (Java), and Java data structures",
                    "Strong knowledge of data engineering concepts, data integration, and data processing",
                    "Experience with ETL development and data pipelines",
                    "Familiarity with cloud platforms like GCP and Azure for data storage and processing",
                    "*ALL successful candidates for this position are required to work directly for PRIMUS"
                ],
                "Responsibilities": [
                    "As a Software Developer/Data Engineer, you will be responsible for developing and implementing data engineering solutions, with a focus on utilizing Apache Spark, GCP (Google Cloud Platform), Azure, Databricks (batch processing), SQL, Kafka Connect (Java), and data structures in Java"
                ]
            },
            "job_job_title": "Engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Rangam Consultants Inc.",
            "employer_logo": "https://res.cloudinary.com/crunchbase-production/image/upload/c_lpad,h_256,w_256,f_auto,q_auto:eco,dpr_1/hum73a6yro6ex9u5d2ch",
            "employer_website": "http://rangam.com",
            "employer_company_type": null,
            "job_publisher": "Rangam",
            "job_id": "A2IULA4j8_8AAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Data Engineer",
            "job_apply_link": "https://rangam.com/jobs/jobdetails/110586/data-engineer-dallas-tx-us",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6779,
            "job_description": "Data Engineer-Contract to hire intention\n\nData Engineering with SQL, Python\nExperience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\nAt least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\nAt least 2 years of experience with AWS cloud (with focus on Data services)\n\nU.S. citizenship required\n\u2022 The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support.\n\u2022 You will develop, test, and maintain data or report solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages.\n\u2022 You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality.\n\nAs the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members.\n\u2022 An associate degree, a bachelor\u2019s degree in computer science or equivalent courses\n\u2022 At least 4 years of experience in Data Engineering with SQL, Python\n\u2022 Experience with relational SQL and NoSQL databases\n\u2022 Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\n\u2022 At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\n\u2022 At least 2 years of experience with AWS cloud (with focus on Data services)\n\u2022 Experience with Databricks a plus\n\u2022 Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs\n\u2022 Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix/Linux shell scripting\n\u2022 Able to interpret user requirements and identify additional information needed in user requirements\n\u2022 Able to see effects of current design with future requirements and see possible coding solutions to meet the requirements\n\u2022 Detailed understanding of logical and physical data structures\n\u2022 Highly skilled in tools, evaluates the need for various tools for continuous integration, testing, automation, deployment etc. and discuss with the team\n\u2022 Highly skilled at designing tests for unfamiliar designs; Evaluates tests for weaknesses and continuously improves them\n\u2022 Detailed understanding of how to effectively test against multiple tools/software\n\u2022 Equivalent education and/or experience may be substituted for any of the above requirements\n\nResponsibilities:\n\u2022 Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns\n\u2022 Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity\n\u2022 Work under general guidance and clear framework of accountability with substantial autonomy\n\u2022 Use best practices and knowledge of internal or external business issues to improve products or services\n\u2022 Solve complex problems; takes a new perspective using existing solutions",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690296823,
            "job_posted_at_datetime_utc": "2023-07-25T14:53:43.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=A2IULA4j8_8AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2024-08-27T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1724716800,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 48,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)",
                    "At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)",
                    "At least 2 years of experience with AWS cloud (with focus on Data services)",
                    "U.S. citizenship required",
                    "The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support",
                    "An associate degree, a bachelor\u2019s degree in computer science or equivalent courses",
                    "At least 4 years of experience in Data Engineering with SQL, Python",
                    "Experience with relational SQL and NoSQL databases",
                    "Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs",
                    "Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix/Linux shell scripting",
                    "Highly skilled in tools, evaluates the need for various tools for continuous integration, testing, automation, deployment etc",
                    "Equivalent education and/or experience may be substituted for any of the above requirements"
                ],
                "Responsibilities": [
                    "As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members",
                    "Able to interpret user requirements and identify additional information needed in user requirements",
                    "Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns",
                    "Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity",
                    "Work under general guidance and clear framework of accountability with substantial autonomy",
                    "Use best practices and knowledge of internal or external business issues to improve products or services",
                    "Solve complex problems; takes a new perspective using existing solutions"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Randstad US",
            "employer_logo": "https://static.rusacdn.com/images/schema.org/hiringOrganization/randstadlogo.png",
            "employer_website": "http://www.randstadusa.com",
            "employer_company_type": "Staffing",
            "job_publisher": "Nexxt",
            "job_id": "iGjbaz-JoBcAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "data engineer",
            "job_apply_link": "https://www.nexxt.com/jobs/data-engineer-dallas-tx-2557826868-job.html?aff=2ED44C72-8FD2-4B5D-BC54-2F623E88BE26",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4909,
            "job_description": "data engineer.\n\u2022 dallas , texas\n\u2022 posted today\n\njob details\n\nsummary\n\u2022 $62 - $72 per hour\n\u2022 contract\n\u2022 bachelor degree\n\u2022 category computer and mathematical occupations\n\u2022 reference1020356\n\njob details\n\njob summary\n\nRequired Qualifications -\n\u2022 Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field.\n\u2022 Option 2: 6 years' experience in software engineering or related field.\n\u2022 Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field. 3 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nNice to have soft skills -\n\u2022 7+ years of experience with 3+ years of Big data development experience\n\u2022 Experience in HDFS, Hive, Hive UDF's, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix.\n\u2022 Demonstrates expertise in writing complex, highly optimized queries across large data sets\n\u2022 Retail experience and knowledge of commercial data is a huge plus\n\u2022 Experience with BI Tool Tableau or Looker is a plus\n\nlocation: Dallas, Texas\n\njob type: Contract\n\nsalary: $62 - 72 per hour\n\nwork hours: 8am to 5pm\n\neducation: Bachelors\n\nresponsibilities\n\nDesigns, develops, and implements Hadoop eco-system based applications to support business requirements.\n\nFollows approved life cycle methodologies, creates design documents, and performs program coding and testing.\n\nResolves technical issues through debugging, research, and investigation\n\nqualifications\n\u2022 Experience level: Experienced\n\u2022 Minimum 4 years of experience\n\u2022 Education: Bachelors\n\nskills\n\u2022 SQL\n\u2022 Python\n\u2022 TableauEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group ~~~ Randstad, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants. If you require a reasonable accommodation to make your application or interview experience a great one, please contact ~~~ offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc. In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility).For certain assignments, Covid-19 vaccination and/or testing may be required by Randstad's client or applicable federal mandate, subject to approved medical or religious accommodations. Carefully review the job posting for details on vaccine/testing requirements or ask your Randstad representative for more information.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690304753,
            "job_posted_at_datetime_utc": "2023-07-25T17:05:53.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "dental_coverage",
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=iGjbaz-JoBcAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field",
                    "Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field",
                    "3 years' experience in data engineering, database engineering, business intelligence, or business analytics",
                    "Nice to have soft skills -",
                    "7+ years of experience with 3+ years of Big data development experience",
                    "Experience in HDFS, Hive, Hive UDF's, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix",
                    "Demonstrates expertise in writing complex, highly optimized queries across large data sets",
                    "Retail experience and knowledge of commercial data is a huge plus",
                    "Experience level: Experienced",
                    "Minimum 4 years of experience",
                    "Education: Bachelors",
                    "SQL",
                    "TableauEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group ~~~ Randstad, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants"
                ],
                "Responsibilities": [
                    "work hours: 8am to 5pm",
                    "Designs, develops, and implements Hadoop eco-system based applications to support business requirements",
                    "Follows approved life cycle methodologies, creates design documents, and performs program coding and testing",
                    "Resolves technical issues through debugging, research, and investigation"
                ],
                "Benefits": [
                    "salary: $62 - 72 per hour",
                    "In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility)"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "561311",
            "job_naics_name": "Employment Placement Agencies"
        },
        {
            "employer_name": "LTIMindtree",
            "employer_logo": "https://www.ltimindtree.com/wp-content/uploads/2022/10/LTIMindtree_Linear_2-1-LT-Blue-1-1.png",
            "employer_website": "http://www.lntinfotech.com",
            "employer_company_type": "Computer Services",
            "job_publisher": "ZipRecruiter",
            "job_id": "jAYPkmjSPLIAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Associate Principal -Data Engineering",
            "job_apply_link": "https://www.ziprecruiter.com/c/LTIMindtree/Job/Associate-Principal-Data-Engineering/-in-Dallas,TX?jid=244b1ab0515b9704",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6232,
            "job_description": "Technical Skills:\n\nDesigns, implements, and documents data architecture and data modeling solutions, which include the use of relational, dimensional\n\nResponsible for the development of the conceptual, logical, and physical data models\n\nImplementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms\n\nDefine and govern data modeling and design standards, tools, best practices, and related development for enterprise data models\n\nHands-on modeling, design, configuration, installation, performance tuning,\n\nExperience in Data Vault modeling approach\n\nExtensive knowledge of Snowflake.\n\nGood knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required\n\nWork closely with the database engineers to create optimal physical data models of datasets\n\nLeadership Skills & Stakeholder Management:\n\nWork very closely with customer stakeholders to understand their needs and make recommendations.\n\nInteract with SMEs (business) and Architect to validate whether the data model is aligned with the technology suggested for the project.\n\nRelay the information gained from onshore to the team and offshore for seamless collaboration and desired outcome.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690467480,
            "job_posted_at_datetime_utc": "2023-07-27T14:18:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=jAYPkmjSPLIAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693008000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience in Data Vault modeling approach",
                    "Extensive knowledge of Snowflake",
                    "Good knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required"
                ],
                "Responsibilities": [
                    "Responsible for the development of the conceptual, logical, and physical data models",
                    "Implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms",
                    "Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models",
                    "Hands-on modeling, design, configuration, installation, performance tuning,",
                    "Interact with SMEs (business) and Architect to validate whether the data model is aligned with the technology suggested for the project",
                    "Relay the information gained from onshore to the team and offshore for seamless collaboration and desired outcome"
                ]
            },
            "job_job_title": "Principal",
            "job_posting_language": "en",
            "job_onet_soc": "11903200",
            "job_onet_job_zone": "5",
            "job_occupational_categories": [
                "15-1199.06: Database Architects"
            ],
            "job_naics_code": "541511",
            "job_naics_name": "Custom Computer Programming Services"
        },
        {
            "employer_name": "Incedo Inc.",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQr1KKB1pa3bQ_NeHM2VwFtfQfSY4V_KClcRf2w&s=0",
            "employer_website": "http://www.incedoinc.com",
            "employer_company_type": null,
            "job_publisher": "LinkedIn",
            "job_id": "_HaktzqqbQEAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-incedo-inc-3675626919",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7006,
            "job_description": "Job Description: Data Engineer (Python and GCP)\n\nLocation: Alpharetta, GA and Dallas, TX\n\nIncedo is a US-based consulting, analytics, and technology services company. We help our clients achieve competitive advantage through End-to-End Digital Transformation and work across Financial Services, Telecom, Life Science & Healthcare, and Product Engineering sectors.\n\nIncedo Software Solution Architect/Solution Lead play a critical role in developing solutions for client engagements. They are detail-driven with a strong technical background in their domain and excellent problem-solving skills. An ideal candidate will have experience in coding large-scale, responsive web sites with an aim towards performance and progressive enhancement. Incedo teams are cross-functional and may be geographically distributed across the US and India.\n\nJoin our team and help develop software to monitor and manage next-generation one of the telecom networks. We are looking to fill out a Software Engineer position requiring solid knowledge of software engineering and architectural best practices.\n\nYou will work on the Big Data platform at the one of the telecom networks scale (hundreds of terabytes of data per day). You will design and develop solutions for Big Data challenges, working with applications deployed on-premise and in the Google Cloud.\n\nQualified candidates should have solid coding experience and be eager and willing to learn. You will be asked to prove your skills during the technical interview.\n\nRequired qualifications:\n\nTechnical lead with data science experience who can also support the team from an architectural standpoint.\n\n\u2022 - GCP\n\n\u2022 - Python\n\n\u2022 - Feature Engineering\n\n\u2022 - Model Selection/Deployments\n\n\u2022 - Solution Architect Experience\n\n\u2022 B.S. in Computer Science or related field, with 10 years\u2019 work experience.\n\nGood to have:\n\n\u2022 Hands-on experience working in Unix/Linux ecosystem\n\n\u2022 Experience with developing or maintaining ETL pipelines in a Big Data Hadoop environment.\n\n\u2022 Experience in setting up and maintaining CI/CD infrastructure.\n\n\u2022 Experience with test automation.\n\nSudha Ray\n\nDirect: (856) 679 0433 | Email:sudha.ray@incedoinc.com",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690319689,
            "job_posted_at_datetime_utc": "2023-07-25T21:14:49.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=_HaktzqqbQEAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-24T21:14:48.000Z",
            "job_offer_expiration_timestamp": 1692911688,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 120,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "An ideal candidate will have experience in coding large-scale, responsive web sites with an aim towards performance and progressive enhancement",
                    "We are looking to fill out a Software Engineer position requiring solid knowledge of software engineering and architectural best practices",
                    "Qualified candidates should have solid coding experience and be eager and willing to learn",
                    "You will be asked to prove your skills during the technical interview",
                    "Technical lead with data science experience who can also support the team from an architectural standpoint",
                    "- GCP",
                    "- Solution Architect Experience",
                    "B.S. in Computer Science or related field, with 10 years\u2019 work experience",
                    "Hands-on experience working in Unix/Linux ecosystem",
                    "Experience with developing or maintaining ETL pipelines in a Big Data Hadoop environment",
                    "Experience in setting up and maintaining CI/CD infrastructure",
                    "Experience with test automation"
                ],
                "Responsibilities": [
                    "Incedo Software Solution Architect/Solution Lead play a critical role in developing solutions for client engagements",
                    "You will work on the Big Data platform at the one of the telecom networks scale (hundreds of terabytes of data per day)",
                    "You will design and develop solutions for Big Data challenges, working with applications deployed on-premise and in the Google Cloud"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Walmart",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Walmart_logo.svg/2560px-Walmart_logo.svg.png",
            "employer_website": "https://www.walmart.com",
            "employer_company_type": "Retail",
            "job_publisher": "Walmart Careers",
            "job_id": "4YnqmBaYvqMAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "(USA) Senior Data Engineer - Data Ventures",
            "job_apply_link": "https://careers.walmart.com/us/jobs/WD1562366-usa-senior-data-engineer-data-ventures",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7968,
            "job_description": "Position Summary...\n\nWhat you'll do...\n\nDo you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart\u2019s environment comes the biggest of big data sets. As a Walmart Data Engineer, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.\u202fYou will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers\u2019 lives.\n\nWhat you'll do:\n\u2022 Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.\n\u2022 Data Transformation and Integration: Extracts data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current analytics trends.\n\u2022 Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.\n\u2022 Data Modeling: Analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyses data-related system integration challenges and proposes appropriate solutions.\n\u2022 Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbook, and provides timely progress updates.\n\u2022 Problem Formulation: Translates business problems within one's discipline to data related or mathematical solutions. Identifies what methods (for example, analytics, big data analytics, automation) would provide a solution for the problem. Shares use cases and gives examples to demonstrate how the method would solve the business problem.\n\u2022 Applied Business Acumen: Provides recommendations to business stakeholders to solve complex business issues. Develops business cases for projects with a projected return on investment or cost savings. Translates business requirements into projects, activities, and tasks and aligns to overall business strategy. Serves as an interpreter and conduit to connect business needs with tangible solutions and results. Recommends new processes and ways of working.\n\u2022 Data Governance: Establishes, modifies, and documents data governance projects and recommendations. Implements data governance practices in partnership with business stakeholders and peers. Interprets company and regulatory policies on data. Educates others on data governance processes, practices, policies, and guidelines. Provides recommendations on needed updates or inputs into data governance policies, practices, or guidelines.\n\u2022 Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others. Supporting and aligning efforts to meet customer and business needs and building commitment for perspectives and rationales.\n\u2022 Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders. Identifying business needs, determining, and carrying out necessary processes and practices.\n\u2022 Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application, ensuring compliance with them.\n\u2022 Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives. Applying suggestions for improving efficiency and cost effectiveness; and participating in and supporting community outreach events.\n\u2022 Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.\n\u2022 Drives the execution of multiple business plans and projects by identifying customer and operational needs. Developing and communicating business plans and priorities, removing barriers and obstacles that impact performance. Providing resources, identifying performance standards, measuring progress, and adjusting performance accordingly. Developing contingency plans and demonstrating adaptability and supporting continuous learning.\n\nWhat you'll bring:\n\u2022 You have consistently high standards, your passion for quality is inherent in everything.\n\u2022 Well versed with Hadoop, Spark, Cloud, Python/Scala and Java, Streaming, Kafka, Backend, J2EE.\n\u2022 You evangelize an extremely high standard of code quality, system reliability, and performance.\n\u2022 You have a proven track record coding with at least one programming language (e.g., Scala, Python)\n\u2022 You\u2019re experienced in one of cloud computing platforms (e.g., GCP, Azure)\n\u2022 You\u2019re skilled in data modeling & data migration protocols.\n\u2022 Experience with GCP, Data warehousing, BI preferred\n\u2022 Experience with the integration tools like Automic, Airflow\n\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That\u2019s what we do at Walmart Global Tech. We\u2019re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world\u2019s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work:\nWe use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.\n\nBenefits:\nBenefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer \u2013 By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas and opinions \u2013 while being inclusive of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor\u2019s degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years\u2019 experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science and 1 year\u2019s experience in software engineering or related\nfield.\n2 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nMaster\u2019s degree in Computer Science, Computer Engineering, Computer Information Systems, Software Engineering, or related area and 1 year's experience in software engineering or related area.\n\nPrimary Location...\n603 MUNGER AVE STE 400, DALLAS, TX 75202, United States of America",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690156800,
            "job_posted_at_datetime_utc": "2023-07-24T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "retirement_savings",
                "paid_time_off"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=4YnqmBaYvqMAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "You have consistently high standards, your passion for quality is inherent in everything",
                    "Well versed with Hadoop, Spark, Cloud, Python/Scala and Java, Streaming, Kafka, Backend, J2EE",
                    "You evangelize an extremely high standard of code quality, system reliability, and performance",
                    "You have a proven track record coding with at least one programming language (e.g., Scala, Python)",
                    "You\u2019re experienced in one of cloud computing platforms (e.g., GCP, Azure)",
                    "You\u2019re skilled in data modeling & data migration protocols",
                    "Experience with the integration tools like Automic, Airflow",
                    "Option 1: Bachelor\u2019s degree in Computer Science and 3 years' experience in software engineering or related field",
                    "Option 2: 5 years\u2019 experience in",
                    "2 years' experience in data engineering, database engineering, business intelligence, or business analytics",
                    "Master\u2019s degree in Computer Science, Computer Engineering, Computer Information Systems, Software Engineering, or related area and 1 year's experience in software engineering or related area"
                ],
                "Responsibilities": [
                    "You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.\u202fYou will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers\u2019 lives",
                    "Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function",
                    "Data Transformation and Integration: Extracts data from identified databases",
                    "Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques",
                    "Develops knowledge of current analytics trends",
                    "Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements",
                    "Helps identify the most suitable source for data that is fit for purpose",
                    "Performs initial data quality checks on extracted data",
                    "Data Modeling: Analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models",
                    "Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs",
                    "Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure",
                    "Evaluates existing data models and physical databases for variances and discrepancies",
                    "Develops efficient data flows",
                    "Analyses data-related system integration challenges and proposes appropriate solutions",
                    "Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical and data requirements",
                    "Creates test cases to review and validate the proposed solution design",
                    "Creates proofs of concept",
                    "Tests the code using the appropriate testing approach",
                    "Deploys software to production servers",
                    "Contributes code documentation, maintains playbook, and provides timely progress updates",
                    "Problem Formulation: Translates business problems within one's discipline to data related or mathematical solutions",
                    "Identifies what methods (for example, analytics, big data analytics, automation) would provide a solution for the problem",
                    "Shares use cases and gives examples to demonstrate how the method would solve the business problem",
                    "Applied Business Acumen: Provides recommendations to business stakeholders to solve complex business issues",
                    "Develops business cases for projects with a projected return on investment or cost savings",
                    "Translates business requirements into projects, activities, and tasks and aligns to overall business strategy",
                    "Serves as an interpreter and conduit to connect business needs with tangible solutions and results",
                    "Recommends new processes and ways of working",
                    "Data Governance: Establishes, modifies, and documents data governance projects and recommendations",
                    "Implements data governance practices in partnership with business stakeholders and peers",
                    "Interprets company and regulatory policies on data",
                    "Educates others on data governance processes, practices, policies, and guidelines",
                    "Provides recommendations on needed updates or inputs into data governance policies, practices, or guidelines",
                    "Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others",
                    "Supporting and aligning efforts to meet customer and business needs and building commitment for perspectives and rationales",
                    "Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders",
                    "Identifying business needs, determining, and carrying out necessary processes and practices",
                    "Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application, ensuring compliance with them",
                    "Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives",
                    "Applying suggestions for improving efficiency and cost effectiveness; and participating in and supporting community outreach events",
                    "Creates training documentation and trains end-users on data modeling",
                    "Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports",
                    "Drives the execution of multiple business plans and projects by identifying customer and operational needs",
                    "Developing and communicating business plans and priorities, removing barriers and obstacles that impact performance",
                    "Providing resources, identifying performance standards, measuring progress, and adjusting performance accordingly",
                    "Developing contingency plans and demonstrating adaptability and supporting continuous learning"
                ],
                "Benefits": [
                    "Benefits: Beyond our great compensation package, you can receive incentive awards for your performance",
                    "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more"
                ]
            },
            "job_job_title": null,
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "452990",
            "job_naics_name": "All Other General Merchandise Stores"
        },
        {
            "employer_name": "Randstad USA",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Monster",
            "job_id": "Awvrb4W-sPUAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.monster.com/job-openings/data-engineer-dallas-tx--53f94751-4d3a-4f39-9305-392e3510b4b8?mstr_dist=true",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5581,
            "job_description": "job summary:\n\nRandstad Technologies has an active need with one of our long-standing end customer, a leading Financial company located in the heart of Dallas. This will be a twelve month long contract with the intent to convert to full time. Our client is looking to add a Data Engineer to support their organization.\n\nlocation: DALLAS, Texas\njob type: Contract\nsalary: $50 - 55 per hour\nwork hours: 8am to 5pm\neducation: Associates\n\nresponsibilities:\n\nResponsibilities\n\n- Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns\n\n- Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity\n\n- Work under general guidance and clear framework of accountability with substantial autonomy\n\n- Use best practices and knowledge of internal or external business issues to improve products or services\n\n- Solve complex problems; takes a new perspective using existing solutions\n\nqualifications:\n\u2022 Experience level: Experienced\n\u2022 Minimum 4 years of experience\n\u2022 Education: Associates\n\nskills:\n\u2022 data engineer\n\u2022 SQL (4 years of experience is required)\n\u2022 Python (4 years of experience is required)\n\u2022 Big Data\n\u2022 AWS\n\u2022 RESTful API\n\u2022 UNIX\n\u2022 Linux\n\u2022 Data Warehouse\n\nEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.\n\nAt Randstad, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants. If you require a reasonable accommodation to make your application or interview experience a great one, please contact HRsupport@randstadusa.com.\n\nPay offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc. In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility).\n\nFor certain assignments, Covid-19 vaccination and/or testing may be required by Randstad's client or applicable federal mandate, subject to approved medical or religious accommodations. Carefully review the job posting for details on vaccine/testing requirements or ask your Randstad representative for more information.\n\nAbout the Company:\nRandstad USA\n\nRandstad was founded in 1960 by Frits Goldschmeding. We've never let go of his passion or the values that he established. By staying true to those fundamentals, we've expanded to represent more than 90 percent of the HR services market.\n\nWe provide outsourcing, staffing, consulting and workforce solutions within the areas of engineering, accounting and finance, healthcare, human resources, IT, legal, life sciences, manufacturing and logistics, office and administration and sales and marketing. We can\u2019t wait to tell you all about it.\n\nOur mission is to be a world leader in matching demand for, and supply of, labor and HR services. We believe in the value of work as a unifying force that shapes society for the better. We live by the core values established early in our company's history: to know, serve and trust, striving for perfection and simultaneous promotion of all interests.\n\nCompany Size:\n10,000 employees or more\n\nIndustry:\nStaffing/Employment Agencies\n\nFounded:\n1960\n\nWebsite:\nhttps://www.randstadusa.com/",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690329600,
            "job_posted_at_datetime_utc": "2023-07-26T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "retirement_savings",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Awvrb4W-sPUAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 48,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience level: Experienced",
                    "Minimum 4 years of experience",
                    "Education: Associates",
                    "data engineer",
                    "SQL (4 years of experience is required)",
                    "Big Data",
                    "AWS",
                    "UNIX",
                    "Linux",
                    "Data Warehouse"
                ],
                "Responsibilities": [
                    "Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns",
                    "Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity",
                    "Work under general guidance and clear framework of accountability with substantial autonomy",
                    "Use best practices and knowledge of internal or external business issues to improve products or services",
                    "Solve complex problems; takes a new perspective using existing solutions"
                ],
                "Benefits": [
                    "salary: $50 - 55 per hour",
                    "Pay offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc",
                    "In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility)"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Costco Wholesale",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Costco_Wholesale_logo_2010-10-26.svg/1280px-Costco_Wholesale_logo_2010-10-26.svg.png",
            "employer_website": "https://www.costco.com",
            "employer_company_type": null,
            "job_publisher": "Glassdoor",
            "job_id": "eAOHe-qYmRwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer - Data Science & Analytics",
            "job_apply_link": "https://www.glassdoor.com/job-listing/data-engineer-data-science-and-analytics-costco-wholesale-JV_IC1139977_KO0,40_KE41,57.htm?jl=1008427328488",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5467,
            "job_description": "This is an environment unlike anything in the high-tech world and the secret of Costco\u2019s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others. In 2021, Costco contributed over $58 million to organizations such as United Way and Children's Miracle Network Hospitals.\n\nCostco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks seventh in Forbes \u201cWorld\u2019s Best Employers\u201d.\n\nThe Data Engineer - Data Analytics is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.\n\nIf you want to be a part of one of the worldwide BEST companies \u201cto work for\u201d, simply apply and let your career be reimagined.\n\nROLE\n\u2022 Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services).\n\u2022 Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.\n\u2022 Designs, develops, and implements ETL/ELT processes using IICS (informatica cloud).\n\u2022 Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.\n\u2022 Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.\n\u2022 Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.\n\u2022 Identifies ways to improve data reliability, efficiency, and quality of data management.\n\u2022 Communicates technical concepts to non-technical audiences both written and verbal.\n\u2022 Performs peer reviews for other data engineer\u2019s work.\n\nREQUIRED\n\u2022 5+ years\u2019 experience engineering and operationalizing data pipelines with large and complex datasets.\n\u2022 5+ years\u2019 of hands on experience with Informatica PowerCenter.\n\u2022 2+ years\u2019 of hands on experience with Informatica IICS.\n\u2022 3+ years\u2019 experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.\n\u2022 5+ years\u2019 experience with Data Modeling, ETL, and Data Warehousing.\n\u2022 2+ years\u2019 hands on experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.\n\u2022 3+ years\u2019 hands on experience with Git / Azure DevOps\n\u2022 Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML.\n\u2022 Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources.\n\u2022 Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing.\n\u2022 Able to work in a fast-paced agile development environment.\n\nRecommended\n\u2022 Microsoft Azure/similar certifications.\n\u2022 Experience delivering data solutions through agile software development methodologies.\n\u2022 Exposure to the retail industry.\n\u2022 Excellent verbal and written communication skills.\n\u2022 Experience working with SAP integration tools including BODS.\n\u2022 Experience with UC4 Job Scheduler.\n\u2022 BA/BS in Computer Science, Engineering, or equivalent software/services experience.\n\nRequired Documents\n\u2022 Cover Letter\n\u2022 Resume\n\u2022 Last two performance reviews\n\u2022 Attendance records for current year (Do not include absences covered by paid sick/personal time, FMLA or other protected absences.)\n\nCalifornia applicants, please click here to review the Costco Applicant Privacy Notice.\n\nApart from any religious or disability considerations, open availability is needed to meet the needs of the business. If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.\n\nPay Ranges:\n\nLevel 2 - $100,000 - $135,000,\n\nLevel 3 - $125,000 - $165,000\n\nLevel 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible\n\nWe offer a comprehensive package of benefits including paid time off, health benefits \u2014 medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees.\n\nCostco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com\n\nIf hired, you will be required to provide proof of authorization to work in the United States.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690156800,
            "job_posted_at_datetime_utc": "2023-07-24T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "dental_coverage",
                "paid_time_off",
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=eAOHe-qYmRwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 100000,
            "job_max_salary": 135000,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "5+ years\u2019 experience engineering and operationalizing data pipelines with large and complex datasets",
                    "5+ years\u2019 of hands on experience with Informatica PowerCenter",
                    "2+ years\u2019 of hands on experience with Informatica IICS",
                    "3+ years\u2019 experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies",
                    "5+ years\u2019 experience with Data Modeling, ETL, and Data Warehousing",
                    "2+ years\u2019 hands on experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL",
                    "3+ years\u2019 hands on experience with Git / Azure DevOps",
                    "Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML",
                    "Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources",
                    "Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing",
                    "Able to work in a fast-paced agile development environment",
                    "Microsoft Azure/similar certifications",
                    "Experience delivering data solutions through agile software development methodologies",
                    "Exposure to the retail industry",
                    "Excellent verbal and written communication skills",
                    "Experience working with SAP integration tools including BODS",
                    "Experience with UC4 Job Scheduler",
                    "BA/BS in Computer Science, Engineering, or equivalent software/services experience",
                    "Last two performance reviews",
                    "Attendance records for current year (Do not include absences covered by paid sick/personal time, FMLA or other protected absences.)",
                    "If hired, you will be required to provide proof of authorization to work in the United States"
                ],
                "Responsibilities": [
                    "This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources",
                    "The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth",
                    "Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services)",
                    "Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration",
                    "Designs, develops, and implements ETL/ELT processes using IICS (informatica cloud)",
                    "Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services",
                    "Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization",
                    "Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery",
                    "Identifies ways to improve data reliability, efficiency, and quality of data management",
                    "Communicates technical concepts to non-technical audiences both written and verbal",
                    "Performs peer reviews for other data engineer\u2019s work"
                ],
                "Benefits": [
                    "Level 2 - $100,000 - $135,000,",
                    "Level 3 - $125,000 - $165,000",
                    "Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible",
                    "We offer a comprehensive package of benefits including paid time off, health benefits \u2014 medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15111100",
            "job_onet_job_zone": "5"
        },
        {
            "employer_name": "Randstad North America, Inc.",
            "employer_logo": "https://mma.prnewswire.com/media/1158927/Randstad_logo.jpg?p=twitter",
            "employer_website": "http://www.randstadusa.com",
            "employer_company_type": null,
            "job_publisher": "Jobrapido.com",
            "job_id": "xlranAfS5CkAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://us.jobrapido.com/jobpreview/2962679032",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4492,
            "job_description": "Required Qualifications - Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years' experience in software engineering or related field. Option 3: Master's degree in Computer Data Engineer, Software Engineer, Computer Science, Database Engineer, Technology, Staffing, Engineer",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690329600,
            "job_posted_at_datetime_utc": "2023-07-26T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=xlranAfS5CkAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Required Qualifications - Option 1: Bachelor's degree in Computer Science and 4 years' experience in software engineering or related field",
                    "Option 2: 6 years' experience in software engineering or related field",
                    "Option 3: Master's degree in Computer Data Engineer, Software Engineer, Computer Science, Database Engineer, Technology, Staffing, Engineer"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Apexon",
            "employer_logo": "https://mms.businesswire.com/media/20220726005920/en/1524777/23/Apexon_Logo_Colour_CMYK.jpg",
            "employer_website": "http://www.apexon.com",
            "employer_company_type": null,
            "job_publisher": "Salary.com",
            "job_id": "H4fbvCM4qssAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "AWS Data Engineer",
            "job_apply_link": "https://www.salary.com/job/apexon/aws-data-engineer/j202307210211154619759",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5713,
            "job_description": "Job Description\n\nTechnical skills required:\n\u2022 Experience with big data tools like Hadoop, Spark, Kafka, fink, Hive, Sqoop etc.\n\u2022 Experience with relational SQL and NoSQL databases like Mysql, Postgres, Mongodb and Cassandra. Experience with data pipeline tools like Airflow, etc.\n\u2022 Experience with AWS cloud services like: EC2, S3, EMR ETC\n\u2022 Experience with stream-processing systems like: Storm, Spark-Streaming, Flink etc.\n\u2022 Experience with object-oriented/object function scripting languages: Python, Java, C , Scala, etc.\n\u2022 Dice Id: tecnos\n\u2022 Position Id: 7970708\n\u2022",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689984000,
            "job_posted_at_datetime_utc": "2023-07-22T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=H4fbvCM4qssAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2024-01-21T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1705795200,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience with big data tools like Hadoop, Spark, Kafka, fink, Hive, Sqoop etc",
                    "Experience with relational SQL and NoSQL databases like Mysql, Postgres, Mongodb and Cassandra",
                    "Experience with data pipeline tools like Airflow, etc",
                    "Experience with AWS cloud services like: EC2, S3, EMR ETC",
                    "Experience with stream-processing systems like: Storm, Spark-Streaming, Flink etc",
                    "Experience with object-oriented/object function scripting languages: Python, Java, C , Scala, etc"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "AT&T Services, Inc.",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS500weGU8Dx9ZPZosQM_sMearUR_pwAnRmX7Ms&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Professional Diversity Network",
            "job_id": "vw-9JOTKx18AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Principal- Big Data Engineer",
            "job_apply_link": "https://www.prodivnet.com/job/principal-big-data-engineer-dallas-texas-13312115",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5681,
            "job_description": "Principal- Big Data Engineer\nAT&T Services, Inc.\nDallas, TX\nresponsible for interpreting the requirements of various Big Data analytics use cases and scenarios. Apply at\nhttp://att.jobs/, select\nJOB SEARCH and APPLY and select Search by Requisition Number at the left bottom\nof the page and enter\nJob Number: 2317733\n\nrecblid j66a6lkvh4wdbjyy95ehj6kmwz3dny\n\nPDN-99b82a39-fa51-469f-ad0d-7cad136b408b",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690205878,
            "job_posted_at_datetime_utc": "2023-07-24T13:37:58.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=vw-9JOTKx18AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-10-22T13:37:58.000Z",
            "job_offer_expiration_timestamp": 1697981878,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Responsibilities": [
                    "responsible for interpreting the requirements of various Big Data analytics use cases and scenarios"
                ]
            },
            "job_job_title": "Principal",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Galaxy i Technologies, Inc.",
            "employer_logo": "https://galaxyitech.com/wp-content/uploads/2023/04/galaxy-logo.gif",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Dice",
            "job_id": "Xxd017dEv5YAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "DATA ENGINEER",
            "job_apply_link": "https://www.dice.com/job-detail/3c29ce84-8ce6-41cb-9d94-f9425b9662e2",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5622,
            "job_description": "DATA ENGINEER\n\nW2 CONTRACT\n\nMA,UT,TX,NC,NH\n\nHYBRID\n\nJOB DESCRIPTION:\n\u2022 5+ years of relevant experience in data analytics or process improvement\n\u2022 Bachelor's degree (e.g., Computer Science, Engineering, Finance) / Master's degree preferred\n\u2022 Immediately apply Industry-leading analytics approaches and tools to transform data into insights\n\u2022 Experience deploying and working with diverse data environments\n\u2022 Financial Services experience preferred\n\u2022 Proven ability in data analytics, data warehousing, and business intelligence\n\u2022 Ability to synthesize sophisticated data from multiple, disparate sources to present analysis and relevant insights\n\u2022 Sophisticated knowledge of database concepts and expertise with SQL (prefer Oracle and Snowflake)\n\u2022 Data integration development experience procedure and tools (e.g. Informatica, Talend, DataStage)\n\u2022 Extensive experience deploying data on public cloud (prefer AWS)\n\u2022 High proficiency in dimensional reporting structures and database design.\n\u2022 Acquainted with the creation and use of semantic layers\n\u2022 Data prep experience using agile tools such as Alteryx, Qlik, R, or Python\n\u2022 Visualization and reporting tools is a plus (e.g. Tableau, OBIEE, Cognos, Domo, Power BI, etc.)\n\u2022 Analyzing data to identify areas of improvement or insights which drive actions to advance organization objectives\n\u2022 Work closely with the squad leaders, engineers & the architects to form strategic partnerships and influence strategic decisions and solution development\n\u2022 Bring depth of experience and influence chosen technologies and design implementation Database, Data Warehousing and Analytics.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690473702,
            "job_posted_at_datetime_utc": "2023-07-27T16:01:42.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Xxd017dEv5YAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-27T22:18:51.000Z",
            "job_offer_expiration_timestamp": 1693174731,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": [
                "AWS"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "5+ years of relevant experience in data analytics or process improvement",
                    "Proven ability in data analytics, data warehousing, and business intelligence",
                    "Ability to synthesize sophisticated data from multiple, disparate sources to present analysis and relevant insights",
                    "Sophisticated knowledge of database concepts and expertise with SQL (prefer Oracle and Snowflake)",
                    "Data integration development experience procedure and tools (e.g",
                    "Informatica, Talend, DataStage)",
                    "Extensive experience deploying data on public cloud (prefer AWS)",
                    "High proficiency in dimensional reporting structures and database design",
                    "Acquainted with the creation and use of semantic layers",
                    "Data prep experience using agile tools such as Alteryx, Qlik, R, or Python",
                    "Tableau, OBIEE, Cognos, Domo, Power BI, etc.)"
                ],
                "Responsibilities": [
                    "Analyzing data to identify areas of improvement or insights which drive actions to advance organization objectives",
                    "Work closely with the squad leaders, engineers & the architects to form strategic partnerships and influence strategic decisions and solution development",
                    "Bring depth of experience and influence chosen technologies and design implementation Database, Data Warehousing and Analytics"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "H-E-B",
            "employer_logo": "https://www.heb.com/img/header/logo.png",
            "employer_website": "http://www.heb.com",
            "employer_company_type": "Retail",
            "job_publisher": "Built In",
            "job_id": "AXlP4nh2A4AAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer, DevX and Platform-Dallas, Austin, or San Antonio, TX (Dallas, TX)",
            "job_apply_link": "https://builtin.com/job/data/senior-data-engineer-devx-and-platform-dallas-austin-or-san-antonio-tx/1665565",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.688,
            "job_description": "Overview\nH-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.\nResponsibilities\nSince H-E-B Digital Technology's inception, we've been investing heavily in our customers' digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital--we're using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. If you enjoy taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.\nOur Partners thrive The H-E-B Way. In the Senior Data Engineer, DevX that means you have a...\nHEART FOR PEOPLE... you can organize multiple engineers, negotiate solutions, and provide upward communication\nHEAD FOR BUSINESS... you consistently demonstrate and uphold the standards of coding, infrastructure, and process\nPASSION FOR RESULTS... you're capable of high-velocity contributions in multiple technical domains\nWhat you will do:\n\u2022 Develop solutions to build and continuously improve monitoring and observability for data pipelines and data platform\n\u2022 Build data platform components using hybrid cloud services (AWS, GCP, and Azure)\n\u2022 Implement features to improve data platform performance and security continuously\n\u2022 Build Real-time data streaming tools and associated experience\n\u2022 Create self-service tools and experience for all enterprise data engineering teams\n\nProject you will impact:\n\u2022 Build a data platform that can handle petabytes of data and help running advanced analytics workloads\n\u2022 Improve the data quality and consumer experience for 100K+ enterprise data consumer\n\nWho you are:\n\u2022 Hands-on experience in Cloud and data pipelines.\n\u2022 Expert in SQL, and experienced programmer in one or more than one of the languages such as Python, Java, or Scala.\n\u2022 Understanding of Big Data and Hybrid Cloud infrastructure. Experienced in more than one of the technologies such as Kafka, Kubernetes, Spark, Databricks, AWS EMR, S3, Data warehouses (Snowflake, Teradata), AWS and GCP Cloud services\n\u2022 Experienced in cloud administration and infrastructure as a code (Terraform, Cloud Formation, Ansible, Chef) is a plus\n\u2022 Experienced in DevOps tools such as GitLab CI/CD, and Jenkins.\n\u2022 Up to date on the latest technology developments. Should be able to evaluate and propose new tooling/solutions for data platforms.\n\u2022 Excellent written, oral communication and presentation skills.\n\u2022 Understanding of MLOps and Data Engineering\n\nBonus:\n\u2022 Databricks or Spark Certifications\n\u2022 DevOps Certifications\n\u2022 Cloud certifications (AWS Preferred)",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1678251629,
            "job_posted_at_datetime_utc": "2023-03-08T05:00:29.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=AXlP4nh2A4AAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Hands-on experience in Cloud and data pipelines",
                    "Expert in SQL, and experienced programmer in one or more than one of the languages such as Python, Java, or Scala",
                    "Understanding of Big Data and Hybrid Cloud infrastructure",
                    "Experienced in more than one of the technologies such as Kafka, Kubernetes, Spark, Databricks, AWS EMR, S3, Data warehouses (Snowflake, Teradata), AWS and GCP Cloud services",
                    "Experienced in DevOps tools such as GitLab CI/CD, and Jenkins",
                    "Up to date on the latest technology developments",
                    "Should be able to evaluate and propose new tooling/solutions for data platforms",
                    "Excellent written, oral communication and presentation skills",
                    "DevOps Certifications"
                ],
                "Responsibilities": [
                    "you consistently demonstrate and uphold the standards of coding, infrastructure, and process",
                    "Develop solutions to build and continuously improve monitoring and observability for data pipelines and data platform",
                    "Build data platform components using hybrid cloud services (AWS, GCP, and Azure)",
                    "Implement features to improve data platform performance and security continuously",
                    "Build Real-time data streaming tools and associated experience",
                    "Create self-service tools and experience for all enterprise data engineering teams",
                    "Build a data platform that can handle petabytes of data and help running advanced analytics workloads",
                    "Improve the data quality and consumer experience for 100K+ enterprise data consumer"
                ],
                "Benefits": [
                    "Databricks or Spark Certifications"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "445110",
            "job_naics_name": "Supermarkets and Other Grocery (except Convenience) Stores"
        },
        {
            "employer_name": "Docyt",
            "employer_logo": "https://res.cloudinary.com/crunchbase-production/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/v1464377633/hffpio7eh8djgtpcteyg.png",
            "employer_website": "http://docyt.com",
            "employer_company_type": null,
            "job_publisher": "Salary.com",
            "job_id": "XBgFtaGDEqoAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Big Data Engineer",
            "job_apply_link": "https://www.salary.com/job/docyt/big-data-engineer/j202307221606596443368",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5621,
            "job_description": "Docyt, a fast-growing FinTech startup based in Silicon Valley, is seeking a highly motivated Big Data Engineer to join our team. The ideal candidate will be responsible for maintaining our data processing infrastructure and optimizing our data architecture, as well as contributing to the development and implementation of new data-driven solutions. At Docyt, we are passionate about empowering businesses to take control of their financial data using an AI-driven super app, and we're looking for a skilled engineer to help us continue to innovate in this exciting space.\n\nResponsibilities\n\u2022 Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse\n\u2022 Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines\n\u2022 Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques\n\u2022 Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities\n\u2022 Build and maintain data models and ensure data accuracy and consistency\n\u2022 Implement and manage data security measures, including backups and access controls\n\u2022 Participate in code reviews, providing constructive feedback to ensure code quality\n\u2022 Bachelor's degree in Computer Science, Engineering, or related field\n\u2022 At least 3 years of experience in big data engineering or related field\n\u2022 Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling\n\u2022 Proficient in at least one programming language, such as Python or Java\n\u2022 Experience with SQL and NoSQL databases\n\u2022 Familiarity with distributed computing frameworks, such as Hadoop or Spark\n\u2022 Understanding of data security and access control best practices\n\u2022 Strong problem-solving skills and ability to work independently and in a team environment\n\u2022 Excellent verbal and written communication skills\n\u2022 Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field\n\u2022 Experience with AWS and Docker is a plus.\n\u2022 Great growth potential at a fast-growing startup, we want you to grow with us!\n\u2022 Company-provided laptop and necessary hardware to ensure your setup for success.\n\u2022 Comprehensive health, dental and vision coverage.\n\u2022 Company-sponsored 401(k)\n\u2022 Inclusive and motivating work culture that values team collaboration.\n\nAbout Us\n\nDocyt, pronounced \u201cdocket\u201d, is a FinTech startup headquartered in Silicon Valley, that is passionately focused on giving businesses control of their financial data. While great strides have been made in sending and receiving payments, businesses still struggle to aggregate all their financial data, understand it, and use it to make well-informed, timely decisions. Docyt brings order to data chaos.\n\nDocyt is a super app that applies AI (artificial intelligence) across the entire accounting tech stack. Docyt digitizes financial data, automates both income and expense workflows, continuously reconciles QuickBooks\u00ae, and generates real-time financial statements. That explains what we do, but here\u2019s why it\u2019s important. A complete, accurate, real-time financial picture empowers businesses to make timely and smart decisions so their business can thrive.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690156800,
            "job_posted_at_datetime_utc": "2023-07-24T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "retirement_savings",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=XBgFtaGDEqoAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2024-01-22T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1705881600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "At least 3 years of experience in big data engineering or related field",
                    "Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling",
                    "Proficient in at least one programming language, such as Python or Java",
                    "Experience with SQL and NoSQL databases",
                    "Familiarity with distributed computing frameworks, such as Hadoop or Spark",
                    "Understanding of data security and access control best practices",
                    "Strong problem-solving skills and ability to work independently and in a team environment",
                    "Excellent verbal and written communication skills",
                    "Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field"
                ],
                "Responsibilities": [
                    "Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse",
                    "Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines",
                    "Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques",
                    "Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities",
                    "Build and maintain data models and ensure data accuracy and consistency",
                    "Implement and manage data security measures, including backups and access controls",
                    "Participate in code reviews, providing constructive feedback to ensure code quality",
                    "Bachelor's degree in Computer Science, Engineering, or related field",
                    "Company-provided laptop and necessary hardware to ensure your setup for success",
                    "Comprehensive health, dental and vision coverage",
                    "Inclusive and motivating work culture that values team collaboration"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "MSRCosmos",
            "employer_logo": "https://mma.prnewswire.com/media/1885557/MSRcosmos_logo.jpg?p=facebook",
            "employer_website": "http://www.msrcosmos.com",
            "employer_company_type": null,
            "job_publisher": "Dice",
            "job_id": "9vfOrrNZWXwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer - Multiple locations",
            "job_apply_link": "https://www.dice.com/job-detail/ee40beb9-1cd0-48fb-b2ce-66f5ccfc1747",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5757,
            "job_description": "Title: Senior Data Engineer\n\nJob Type: Contract to Hire\n\nLocation: Can work from one of client's offices within USA (they are located in almost every state)\n\nDescription:\n\nThe Data Engineer will help build and maintain the cloud Delta Lake platform leveraging Databricks. Candidates will be expected to contribute to all stages of the data lifecycle including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation.\n\nJob Responsibilities may include:\n\u2022 Design, implement (deploy) and support on-premise and cloud-based data infrastructure (systems, flow) that are resilient to disruptions and failures\n\u2022 Enhance and support corporate SQL/NoSQL database, DWH assets and streaming data solutions\n\u2022 Ensure high uptime for all data services and consider enhanced solutions through scheduled or event-driven design\n\u2022 Bring multi cloud/cross-platform agnostic technologies and practices into the system to enhance reliability and support rapid scaling of the business's data needs\n\u2022 Scale up our data infrastructure to meet cross-functional, multi industry business needs\n\u2022 Develop, leverage and maintain end-to-end data pipelines in production\n\u2022 Provide subject matter expertise and hands on delivery of data acquisition, curation and consumption pipelines on Azure, Databricks, AWS, Confluent\n\u2022 Responsible for maintaining current and emerging state of the art compute and cloud based solutions and technologies.\n\u2022 Build effective relationships with internal stakeholders\n\u2022 Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\n\u2022 Hands-on experience implementing analytics solutions leveraging Python, Spark SQL, Databricks Lakehouse Architecture, orchestration tools Kubernetes, Docker\n\u2022 All other duties as assigned\n\nRequirements:\n\u2022 Bachelor's degree in Computer Science, Information Technology, Management Information Systems (MIS), Data Science or related field. Applicable years of experience may be substituted for degree requirement.\n\u2022 Minimum 8 years of experience in software engineering\n\u2022 Experience with leading large and complex data projects, preferred\n\u2022 Experience with large-scale data warehousing architecture and data modeling, preferred\n\u2022 Worked with Cloud-based architecture such as Azure, AWS or Google Cloud, preferred\n\u2022 Experience working with big data technologies e.g. Snowflake, Redshift, Synapse, Postgres, Airflow, Kafka, Spark, DBT, preferred\n\u2022 Experience implementing pub/sub and streaming use cases, preferred\n\u2022 Experience leading design reviews, preferred\n\u2022 Experience influencing a team's technical and business strategy by making insightful contributions to team priorities and approaches, preferred\n\u2022 Working knowledge of relational databases, preferred\n\u2022 Expert in SQL, Python, Java and high-level languages such as Scala, C#, or C preferred\n\u2022 Demonstrate the ability to analyze large data sets to identify gaps and inconsistencies in ETL pipeline and provide solutions for pipeline reliability and data quality, preferred\n\u2022 Experience in designing and implementing an infrastructure as code / CICD development environment, preferred\n\u2022 Proven ability to build, manage and foster a team-oriented environment\n\u2022 Excellent communication (written and oral) and interpersonal skills\n\u2022 Excellent organizational, multi-tasking, and time-management skills",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690403463,
            "job_posted_at_datetime_utc": "2023-07-26T20:31:03.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=9vfOrrNZWXwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T21:46:01.000Z",
            "job_offer_expiration_timestamp": 1693086361,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 96,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": [
                "Python",
                "Azure",
                "Data lake",
                "data engineer"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's degree in Computer Science, Information Technology, Management Information Systems (MIS), Data Science or related field",
                    "Applicable years of experience may be substituted for degree requirement",
                    "Minimum 8 years of experience in software engineering",
                    "Proven ability to build, manage and foster a team-oriented environment",
                    "Excellent communication (written and oral) and interpersonal skills",
                    "Excellent organizational, multi-tasking, and time-management skills"
                ],
                "Responsibilities": [
                    "The Data Engineer will help build and maintain the cloud Delta Lake platform leveraging Databricks",
                    "Candidates will be expected to contribute to all stages of the data lifecycle including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation",
                    "Design, implement (deploy) and support on-premise and cloud-based data infrastructure (systems, flow) that are resilient to disruptions and failures",
                    "Enhance and support corporate SQL/NoSQL database, DWH assets and streaming data solutions",
                    "Ensure high uptime for all data services and consider enhanced solutions through scheduled or event-driven design",
                    "Bring multi cloud/cross-platform agnostic technologies and practices into the system to enhance reliability and support rapid scaling of the business's data needs",
                    "Scale up our data infrastructure to meet cross-functional, multi industry business needs",
                    "Develop, leverage and maintain end-to-end data pipelines in production",
                    "Provide subject matter expertise and hands on delivery of data acquisition, curation and consumption pipelines on Azure, Databricks, AWS, Confluent",
                    "Responsible for maintaining current and emerging state of the art compute and cloud based solutions and technologies",
                    "Build effective relationships with internal stakeholders",
                    "Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc",
                    "All other duties as assigned"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Saxon Global",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSlFP-6TjZGEvHFMa5VEr-0uRMHpyoOdoANCQHs&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Lensa",
            "job_id": "tHzZ5aQ6XEYAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Mid- Data Engineer",
            "job_apply_link": "https://lensa.com/mid-data-engineer-jobs/dallas/jd/b6d4b47da82eebd9829510cce511dcca",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.462,
            "job_description": "Understanding Business Context Requires knowledge of:\n\nIndustry and environmental factors; Common business vernacular; Business practices across two or more domains such as product, finance, marketing, sales, technology, business systems, and human resources and in-depth knowledge of related practices; Directly relevant business metrics and business areas. To support the development of business cases and recommendations. Drives delivery of project activity and tasks assigned by others. Supports process updates and changes. Support, under guidance, in solving business issues.\n\nData Governance Requires knowledge of:\n\nData value chains; Data processes and practices; Regulatory and ethical requirements around data; Data modeling, storage, integration, and warehousing; Data value chains (identification, ingestion, processing, storage, analysis, and utilization); Data quality framework and metrics; Regulatory and ethical requirements around data privacy, security, storage, retention, and documentation; Business implications on data usage; Data Strategy; Enterprise regulatory and ethical policies and strategies. To support the documentation of data governance processes. Supports the implementation of data governance practices.\n\nData Strategy Requires knowledge of: Understanding of business value and relevance of data and data enabled insights / decisions; Appropriate application and understanding of data ecosystem including Data Management, Data Quality Standards and Data Governance, Accessibility, Storage and Scalability etc; Understanding of the methods and applications that unlock the monetary value of data assets. To understand, articulate, and apply principles of the defined strategy to routine business problems that involve a single function. Data Source Identification Requires knowledge of: Functional business domain and scenarios; Categories of data and where it is held; Business data requirements; Database technologies and distributed datastores (e.g. SQL, NoSQL); Data Quality; Existing business systems and processes, including the key drivers and measures of success. To support the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data. Data Transformation and Integration Requires knowledge of: Internal and external data sources including how they are collected, where and how they are stored, and interrelationships, both within and external to the organization; Techniques like ETL batch processing, streaming ingestion, scrapers, API and crawlers; Data warehousing service for structured and semi-structured data, or to MPP databases such as Snowflake, Microsoft Azure, Presto or Google BigQuery; Pre-processing techniques such as transformation, integration, normalization, feature extraction, to identify and apply appropriate methods; Techniques such as decision trees, advanced regression techniques such as LASSO methods, random forests etc; Cloud and big data environments like EDO2 systems. To extract data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends. Data Modeling Requires knowledge of: Cloud data strategy, data warehouse, data lake, and enterprise big data platforms; Data modeling techniques and tools (For example, Dimensional design and scalability), Entity Relationship diagrams, Erwin, etc. ; Query languages SQL / NoSQL; Data flows through the different systems; Tools supporting automated data loads; Artificial Intelligent - enabled metadata management tools and techniques. To analyze complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows.\n\nAnalyzes data Job Profile: (USA) Data Engineer III 09:57 AM 06/29/2022 Page 1 of 3 related system integration challenges and proposes appropriate solutions. Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports. Code Development and Testing Requires knowledge of: Coding languages like SQL, Java, C++, Python and others; Testing methods such as static, dynamic, software composition analysis, manual penetration testing and others; Business, domain understanding. To write code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates. Demonstrates up-to-date expertise and applies this to the development, execution, an\n\nRequired Skills : .jpg, sql, GCP, Python\nBasic Qualification :\nAdditional Skills :\nBackground Check :Yes\nDrug Screen :Yes\nNotes :remote\nSelling points for candidate :\nProject Verification Info :\nCandidate must be your W2 Employee :No\nExclusive to Apex :No\nFace to face interview required :No\nCandidate must be local :No\nCandidate must be authorized to work without sponsorship ::No\nInterview times set : :No\nType of project :Development/Engineering\nMaster Job Title :Big Data: Other\nBranch Code :Dallas",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690215552,
            "job_posted_at_datetime_utc": "2023-07-24T16:19:12.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=tHzZ5aQ6XEYAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-23T16:19:12.000Z",
            "job_offer_expiration_timestamp": 1692807552,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Reading Comprehension",
                "Active Listening",
                "Writing",
                "Speaking",
                "Critical Thinking",
                "Active Learning",
                "Monitoring",
                "Social Perceptiveness",
                "Coordination",
                "Complex Problem Solving",
                "Programming",
                "Judgment and Decision Making",
                "Systems Analysis",
                "Systems Evaluation"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {},
            "job_job_title": null,
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06 Database Architects"
            ]
        },
        {
            "employer_name": "Infojini",
            "employer_logo": "https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=100064129355795",
            "employer_website": "http://www.infojiniconsulting.com",
            "employer_company_type": null,
            "job_publisher": "Jobrapido.com",
            "job_id": "nKAVtQsHlHgAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Jr. Data Engineer with AWS",
            "job_apply_link": "https://us.jobrapido.com/jobpreview/2962504484",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4464,
            "job_description": "We are looking for a Jr. Data Engineer for SWA. Please see below:\nProject length: 12 Months CTH\nLooking for Junior level resource\nscripting (python or powershell), general knowledge of AWS\nwill be touching multiple technologies - someone that is ok to pick up and learn\nNeed to have a great attitude - they can coach, teach mentor in areas that they may be lacking. Need engineer that is hungry to learn.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690329600,
            "job_posted_at_datetime_utc": "2023-07-26T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=nKAVtQsHlHgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Looking for Junior level resource",
                    "scripting (python or powershell), general knowledge of AWS",
                    "will be touching multiple technologies - someone that is ok to pick up and learn",
                    "Need to have a great attitude - they can coach, teach mentor in areas that they may be lacking"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Southwest Airlines",
            "employer_logo": "https://1000logos.net/wp-content/uploads/2019/08/southwest-airlines-logo.png",
            "employer_website": "http://www.southwest.com",
            "employer_company_type": "Logistics",
            "job_publisher": "Southwest Careers - Southwest Airlines",
            "job_id": "Fw73q18LK2YAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Sr Data Engineer",
            "job_apply_link": "https://careers.southwestair.com/job/R-2023-32207/Sr-Data-Engineer",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.8069,
            "job_description": "Department:\nTechnology\n\nOur Company Promise\n\nWe are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Southwest Airlines. Above all, Employees will be provided the same concern, respect, and caring attitude within the organization that they are expected to share externally with every Southwest Customer.\n\nJob Description:\n\nJob Summary\n\u2022 Work on complex problems, where analysis of situations or data requires an in-depth evaluation of multiple factors. Lead and/or provide expertise to functional project teams and participate in cross-functional initiatives. Provide direction and guidance to process improvements, including helping to establish/advise on policies. Work with a number of external vendors, helping to provide them with effective solutions and insights. Use independent judgment within broadly defined policies and practices, including determining the best method for accomplishing work.\n\nAdditional details:\n\u2022 Please know this posting is for multiple open Sr Data Engineer positions across various Teams in Technology. These Teams all contribute to important initiatives and are helping to shape the future of Technology at Southwest Airlines. Our Recruiting Team will provide additional Team and technology-specific details in the interview stage. Please reach out to TechnologyRecruitingInformation@wnco.com with any questions.\n\u2022 This role is offered as a remote workplace position, which may require travel for trainings, meetings, conferences, etc. Outside of those required visits, the majority of your working time may be spent in a remote location, away from our Corporate Campus. Please note, while this is a remote position, there is limited group of states or localities ineligible for Employees to regularly perform their work off-site. Those ineligible locations are: Alaska,\u202fDelaware, New Jersey, North Dakota, South Dakota, Vermont, West Virginia, and Wyoming.\u202f\u202f\n\u2022 U.S. citizenship or current authorization to work in the U.S. is required, and no current or future work authorization sponsorship available.\n\nSouthwest Airlines is an Equal Opportunity Employer.\u202fWe continue to look for opportunities to reflect the communities we serve, and welcome applicants with diverse thoughts, backgrounds, and experiences.\n\u2022 Responsibilities\n\u2022 Assemble large, complex sets of data that meet non-functional and functional business requirements\n\u2022 Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes\n\u2022 Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies\n\u2022 Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition\n\u2022 Work with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues\n\u2022 Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues\n\u2022 Generate or adapt equipment and technology to serve user needs\n\u2022 May perform other job duties as directed by Employee's Leaders\n\u2022 Knowledge, Skills and Abilities\n\u2022 Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services\n\u2022 Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models\n\u2022 Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems\n\u2022 Ability to understand the implications of new information for both current and future problem-solving and decision-making\n\u2022 Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions\n\u2022 Ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem\n\u2022 Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)\n\u2022 Ability to shift back and forth between two or more activities or sources of information (such as speech, sounds, touch, or other sources)\n\u2022 Ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations)\n\u2022 Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material\n\u2022 Education\n\u2022 Required: High School Diploma or GED\n\u2022 Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training\n\u2022 Experience\n\u2022 Required: Advanced level experience, seasoned and specialized knowledge in:\n\u2022 Cloud infrastructure, DataLake\n\u2022 ETL experience ensuring source to target data integrity\n\u2022 Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque)\n\u2022 ServiceBus, setting up ingress and egress within a subscription, or relevant AWS Cloud services administrative experience\n\u2022 Unit Testing, Code Quality tools, CI/CD Technologies, Security and Container Technologies\n\u2022 Agile development experience and Agile ceremonies and practices\n\u2022 Licensing/Certification\n\u2022 N/A\n\u2022 Physical Abilities\n\u2022 Ability to perform work duties from [limited space work station/desk/office area] for extended periods of time\n\u2022 Ability to communicate and interact with others in the English language to meet the demands of the job\n\u2022 Ability to use a computer and other office productivity tools with sufficient speed and accuracy to meet the demands of the job\n\u2022 Other Qualifications\n\u2022 Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines\n\u2022 Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986\n\u2022 Must be at least 18 years of age\n\u2022 Must be able to comply with Company attendance standards as described in established guidelines\n\u2022 Pay & Benefits\n\u2022 Competitive market salary from $137,250 per year to $152,500 per year* depending on qualifications and experience. For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company.\n\u2022 Benefits you'll love:\n\u2022 Fly for free, as a privilege, on any open seat on all Southwest flights\u2014your eligible dependents too.\n\u2022 Up to a 9.3% 401(k) Company match, dollar for dollar, per paycheck.*\n\u2022 Potential for annual ProfitSharing contribution toward retirement - when Southwest profits, you profit.**\n\u2022 Explore more Benefits you\u2019ll love: swa.is/benefits\n\u2022 Pay amount doesn\u2019t guarantee employment for any particular period of time\n\u2022 *401(k) match contributions are subject to the plan\u2019s vesting schedule and applicable IRS limits\n\u2022 **ProfitSharing contributions are subject to plan\u2019s vesting schedule and are made at the discretion of the Company\n\nSouthwest Airlines is an Equal Opportunity Employer.\nPlease print/save this job description because it won't be available after you apply.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690502400,
            "job_posted_at_datetime_utc": "2023-07-28T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Fw73q18LK2YAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "U.S. citizenship or current authorization to work in the U.S. is required, and no current or future work authorization sponsorship available",
                    "Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services",
                    "Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models",
                    "Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems",
                    "Ability to understand the implications of new information for both current and future problem-solving and decision-making",
                    "Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions",
                    "Ability to tell when something is wrong or is likely to go wrong",
                    "It does not involve solving the problem, only recognizing there is a problem",
                    "Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material",
                    "Required: High School Diploma or GED",
                    "Required: Bachelor's Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training",
                    "Required: Advanced level experience, seasoned and specialized knowledge in:",
                    "Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines",
                    "Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986",
                    "Must be at least 18 years of age",
                    "Must be able to comply with Company attendance standards as described in established guidelines"
                ],
                "Responsibilities": [
                    "Work on complex problems, where analysis of situations or data requires an in-depth evaluation of multiple factors",
                    "Lead and/or provide expertise to functional project teams and participate in cross-functional initiatives",
                    "Provide direction and guidance to process improvements, including helping to establish/advise on policies",
                    "Work with a number of external vendors, helping to provide them with effective solutions and insights",
                    "Use independent judgment within broadly defined policies and practices, including determining the best method for accomplishing work",
                    "This role is offered as a remote workplace position, which may require travel for trainings, meetings, conferences, etc",
                    "Assemble large, complex sets of data that meet non-functional and functional business requirements",
                    "Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes",
                    "Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies",
                    "Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition",
                    "Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues",
                    "Generate or adapt equipment and technology to serve user needs",
                    "May perform other job duties as directed by Employee's Leaders",
                    "Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)",
                    "ETL experience ensuring source to target data integrity",
                    "Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque)"
                ],
                "Benefits": [
                    "Competitive market salary from $137,250 per year to $152,500 per year* depending on qualifications and experience",
                    "For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company",
                    "Fly for free, as a privilege, on any open seat on all Southwest flights\u2014your eligible dependents too",
                    "Up to a 9.3% 401(k) Company match, dollar for dollar, per paycheck.*",
                    "Potential for annual ProfitSharing contribution toward retirement - when Southwest profits, you profit",
                    "Explore more Benefits you\u2019ll love: swa.is/benefits",
                    "Pay amount doesn\u2019t guarantee employment for any particular period of time",
                    "*401(k) match contributions are subject to the plan\u2019s vesting schedule and applicable IRS limits",
                    "**ProfitSharing contributions are subject to plan\u2019s vesting schedule and are made at the discretion of the Company"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "Technology"
            ],
            "job_naics_code": "481111",
            "job_naics_name": "Scheduled Passenger Air Transportation"
        },
        {
            "employer_name": "MSIT",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTPd-hps7yHu6ZmQPwa8QzKacpqKLbzwCNNoBVY&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "OPTnation",
            "job_id": "t0zy0CTqYuoAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.optnation.com/data-engineer-job-in-dallas-tx-view-jobid-33660",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6987,
            "job_description": "Responsiblties Port existing data pipelines and make data available to an internal data fabric. Build new data acquisition and transformation pipelines using big data and cloud technologies. Work with the broader technology team including information technology information systems and 3rd parties to align pipelines. Contribute to proof-of-concept efforts in Advanced Data Analytics. Perform data analysis required to troubleshoot data-related issues and assist in the resolution of data issues. Participate in establishing system documentation standards and QA methodologies.\n\nKey Skillls Strong programming skills. Particularly in languages. Python Java Scala and SQL.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1685923200,
            "job_posted_at_datetime_utc": "2023-06-05T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=t0zy0CTqYuoAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Python",
                "Java",
                "SQL."
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 106200,
            "job_max_salary": 107600,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "Key Skillls Strong programming skills",
                    "Particularly in languages",
                    "Python Java Scala and SQL"
                ],
                "Responsibilities": [
                    "Build new data acquisition and transformation pipelines using big data and cloud technologies",
                    "Work with the broader technology team including information technology information systems and 3rd parties to align pipelines",
                    "Contribute to proof-of-concept efforts in Advanced Data Analytics",
                    "Perform data analysis required to troubleshoot data-related issues and assist in the resolution of data issues",
                    "Participate in establishing system documentation standards and QA methodologies"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "InfoVision Inc.",
            "employer_logo": "https://infovision.com/wp-content/uploads/2018/08/infovision_logo_public.png",
            "employer_website": "http://www.infovision.com",
            "employer_company_type": "Computer Services",
            "job_publisher": "LinkedIn",
            "job_id": "UQvc_eGI98wAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Big Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/big-data-engineer-at-infovision-inc-3672479393",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5886,
            "job_description": "Title: Big Data Engineer\n\nLocation: Dallas, TX\n\nDuration: Contract\n\nJob Description:\n\nMain Skills:\n\u2022 Hadoop\n\u2022 Apache Spark\n\u2022 Spark Streaming\n\u2022 Apache Kafka\n\u2022 AWS Amazon Web Services\n\nThanks & Regards,\n\nKhaja Shareef",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690214955,
            "job_posted_at_datetime_utc": "2023-07-24T16:09:15.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=UQvc_eGI98wAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-23T16:09:14.000Z",
            "job_offer_expiration_timestamp": 1692806954,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Hadoop",
                    "Apache Spark"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541511",
            "job_naics_name": "Custom Computer Programming Services"
        },
        {
            "employer_name": "UNAVAILABLE",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "H-E-B, L.P. - ICIMS",
            "job_id": "Zw8-q8mSkhsAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Staff Data Engineer, Health & Wellness - Dallas, TX",
            "job_apply_link": "https://careers-heb.icims.com/jobs/10643/staff-data-engineer%2C-health-%26-wellness---dallas%2C-tx/job",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.6721,
            "job_description": "Overview\n\nH-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor,\nH-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace.\nH-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.\n\nResponsibilities\n\nAbout H-E-BH-E-B is one of the largest, independently owned food retailers in the nation operating over 400 stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.\n\nH-E-B Digital is seeking new team members (Partners)! Since our inception, we\u2019ve been investing heavily in our customers\u2019 digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital, and we\u2019re hiring across the stack: front-end web and mobile, full-stack, and backend engineering. We\u2019re using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. Our digital solutions are growing in popularity and adoption\u2014like Curbside and Home Delivery\u2014so you\u2019ll get the opportunity to define the user experience for millions of customers and hundreds of thousands of Partners. If you\u2019re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.Our Partners thrive The H-E-B Way. In the Staff Data Engineer position, that means you have a\u2026\n\nHEART FOR PEOPLE\u2026 you can organize multiple engineers, negotiate solutions, and provide upward communication\n\nHEAD FOR BUSINESS\u2026 you consistently demonstrate and uphold the standards of coding, infrastructure, and process\n\nPASSION FOR RESULTS\u2026 you\u2019re capable of high-velocity contributions in multiple technical domainsWhat you\u2019ll do\n\u2022 Work with HEB Digital teams to provide data solutions for health and wellness\n\u2022 Contribute to existing data platforms and implement new technologies\n\u2022 Develop a deep understanding of HEB\u2019s data and become a domain expert\n\u2022 Ensure data is distributed in a timely and accurate manner\n\u2022 Make data discoverable and accessible to business users\n\u2022 Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed\n\u2022 Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications\n\u2022 Identify, scope, and architect solutions for new features while applying sound technical judgment that considers technology alternatives, impact on affected / adjacent systems, and tradeoffs.\n\u2022 Get the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team\u2019s architecture\n\nProjects you\u2019ll impact\n\u2022 Implement data pipelines on AWS using Argo, Kubernetes, Spark and Python\n\u2022 Integrate health, pharmacy and nutritional data systems\n\u2022 Evaluate new technologies to improve quality, performance and cost of data pipelines\n\nWho You Are\n\u2022 7+ years of data engineering experience\n\u2022 Proficient with data technologies (e.g. Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)\n\u2022 Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka.\n\u2022 Strong understanding of SQL and data modeling\n\u2022 Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes\n\u2022 Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)\n\u2022 Bachelor's degree in computer science or comparable field or equivalent experience\n\u2022 A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling\n\nWhat are the Perks?\n\nA robust Benefits plan with coverage starting Day OneDental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coveragePartner Care Team, for any time you have healthcare or coverage questionsTelehealth offers 24/7 access to board-certified doctors by phonePartner Guidance allows free counselor visitsFuneral leave, jury duty, and military pay (subject to applicable law)Maternal / paternal leave for new parents, including adoptions10% off H-E-B brand products in-store and onlineEligibility to participate in 401(k)Opportunity to become a \u201cPartner-Owner\u201d after 12 months\n\nWho We AreH-E-B is one of the largest, independently owned food retailers in the nation, operating over 400 stores throughout Texas and Mexico, with annual sales generating over $25 billionWe hire talented people (109,000+ Partners), and give them autonomy to be creative in how they impact the businessWe\u2019re a Partner-driven company with a Bold Promise \u2013 Because People MatterWe embrace Diversity and Inclusion as core values, and support them with thriving company-wide programsWe\u2019re a truly original Texas-based company that created the Spirit of Giving to help Texas communities every dayOnce eligible, our Partners become Owners in the company. \u201cPartner-owned\u201d means our most important resources\u2014People\u2014drive the innovation, growth, and success that make H-E-B The Greatest Retailing Company\n\nHiring in Dallas!\n\nDATA3232",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1627261705,
            "job_posted_at_datetime_utc": "2021-07-26T01:08:25.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "dental_coverage",
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Zw8-q8mSkhsAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2024-07-26T01:08:25.000Z",
            "job_offer_expiration_timestamp": 1721956105,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "7+ years of data engineering experience",
                    "Proficient with data technologies (e.g",
                    "Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)",
                    "Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka",
                    "Strong understanding of SQL and data modeling",
                    "Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes",
                    "Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)",
                    "Bachelor's degree in computer science or comparable field or equivalent experience",
                    "A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling"
                ],
                "Responsibilities": [
                    "HEART FOR PEOPLE\u2026 you can organize multiple engineers, negotiate solutions, and provide upward communication",
                    "HEAD FOR BUSINESS\u2026 you consistently demonstrate and uphold the standards of coding, infrastructure, and process",
                    "Work with HEB Digital teams to provide data solutions for health and wellness",
                    "Contribute to existing data platforms and implement new technologies",
                    "Develop a deep understanding of HEB\u2019s data and become a domain expert",
                    "Ensure data is distributed in a timely and accurate manner",
                    "Make data discoverable and accessible to business users",
                    "Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed",
                    "Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications",
                    "Identify, scope, and architect solutions for new features while applying sound technical judgment that considers technology alternatives, impact on affected / adjacent systems, and tradeoffs",
                    "Get the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team\u2019s architecture",
                    "Implement data pipelines on AWS using Argo, Kubernetes, Spark and Python",
                    "Integrate health, pharmacy and nutritional data systems",
                    "Evaluate new technologies to improve quality, performance and cost of data pipelines"
                ],
                "Benefits": [
                    "A robust Benefits plan with coverage starting Day OneDental, vision, life, and other insurance plans; flexible spending accounts; short term / long term disability coverage",
                    "Partner Care Team, for any time you have healthcare or coverage questionsTelehealth offers 24/7 access to board-certified doctors by phone",
                    "Partner Guidance allows free counselor visits",
                    "Funeral leave, jury duty, and military pay (subject to applicable law)Maternal / paternal leave for new parents, including adoptions10% off H-E-B brand products in-store and online"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "Digital"
            ]
        },
        {
            "employer_name": "Hinge",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLPWFWVwd5KOcMLGD-nVhSrioNWrTds9AlMEfr&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "The Muse",
            "job_id": "zpHp8hO8r5gAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Sr. Data Engineer",
            "job_apply_link": "https://www.themuse.com/jobs/hinge/sr-data-engineer",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5766,
            "job_description": "As a Senior Data Engineer, you will be implementing critical ETL pipelines and advancing best practices for the data engineering team, and the rest of the organization. You will work on delivering an actual big data architecture while concentrating on real-world problems such as privacy concerns.\n\nThis role is key to the success of Match Group. Not only will you help power the love lives of millions of people, but you will play a critical part in the functioning of every brand at Match Group (Match, Tinder, Hinge, Okcupid, PlentyofFish, BLK, and others), with stakeholders ranging from customer experience to marketing to leadership.\n\nWe\u2019re based in Dallas, TX. However, we are open to considering a remote office for a qualified candidate who can travel periodically to Dallas (when safe to do so).\n\nHow you\u2019ll make an impact:\n\u2022 Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers\n\u2022 Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large\n\u2022 Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and building scalable/reliable solutions\n\u2022 Work with stakeholders and translate their needs and expectations into action items and deliverables\n\u2022 Lead infrastructure initiatives, from design to implementation to delivery\n\u2022 Support existing on-prem infrastructure and help expand our processes into the cloud (AWS)\n\nWe could be a match if you bring:\n\u2022 Expertise in SQL, Data Modeling, and Python\n\u2022 Used Redshift, Airflow, Spectrum and relational database like SQL Server\n\u2022 Capability to drive initiatives and articulate their value to Engineering and other stakeholders\n\u2022 Experience delivering data products from conception to delivery\n\u2022 Good communication skills (written/verbal)\n\u2022 Passionate about designing elegant ETL pipelines\n\u2022 5+ years of professional/industry experience\n\nOur team culture:\n\u2022 Authenticity: Share your genuine thoughts and opinions directly\n\u2022 Courage: Invite and deeply consider challenges and criticism\n\u2022 Empathy: Be empathetic, communitarian and trustworthy\n\nWhat's the team like?\n\u2022 Our BI team is a service organization that delivers reporting solutions to the entire Match Group enterprise\n\u2022 The BI team is responsible for architecting and engineering new data systems and reporting to help facilitate business decision-making\n\n#LI-CENTRAL\n#LI-REMOTE\n\nWhy Match Group?\n\nOur mission is simple \u2013 to help people find love and happiness!\nWe love our employees too \u2013 here are some examples how:\n\nAnnual training budget for each employee\n100% employer match on 401k contributions\nSpecific COVID-19 allowance for home office set-up\nMatched giving to qualified organizations\n100% paid Parental Leave\nHappy Hours and Company events (they are all virtual now, but still a ton of fun!)\n\nWe value diversity at our company and will never discriminate based on someone\u2019s race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1619044350,
            "job_posted_at_datetime_utc": "2021-04-21T22:32:30.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=zpHp8hO8r5gAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Expertise in SQL, Data Modeling, and Python",
                    "Used Redshift, Airflow, Spectrum and relational database like SQL Server",
                    "Capability to drive initiatives and articulate their value to Engineering and other stakeholders",
                    "Experience delivering data products from conception to delivery",
                    "Good communication skills (written/verbal)",
                    "Passionate about designing elegant ETL pipelines",
                    "5+ years of professional/industry experience",
                    "Empathy: Be empathetic, communitarian and trustworthy"
                ],
                "Responsibilities": [
                    "As a Senior Data Engineer, you will be implementing critical ETL pipelines and advancing best practices for the data engineering team, and the rest of the organization",
                    "You will work on delivering an actual big data architecture while concentrating on real-world problems such as privacy concerns",
                    "Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers",
                    "Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large",
                    "Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and building scalable/reliable solutions",
                    "Work with stakeholders and translate their needs and expectations into action items and deliverables",
                    "Lead infrastructure initiatives, from design to implementation to delivery",
                    "Support existing on-prem infrastructure and help expand our processes into the cloud (AWS)"
                ],
                "Benefits": [
                    "Annual training budget for each employee",
                    "100% employer match on 401k contributions",
                    "Specific COVID-19 allowance for home office set-up",
                    "Matched giving to qualified organizations",
                    "100% paid Parental Leave"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Georgia IT Inc.",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Talent.com",
            "job_id": "n2SRAN_BDGAAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data engineer",
            "job_apply_link": "https://www.talent.com/view?id=376ab70841cb",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4858,
            "job_description": "Data Engineer\n\nLocation : Dallas, Texas\n\nType : Contract\n\nRate : DOE\n\nNO third-party Corp to Corp accepted for this job\n\nDescription\n\u2022 Strong Python, bash, Linux shell or similar\n\u2022 Experience integrating with native AWS services (CodePipeline, CodeCommit, CodeBuild, CodeDeploy, EC2, EKS, ECR, S3)\n\u2022 Knowledge of networking, IAM, API and security assessment tools / methodologies.\n\u2022 Familiar with IAM protocols such as SAML, SPML, XACML, SCIM, OpenID and OAuth.\n\u2022 Understanding of the cyber threat landscape and methodologies to protect technology assets.\n\u2022 AWS Certified Developer Associate or AWS Certified Solutions Architect Associate preferred.\n\u2022 Excellent verbal and written communication skills\n\u2022 Proficient in automation and deploying CI and CD tools and services (Jenkins Pipeline as Code, Git, Maven).\n\u2022 Hands on experience building solutions with tools and services like AWS CloudFormation, Terraform, or custom build orchestration tools leveraging SDKs or directly interacting with APIs\n\u2022 Strong understanding of fundamental Application and Infrastructure Security concepts, including common types of attacks and exploitation techniques.\n\u2022 Solid Experience with various application security tools (Example ZAP, BURP, Tenable,Check Mark, Semmel, fortify, Sonatype, Kali, WebInspect / AppScan, dependency check).\n\u2022 Solid understanding of common web and systems application vulnerabilities.Experience integrating security tools into the DevOps environment (such as Zap or Burp)\n\nLast updated : 2023-07-26",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690329600,
            "job_posted_at_datetime_utc": "2023-07-26T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=n2SRAN_BDGAAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-09-28T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1695859200,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Strong Python, bash, Linux shell or similar",
                    "Experience integrating with native AWS services (CodePipeline, CodeCommit, CodeBuild, CodeDeploy, EC2, EKS, ECR, S3)",
                    "Knowledge of networking, IAM, API and security assessment tools / methodologies",
                    "Familiar with IAM protocols such as SAML, SPML, XACML, SCIM, OpenID and OAuth",
                    "Understanding of the cyber threat landscape and methodologies to protect technology assets",
                    "Excellent verbal and written communication skills",
                    "Proficient in automation and deploying CI and CD tools and services (Jenkins Pipeline as Code, Git, Maven)",
                    "Hands on experience building solutions with tools and services like AWS CloudFormation, Terraform, or custom build orchestration tools leveraging SDKs or directly interacting with APIs",
                    "Strong understanding of fundamental Application and Infrastructure Security concepts, including common types of attacks and exploitation techniques",
                    "Solid Experience with various application security tools (Example ZAP, BURP, Tenable,Check Mark, Semmel, fortify, Sonatype, Kali, WebInspect / AppScan, dependency check)",
                    "Solid understanding of common web and systems application vulnerabilities",
                    "Experience integrating security tools into the DevOps environment (such as Zap or Burp)",
                    "Last updated : 2023-07-26"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1243.00"
            ]
        },
        {
            "employer_name": "ryan",
            "employer_logo": "http://companies.naukri.com/ryan-jobs/wp-content/uploads/sites/2082/2015/02/product.jpg",
            "employer_website": "http://www.ryan.com",
            "employer_company_type": null,
            "job_publisher": "Salary.com",
            "job_id": "hH0fMST3sloAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer, Data Engineering",
            "job_apply_link": "https://www.salary.com/job/ryan/senior-data-engineer-data-engineering/j202305230636442430293",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6703,
            "job_description": "The Senior Data Engineer is responsible for the identifying, developing, and maintaining the technologies that enable the efficient flow of data throughout the organization. This role requires an enterprise mindset to build out robust, high-performance technology. Duties and Responsibilities, aligned with Key Results: People Use a variety of programming languages and tools to develop, test, and maintain data pipelines within the Platform Reference Architecture. Working directly with management, product teams and practice personnel to understand their platform data requirements Maintaining a positive work atmosphere by behaving and communicating in a manner that encourages productive interactions with customers, co-workers and supervisors Developing and engaging with team members by creating a motivating work environment that recognizes, holds team members accountable, and rewards strong performance Fostering an innovative, inclusive and diverse team environment, promoting positive team culture, encouraging collaboration and self-organization while delivering high quality solutions Client Collaborating on an Agile team to design, develop, test, implement and support highly scalable data solutions Collaborating with product teams and clients to deliver robust cloud-based data solutions that drive tax decisions and provide powerful experiences Analyzing user feedback and activity and iterate to improve the services and user experience Value Securing data in alignment with internal information and data security policies, best practices and client requirements Creating and implementing robust cloud-based data solutions that scale effectively, and provide powerful experiences for both internal teams and clients Performing unit tests and conducting reviews with other team members to make sure solutions and code are rigorously designed, elegantly coded and effectively tuned for performance Staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities and mentoring other members of the engineering community Perform other duties as assigned Education and Experience: Bachelor\u2019s and/or Master\u2019s degree in a related field. 7 years of experience developing data technologies. 7 years of experience deploying ETL solutions in production environments. 7 years of experience with cloud-based data services, preferably in AWS or Azure. 7 years of experience developing Python, Scala, Java, .Net or similar solutions in a backend or data wrangling capacity. 7 years of experience in mixed Windows/Linux environments. Additional Required Skills and Experience: Results-proven track record of exceeding goals and evidence of the ability to consistently make good decisions through a combination of analysis, experience and judgment Fluency in one or more databases, preferably relational and NoSQL is a plus. Experience with distributed data platforms is a plus. Exposure to AI/ML pipelines is preferred. Experience deploying, monitoring, and maintaining data pipelines in production environments Computer Skills: To perform this job successfully, an individual must have intermediate knowledge of Microsoft Project, Word, Excel, Access, PowerPoint, Outlook, and Internet navigation and research. Supervisory Responsibilities: Requires supervisory responsibilities, including training employees, assigning work, and assuring quality throughout any deliverables Work Environment: Standard indoor working environment. Occasional long periods of sitting while working at computer. Position requires regular interaction with employees at all levels of the Firm and interface with external vendors as necessary. Independent travel requirement: As Needed Remote position * For Denver, CO-based roles, the base salary hiring range for this position is $135,000 - $165,000. * For New York, NY-based roles, the base salary hiring range for this position is $155,000-$185,000. * For Bellevue, WA- based roles, the base salary hiring range for this position is $140,750-$173,000. * For Carlsbad, Glendale, Irvine, Los Angelos, Sacramento, and San Diego, CA-based roles, the base salary hiring range for this position is $140,750-$173,000. * For Oakland and San Jose, CA-based roles, the base salary hiring range for this position is $155,000-$185,000. *The Company makes offers based on many factors, including qualifications and experience Ryan offers outstanding opportunities to work in a dynamic, rapidly expanding tax services firm serving the world\u2019s most respected Global 5000 companies. Our innovative work environment, accelerated growth path for high performers, competitive benefits package, and outstanding earning potential provide the most rewarding career experience available in the industry. Ryan employees are given the freedom to do their best work in the way they work best. With a clear understanding of expectations and accountabilities, our employees are given ownership of their time and flexibility to meet the demands of their professional and personal lives. Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Ryan LLC (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status). Job duties related to this role are to be conducted in a manner that adheres to privacy laws, as well as follows internal governance related to protecting confidential information and trade secrets, and to securing data and company records. Equal Employment Opportunity/Affirmative Action/Accommodation Ryan, LLC is an equal opportunity employer and is committed to compliance with all applicable laws prohibiting employment discrimination. It is our policy to take all employment actions and make all employment decisions without regard to race, color, religion, creed, gender, sex (including pregnancy), affectional or sexual orientation, gender identity or expression, national origin, ancestry, age, marital status, citizenship status, genetic predisposition or carrier status, disability, military status, status as a disabled or other protected veteran or any other protected status under applicable law. It is Ryan's policy to make reasonable accommodation for qualified individuals with disabilities. Please contact our People Group at 972.934.0022 or peoplegroup@ryan.com if you are interested in applying and need assistance to submit your application, or if you are interested in a position and believe you may require a reasonable commodation in order for you to perform its essential functions. Click here to view the entire EEO poster and supplement. *Notice to Canada Candidates \u2013 In accordance with the Accessibility for Ontarians with Disabilities Act (AODA), Ryan ULC will provide accommodation, accessible formats and communication supports for the interview upon request. Ryan welcomes and encourages applications from people with disabilities*. Ryan recognizes and is committed to compliance with the new General Data Protection Regulation (GDPR) promulgated by the European Union (EU). Please access our Privacy Notice in relation to this at the following link. Please access our Personal Data Protection Policy at the following link.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688515200,
            "job_posted_at_datetime_utc": "2023-07-05T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=hH0fMST3sloAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2024-01-03T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1704240000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {},
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Costco Wholesale",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMEXpjzxSXP_hEYETqQMZmfop0Uwnb1a8puH1-&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "JobzMall",
            "job_id": "LNndS8ll-f4AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer - IT Sustainability",
            "job_apply_link": "https://www.jobzmall.com/costco-wholesale/job/data-engineer-it-sustainability-1",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5613,
            "job_description": "We are looking for an experienced and passionate Data Engineer \u2013 IT Sustainability to join the IT team at Costco Wholesale. In this role, you will be responsible for developing and managing data solutions that support our IT sustainability objectives. The successful candidate should have a strong background in data engineering and experience working with sustainability initiatives in the IT space.You will need to be a highly-motivated, self-driven individual with strong interpersonal and problem-solving skills. We are looking for someone who is detail-oriented and can think outside the box to develop innovative solutions to complex problems. If you have a passion for sustainability initiatives, technical expertise, and a commitment to excellence, then this is the perfect opportunity for you.The successful candidate will possess a degree in a related field such as computer science, engineering, or information technology. Additionally, a minimum of two years of professional experience in data engineering and/or working with sustainability initiatives is required. Knowledge of database architecture and experience with data analytics and data visualization tools is also preferred.\n\nCostco Wholesale is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1676932976,
            "job_posted_at_datetime_utc": "2023-02-20T22:42:56.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=LNndS8ll-f4AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-25T23:59:59.000Z",
            "job_offer_expiration_timestamp": 1693007999,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 24,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": [
                "Sql <br>Security <br>Python <br>Networking <br>Linux <br>Data Analysis <br>Communication <br>Leadership <br>Problem Solving <br>Time management <br>Big Data <br>Data Modeling <br>ETL <br>Data Visualization <br>Automation <br>Cloud Computing <br>Interpersonal Skills <br>creativity <br>flexibility <br>Organizational skills <br>Performance tuning <br>Data governance <br>collaboration <br>Adaptability <br>Data WarehouseCommunication <br>Leadership <br>Problem Solving <br>Time management <br>Interpersonal Skills <br>creativity <br>flexibility <br>Organizational skills <br>collaboration <br>Adaptability"
            ],
            "job_required_education": {
                "postgraduate_degree": true,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "The successful candidate should have a strong background in data engineering and experience working with sustainability initiatives in the IT space",
                    "You will need to be a highly-motivated, self-driven individual with strong interpersonal and problem-solving skills",
                    "We are looking for someone who is detail-oriented and can think outside the box to develop innovative solutions to complex problems",
                    "The successful candidate will possess a degree in a related field such as computer science, engineering, or information technology",
                    "Additionally, a minimum of two years of professional experience in data engineering and/or working with sustainability initiatives is required"
                ],
                "Responsibilities": [
                    "In this role, you will be responsible for developing and managing data solutions that support our IT sustainability objectives"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Deloitte",
            "employer_logo": "https://1000logos.net/wp-content/uploads/2021/05/Deloitte-logo.png",
            "employer_website": "http://www.deloitte.com",
            "employer_company_type": "Consulting",
            "job_publisher": "Deloitte Jobs",
            "job_id": "XJyrP7Ypp5EAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Cloud Data Engineer - Healthcare",
            "job_apply_link": "https://jobsus.deloitte.com/dallas-tx/cloud-data-engineer-healthcare/5347059EE1A64B8EB63C4D1D17227594/job/",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7262,
            "job_description": "Are you an experienced, passionate pioneer in technology who wants to work in a collaborative environment? As an experienced Cloud Data Engineer - Healthcare you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. This position is working on a multi-year project for a major healthcare client. This is a remote role.\n\nWork you'll do/Responsibilities\n\nYou will determine processes and automation tools to reduce IT spend and increase efficiencies on multiple projects within the Healthcare domain.\n\nThis position includes collaborating with DevOps teams to implement CI/CD pipelines, automated deployments, and infrastructure as code (IaC) practices for AWS-based solutions. Document design, development, and deployment processes, as well as create technical specifications and user guides for developed solutions.\n\nYour role will be to design, develop, and deploy cloud-based solutions for data processing, analytics, and integration using cloud services and big data technologies. Collaborate with architects, data engineers, and business stakeholders to understand requirements and translate them into technical solutions.\n\nYou will implement data ingestion, transformation, and storage processes using cloud services like AWS's S3, Glue, Athena, Redshift, and EMR. Implement security, data governance, and compliance measures to ensure data integrity and protection in AWS-based solutions. Develop and optimize data pipelines using Snowpark, SnowSQL, Hadoop and PySpark to extract, transform, and load data efficiently.\n\nYou will conduct performance tuning and optimization of data processing and analytics workflows to maximize efficiency and scalability. Work with cross-functional teams to troubleshoot and resolve issues related to data processing, data integration, and analytics solutions.\n\nCommunicate regularly with Engagement Managers (Directors), project team members, and representatives from various functional and / or technical teams, including escalating any matters that require additional attention and consideration from engagement management\n\nThe Team\n\nAs a part of the US Strategy & Analytics Offering Portfolio, the AI & Data Operations offering provides managed AI, Intelligent Automation, and Data DevOps services across the advise-implement-operate spectrum.\n\nQualifications\n\nRequired\n\n\u2022 3 + years' experience as a Cloud Data Engineer\n\n\u2022 3 + years' hands on experience in Snowpark, SnowSQL, Hadoop and PySpark\n\n\u2022 3 + years' experience in AWS services such as S3, Glue, Athena, Redshift, EMR, Lambda and Cloud Formation.\n\n\u2022 3 + years' experience in Python with a focus on data processing and analytics\n\n\u2022 3+ years in healthcare domain\n\n\u2022 3 + years in consulting\n\n\u2022 Strong knowledge and hands-on experience in designing, developing, and deploying scalable solutions on the cloud platforms\n\n\u2022 Expertise in SQL and database technologies for data manipulation and querying\n\n\u2022 Bachelor's degree or equivalent experience\n\n\u2022 Limited immigration sponsorship may be available\n\nPreferred\n\n\u2022 Familiarity with data modeling, data warehousing, and data integration concepts.\n\n\u2022 Experience with DevOps practices, CI/CD pipelines, and infrastructure as code (IAAC) using tools like Jenkins, Git, and Terraform.\n\n\u2022 Strong analytical and problem-solving skills, with the ability to troubleshoot and resolve complex technical issues.\n\n\u2022 Familiarity with agile development methodologies and experience working in Agile teams\n\n\u2022 Ability to travel 10%, on average, based on the work you do and the clients and industries/sectors you serve\n\n\u2022 Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience\n\n\u2022 Analytical/ decision making responsibilities\n\n\u2022 Analytical ability to manage multiple projects and prioritize tasks into manageable work products\n\n\u2022 Can operate independently or with minimum supervision\n\n\u2022 Excellent communication skills\n\n\u2022 Ability to deliver technical demonstrations\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689047561,
            "job_posted_at_datetime_utc": "2023-07-11T03:52:41.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=XJyrP7Ypp5EAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "3 + years' experience as a Cloud Data Engineer",
                    "3 + years' hands on experience in Snowpark, SnowSQL, Hadoop and PySpark",
                    "3 + years' experience in AWS services such as S3, Glue, Athena, Redshift, EMR, Lambda and Cloud Formation",
                    "3 + years' experience in Python with a focus on data processing and analytics",
                    "3+ years in healthcare domain",
                    "3 + years in consulting",
                    "Strong knowledge and hands-on experience in designing, developing, and deploying scalable solutions on the cloud platforms",
                    "Expertise in SQL and database technologies for data manipulation and querying",
                    "Bachelor's degree or equivalent experience",
                    "Limited immigration sponsorship may be available"
                ],
                "Responsibilities": [
                    "You will determine processes and automation tools to reduce IT spend and increase efficiencies on multiple projects within the Healthcare domain",
                    "This position includes collaborating with DevOps teams to implement CI/CD pipelines, automated deployments, and infrastructure as code (IaC) practices for AWS-based solutions",
                    "Document design, development, and deployment processes, as well as create technical specifications and user guides for developed solutions",
                    "Your role will be to design, develop, and deploy cloud-based solutions for data processing, analytics, and integration using cloud services and big data technologies",
                    "Collaborate with architects, data engineers, and business stakeholders to understand requirements and translate them into technical solutions",
                    "You will implement data ingestion, transformation, and storage processes using cloud services like AWS's S3, Glue, Athena, Redshift, and EMR",
                    "Implement security, data governance, and compliance measures to ensure data integrity and protection in AWS-based solutions",
                    "Develop and optimize data pipelines using Snowpark, SnowSQL, Hadoop and PySpark to extract, transform, and load data efficiently",
                    "You will conduct performance tuning and optimization of data processing and analytics workflows to maximize efficiency and scalability",
                    "Work with cross-functional teams to troubleshoot and resolve issues related to data processing, data integration, and analytics solutions",
                    "Communicate regularly with Engagement Managers (Directors), project team members, and representatives from various functional and / or technical teams, including escalating any matters that require additional attention and consideration from engagement management"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541613",
            "job_naics_name": "Marketing Consulting Services"
        },
        {
            "employer_name": "Logic20/20 Inc.",
            "employer_logo": "https://res.cloudinary.com/crunchbase-production/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/ffwrq37xnvyhjt1nwrdm",
            "employer_website": "http://www.logic2020.com",
            "employer_company_type": null,
            "job_publisher": "Recruit.net",
            "job_id": "_M1aY3afaAIAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Cloud Data Engineer",
            "job_apply_link": "https://www.recruit.net/job/data-engineer-jobs/06F50F3CC7184312",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4154,
            "job_description": "Job Description\n\nAs a senior or lead Data Engineer joining Logic20/20's Advanced Analytics practice, you'll be supporting a team at one of the largest utility companies in California creating data pipelines to land data in an AWS data lake, and working with data transformation and integration to support future analytics. You will improve risk modeling that will allow the client to better understand the integrity of their pipelines for improved customer safety.\n\nHear more about these efforts\u202fas Jeff Lovington shares his experience working in Data Science and Machine Learning for the Energy & Utilities sector.\n\nAbout the Team\n\nThe Logic20/20 Advanced Analytics team is where skilled professionals in data engineering, data science, and visual analytics join forces to build simple solutions for complex data problems. We make it look like magic, but for us, it\u2019s all in a day\u2019s work. As part of our team, you\u2019ll collaborate on projects that help clients spin their data into a high-performance asset, all while enjoying the company of kindred spirits who are as committed to your success as you are. And when you\u2019re ready to level up in your career, you\u2019ll have access to the training, the project opportunities, and the mentorship to get you where you want to go.\n\n\u201cWe build an environment where we really operate as one team, building up each other\u2019s careers and capabilities.\u201d \u2013 Adam Cornille, Senior Director, Advanced Analytics",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690502400,
            "job_posted_at_datetime_utc": "2023-07-28T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=_M1aY3afaAIAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-27T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693094400,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Responsibilities": [
                    "You will improve risk modeling that will allow the client to better understand the integrity of their pipelines for improved customer safety"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Anblicks",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQELFQA-mOEuM-uxSMFuX5c178DZQmERReaPz9&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Built In",
            "job_id": "iIPKNvIu2lsAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer (Dallas, TX)",
            "job_apply_link": "https://builtin.com/job/data/senior-data-engineer/1600833",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7071,
            "job_description": "Job Title: Senior Data Engineer\n\nJOB DUTIES:\n\nParticipate in daily agile and scrum processes to understand changing business requirements, including examining system configurations and operating procedures, as well as functional requirements gathering. Ingest and prepare business-ready data in the cloud using Azure Data Factory (ADF) to build ELT(Extract Load and Transform)/ETL (Extract Transform and Load) data pipelines, then move the data into a data warehouse (Dedicated SQL Pool) and create data lake zones for data analytics and visualization. Work with a combination of Azure Data Factory and Azure Databricks, extract, load, and transform data from cloud sources and on-premises databases such as Oracle, SAP, and SQL Server to Data Lake Storage and Azure Synapse Analytics. Create an Azure Data Factory Pipeline Template to migrate an on-premises data platform to the Azure Cloud using batch processing methods by incremental or full load, and an ADF config driven framework to pull data from multiple sources with different table structures with less manual work and less resource usage. Analyze, design, and build modern data solutions using Azure PaaS services to support data visualization. Understand the current production state of the application and the impact of new implementation on existing business processes. Enable private end point, firewall setting and Azure Key Vault for robust data security. Analyze the existing SSIS packages and integrate it with Azure Data Factory, using SSIS transformations like Lookup, Derived column, Data conversion, Aggregate, Conditional split, SQL task, Script task, and Send Mail task. Create a JSON structure for data storage in Azure Cosmos DB (SQL API), write stored procedures and functions and work with the API team to create Cosmos DB queries that use fewer request units. Data Model in Snowflake and ELT using Snowflake SQL, implementing complex stored procedures and best practices with data warehouse and ETL concepts. Design and customize dimension data models for data warehouse supporting data using Azure Synapse Analytics, and select the appropriate distribution method in dimension and fact tables to load the data in an optimized manner, as well as implement complex stored procedures and best practices with data warehouse. Build a distributed in-memory application using spark applications and perform analytics efficiently on large datasets using python and Spark SQL, also use Spark SQL to implement transformation logic in Databricks and mount/unmount Azure Blob Storage. Read the data from different file format parquet, avro, csv and json using pySpark (Python API) in Azure Databricks and perform data extraction, transformation to uncover insights into customer usage patterns and insert curated data into a data warehouse. Create data visualization reports and dashboards in Power BI using data from the data warehouse, flat files, and Azure SQL. Responsible for fixing problems and conducting investigations into SQL queries, Stored Procedures related to long running jobs and Azure service performance. Utilize Azure Monitor and Alert services, create monitors, alerts, and notifications for Data Factory, Synapse Analytics, and Data Lake Storage. Perform the required daily GIT support for various projects. Be in charge of maintaining GIT repositories and access control procedures. Create CI&CD using azure dev ops pipeline to deploy Azure Services (Storage, Data factory, Key vault & Logic App) using ARM Templates.\n\nJOB REQUIREMENT:\n\nMaster's degree in Computer Science, Computer Information Systems, or Engineering related or Technical related fields plus 2 years of experience. In lieu of the above, we will also accept a Bachelor's degree in Computer Science, Computer Information Systems, or Engineering related or Technical related fields plus 5 years of progressively responsible post-baccalaureate experience. Foreign degree equivalent is acceptable. We will also accept any suitable combination of education, training and/or experience. Experience to include working on Azure Data Factory (ADF), Oracle, SQL server, Azure Databricks, Azure Synapse Analytics, Data Lake Storage, Azure PaaS services, SSIS packages, Azure Cosmos DB (SQL API), Python, Spark applications, Azure SQL, SQL queries, Azure Monitor and Alert services, GIT Support, ARM Templates.\n\nHOURS: M-F, 8:00 a.m. - 5:00 p.m.\n\nJOB LOCATION: Dallas, Texas. Travel is not required, but candidates must be willing to relocate to unanticipated work locations across the country per contract demand.\n\nCONTACT: Email resume referencing job code# SDE01302023ANB to Maruthi Technologies INC. DBA Anblicks at hr@anblicks.com",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1675497656,
            "job_posted_at_datetime_utc": "2023-02-04T08:00:56.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=iIPKNvIu2lsAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Create CI&CD using azure dev ops pipeline to deploy Azure Services (Storage, Data factory, Key vault & Logic App) using ARM Templates",
                    "In lieu of the above, we will also accept a Bachelor's degree in Computer Science, Computer Information Systems, or Engineering related or Technical related fields plus 5 years of progressively responsible post-baccalaureate experience",
                    "Foreign degree equivalent is acceptable",
                    "We will also accept any suitable combination of education, training and/or experience",
                    "Experience to include working on Azure Data Factory (ADF), Oracle, SQL server, Azure Databricks, Azure Synapse Analytics, Data Lake Storage, Azure PaaS services, SSIS packages, Azure Cosmos DB (SQL API), Python, Spark applications, Azure SQL, SQL queries, Azure Monitor and Alert services, GIT Support, ARM Templates"
                ],
                "Responsibilities": [
                    "Participate in daily agile and scrum processes to understand changing business requirements, including examining system configurations and operating procedures, as well as functional requirements gathering",
                    "Ingest and prepare business-ready data in the cloud using Azure Data Factory (ADF) to build ELT(Extract Load and Transform)/ETL (Extract Transform and Load) data pipelines, then move the data into a data warehouse (Dedicated SQL Pool) and create data lake zones for data analytics and visualization",
                    "Work with a combination of Azure Data Factory and Azure Databricks, extract, load, and transform data from cloud sources and on-premises databases such as Oracle, SAP, and SQL Server to Data Lake Storage and Azure Synapse Analytics",
                    "Analyze, design, and build modern data solutions using Azure PaaS services to support data visualization",
                    "Understand the current production state of the application and the impact of new implementation on existing business processes",
                    "Enable private end point, firewall setting and Azure Key Vault for robust data security",
                    "Analyze the existing SSIS packages and integrate it with Azure Data Factory, using SSIS transformations like Lookup, Derived column, Data conversion, Aggregate, Conditional split, SQL task, Script task, and Send Mail task",
                    "Create a JSON structure for data storage in Azure Cosmos DB (SQL API), write stored procedures and functions and work with the API team to create Cosmos DB queries that use fewer request units",
                    "Data Model in Snowflake and ELT using Snowflake SQL, implementing complex stored procedures and best practices with data warehouse and ETL concepts",
                    "Design and customize dimension data models for data warehouse supporting data using Azure Synapse Analytics, and select the appropriate distribution method in dimension and fact tables to load the data in an optimized manner, as well as implement complex stored procedures and best practices with data warehouse",
                    "Build a distributed in-memory application using spark applications and perform analytics efficiently on large datasets using python and Spark SQL, also use Spark SQL to implement transformation logic in Databricks and mount/unmount Azure Blob Storage",
                    "Read the data from different file format parquet, avro, csv and json using pySpark (Python API) in Azure Databricks and perform data extraction, transformation to uncover insights into customer usage patterns and insert curated data into a data warehouse",
                    "Create data visualization reports and dashboards in Power BI using data from the data warehouse, flat files, and Azure SQL",
                    "Responsible for fixing problems and conducting investigations into SQL queries, Stored Procedures related to long running jobs and Azure service performance",
                    "Utilize Azure Monitor and Alert services, create monitors, alerts, and notifications for Data Factory, Synapse Analytics, and Data Lake Storage",
                    "Perform the required daily GIT support for various projects",
                    "Be in charge of maintaining GIT repositories and access control procedures"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Amazon.com Services LLC",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Dallas, TX - Geebo",
            "job_id": "GVXNPo1qmE4AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://dallas-tx.geebo.com/jobs-online/view/id/1082323003-data-engineer-/",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.424,
            "job_description": "Job summaryAmazon is one of the largest employers on the planet with hundreds of thousands of employees across the globe.\nAmazon's HR mission is to build a workplace for Amazonians to invent on behalf of our customers.\nWe are enablers to help Amazon scale quickly and efficiently as we continue to expand rapidly across countries, business lines and size in general.\nThe volume of internal human resource transactions and range of experiences to fulfill in this scale and diversity are massive and unique respectively.\nThis requires a highly reliable, scalable and customized series of software applications that ensure our global employee base receives accurate, timely and best experiences throughout their employee lifecycle.\nAmazon has grown significantly since the deployment of these tools and we are now at an inflection point where we are making long term decisions to support our explosive global growth.\nOur Time and Attendance group is seeking a bright and motivated Data Engineer for leading the launch of timekeeping systems to support business expansions and deliver automations to achieve process efficiency and perfect pay for associates.\nJob\nResponsibilities:\nTranslate business and functional requirements into robust, scalable, operable solutions that work well within the overall data architecture.\nDesign, develop, implement, test, document, and operate large-scale, high-volume and low latency applications.\nBuild integrations between HR, Time & Attendance and Payroll systems to automate employee hire, schedule, punch and pay.\nDesigns data integrations and data quality framework.\nDevelops and maintains scalable data pipelines and builds out new API integrations.\nImplement data structures using best practices in data modeling, ETL/ELT processes, SQL, and Oracle.\nManage stakeholder communication, prioritization of tasks and on time solution delivery.\nParticipate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance.\nProduce comprehensive, usable dataset documentation and metadata.\nDesign and develop operational and analytical reports as per the customer needs by using the tools.\nEvaluate and make decisions around the use of new or existing software products and tools.\nMentor junior data engineers.\n.\nEstimated Salary: $20 to $28 per hour based on qualifications.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416000,
            "job_posted_at_datetime_utc": "2023-07-27T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=GVXNPo1qmE4AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-03T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1691020800,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 20,
            "job_max_salary": 28,
            "job_salary_currency": "USD",
            "job_salary_period": "HOUR",
            "job_highlights": {
                "Responsibilities": [
                    "Translate business and functional requirements into robust, scalable, operable solutions that work well within the overall data architecture",
                    "Design, develop, implement, test, document, and operate large-scale, high-volume and low latency applications",
                    "Build integrations between HR, Time & Attendance and Payroll systems to automate employee hire, schedule, punch and pay",
                    "Designs data integrations and data quality framework",
                    "Develops and maintains scalable data pipelines and builds out new API integrations",
                    "Implement data structures using best practices in data modeling, ETL/ELT processes, SQL, and Oracle",
                    "Manage stakeholder communication, prioritization of tasks and on time solution delivery",
                    "Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance",
                    "Produce comprehensive, usable dataset documentation and metadata",
                    "Design and develop operational and analytical reports as per the customer needs by using the tools",
                    "Evaluate and make decisions around the use of new or existing software products and tools"
                ],
                "Benefits": [
                    "Estimated Salary: $20 to $28 per hour based on qualifications"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Galderma S.A.",
            "employer_logo": "https://cdn.cookielaw.org/logos/9eb64978-23c0-4924-97a4-03eb6aab1106/83dc1859-114b-46cf-b32e-e3bbebc77feb/ad633653-014e-4c29-b120-efef9829e7a2/GALDERMA_LOGO_BLACK_RGB.jpg",
            "employer_website": "http://www.galderma.com",
            "employer_company_type": "Manufacturing",
            "job_publisher": "ZipRecruiter",
            "job_id": "6F_oejzQGBcAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.ziprecruiter.com/c/Galderma-S.A./Job/Data-Engineer/-in-Dallas,TX?jid=a2ca00aba7b601c2",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7258,
            "job_description": "With a unique legacy in dermatology as well as decades of cutting-edge innovation, Galderma is the pure-play dermatology category leader, present in approximately 90 countries. We deliver an innovative, science-based portfolio of premium flagship brands and services that spans the full spectrum of the fast-growing dermatology market through Injectable Aesthetics, Dermo-cosmetics, and Therapeutic Dermatology. Since our foundation in 1981, we have dedicated our focus and passion to the human body's largest organ - the skin - meeting individual consumer and patient needs with superior outcomes in partnership with healthcare professionals. Because we understand that the skin we're in shapes our lives, we are advancing dermatology for every skin story.\n\nWe look for people who focus on getting results, embrace learning and bring a positive energy. They must combine initiative with a sense of teamwork and collaboration. Above all, they must be passionate about doing something meaningful for consumers, patients, and the healthcare professionals we serve every day. We aim to empower each employee and promote their personal growth while ensuring business needs are met now and into the future. Across our company, we embrace diversity and respect the dignity, privacy, and personal rights of every employee.\n\nAt Galderma, we actively give our teams reasons to believe in our bold ambition to become the leading dermatology company in the world. With us, you have the ultimate opportunity to gain new and challenging work experiences and create an unparalleled, direct impact.\n\nJob Title: Data Engineer\n\nLocation: Dallas, TX (Hybrid)\n\nJob Description\n\nThe Data Engineer leads initiatives to develop and implement business-driven BI and Analytics solutions to enable business to win in the markets and gain competitive advantage. This individual serves as the primary liaison between IT Analytics, Shared Services, and the Business Units.\n\nKey Responsibilities\n\u2022 Understand customers' overall data estate business, related success measures, and IT priorities in order to design data solutions that drive business value\n\u2022 Educate the business and promote best practices on how to leverage BI and analytics solutions, as well as help facilitate discussions to anticipate future needs and opportunities\n\u2022 Assist in the identification and integration of data sources\n\u2022 Clearly communicate findings, recommendations, and opportunities to improve data systems and solutions\n\u2022 Seek out information to learn about emerging methodologies and technologies\n\u2022 Work closely with DevOps team to automate the deployment of resources to our various Azure subscription\n\u2022 Performs technical design reviews and code reviews\n\nSkills & Qualifications\n\u2022 Bachelor's degree in Information Technology or a related field, required\n\u2022 5 years of experience in implementing DWH / BI solutions\n\u2022 3 years of experience leading projects in analysis, architecture, design, and development of traditional data warehouse, data pipeline and business intelligence solutions\n\u2022 3 years of experience as a Data Engineer using Data Brick and Snowflake\n\u2022 Good understanding of various Data Models such as Dimensional Data Modeling and DataVault Modeling\n\u2022 Experience with Azure cloud services such as azure cloud security, monitoring, logging, scaling, caching, etc.\n\u2022 Experience dealing with cloud workloads, application, and infrastructure architecture, data ingestion architecture, data storage, and transformations, data analytics, serverless function/application design, micro-services, DevOps\n\u2022 Experience with Linux and cloud environment including commands and scripting\n\u2022 Solid experience in writing complex SQL queries as well as PL/SQL programs on Oracle database\n\u2022 Familiarity with DevOps best practices and automation of documentation, testing, build, deployment, configuration, and monitoring\n\u2022 Experience in visualization tools/solution such as Tableau and Power BI\n\u2022 Solid communication skills, particularly in writing technical solution proposals\n\nWhat we offer in return\n\nYou will be working for an organization that embraces diversity & inclusion and believe we will deliver better outcomes by reflecting the perspectives of our diverse customer base. You will also have access to a range of company benefits, including a competitive wage with shift differential, annual bonus opportunities and career advancement and cross-training.\n\nNext Steps\n\u2022 If your profile is a match, we will invite you for a first virtual conversation with the recruiter.\n\u2022 The next step is a virtual conversation with the hiring manager\n\u2022 The final step is a panel conversation with the extended team\n\nOur people make a difference:\n\nAt Galderma, you'll work with people who are like you. And people that are different. We value what every member of our team brings. Professionalism, collaboration, and a friendly, supportive ethos is the perfect environment for people to thrive and excel in what they do.\n\nEmployer's Rights:\n\nThis job description does not list all the duties of the job. You may be asked by your supervisors or managers to perform other duties. You will be evaluated in part based on your performance of the tasks listed in this job description. The employer has the right to revise this job description at any time. This job description is not an employment contract, and either you or the employer may terminate employment at any time, for any reason. In addition, reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689552000,
            "job_posted_at_datetime_utc": "2023-07-17T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=6F_oejzQGBcAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693008000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's degree in Information Technology or a related field, required",
                    "5 years of experience in implementing DWH / BI solutions",
                    "3 years of experience leading projects in analysis, architecture, design, and development of traditional data warehouse, data pipeline and business intelligence solutions",
                    "3 years of experience as a Data Engineer using Data Brick and Snowflake",
                    "Good understanding of various Data Models such as Dimensional Data Modeling and DataVault Modeling",
                    "Experience with Azure cloud services such as azure cloud security, monitoring, logging, scaling, caching, etc",
                    "Experience dealing with cloud workloads, application, and infrastructure architecture, data ingestion architecture, data storage, and transformations, data analytics, serverless function/application design, micro-services, DevOps",
                    "Experience with Linux and cloud environment including commands and scripting",
                    "Solid experience in writing complex SQL queries as well as PL/SQL programs on Oracle database",
                    "Familiarity with DevOps best practices and automation of documentation, testing, build, deployment, configuration, and monitoring",
                    "Experience in visualization tools/solution such as Tableau and Power BI",
                    "Solid communication skills, particularly in writing technical solution proposals"
                ],
                "Responsibilities": [
                    "This individual serves as the primary liaison between IT Analytics, Shared Services, and the Business Units",
                    "Understand customers' overall data estate business, related success measures, and IT priorities in order to design data solutions that drive business value",
                    "Educate the business and promote best practices on how to leverage BI and analytics solutions, as well as help facilitate discussions to anticipate future needs and opportunities",
                    "Assist in the identification and integration of data sources",
                    "Clearly communicate findings, recommendations, and opportunities to improve data systems and solutions",
                    "Seek out information to learn about emerging methodologies and technologies",
                    "Work closely with DevOps team to automate the deployment of resources to our various Azure subscription",
                    "Performs technical design reviews and code reviews"
                ],
                "Benefits": [
                    "You will also have access to a range of company benefits, including a competitive wage with shift differential, annual bonus opportunities and career advancement and cross-training"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-2041.00: Statisticians"
            ],
            "job_naics_code": "325412",
            "job_naics_name": "Pharmaceutical Preparation Manufacturing"
        },
        {
            "employer_name": "Sage IT Inc",
            "employer_logo": "https://sageitinc.com/sagefiles/2021/12/Sage-IT-Logo-SVG.svg",
            "employer_website": "http://sageitinc.com",
            "employer_company_type": null,
            "job_publisher": "Salary.com",
            "job_id": "42NJql7l_7EAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Azure Data Engineer",
            "job_apply_link": "https://www.salary.com/job/sage-it-inc/azure-data-engineer/j202301310152169988722",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6615,
            "job_description": "Job Description\nHello\nHope you are doing well.\nMy name is Fayyaz, and I am a staffing specialist in Sage IT. We are looking for Aws Data Engineer with one of our Client\n\nTo speed up the process, I would really appreciate it if you could review the job description below and get back to me with the most recent word format of your Resume at .\n\nTitle : . Azure Data Engineer\nLocation : Jersey City, NJ\nJob Type : W2 Contract Role\nLocation: Plano TX || Day One Hybrid Onsite\n\nRoles and Responsibilities:\nEssential Skills/Basic Qualifications:\nThe Data Engineer Lead will be responsible for guiding development of multiple (2-4) projects at a single time.\nHis/her tasks will include directing Data Engineer team members in best practices and design/implementation decisions to support Data and Analytics Business Customers.\nHe/she will also be responsible to actively develop at least one of the projects under their leadership.\nThe proper candidate will be expected to identify risks and potential blockers and elevate appropriately or mitigate as needed.\nThis candidate should have strong communication skills.\nTechnical Skills: SAP Development Very Strong\nAzure Development Strong to Very Strong\n\nFor more information, you can reach me onlt;/div> Have a great day ahead\n\nAbu Fayyaz | Sales Manager\nDirect:\n\n;/b>\n\nReport this job\n\u2022 Dice Id: 10120222\n\u2022 Position Id: 2023-38552\n\u2022",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690243200,
            "job_posted_at_datetime_utc": "2023-07-25T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=42NJql7l_7EAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-10-31T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1698710400,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "The proper candidate will be expected to identify risks and potential blockers and elevate appropriately or mitigate as needed",
                    "This candidate should have strong communication skills",
                    "Technical Skills: SAP Development Very Strong"
                ],
                "Responsibilities": [
                    "The Data Engineer Lead will be responsible for guiding development of multiple (2-4) projects at a single time",
                    "His/her tasks will include directing Data Engineer team members in best practices and design/implementation decisions to support Data and Analytics Business Customers",
                    "He/she will also be responsible to actively develop at least one of the projects under their leadership"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Koch Industries",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQJLPxwM4W8LjSXGoaKBgxNiDWrXZE5FSPfBg&usqp=CAU",
            "employer_website": "http://www.kochind.com",
            "employer_company_type": "Manufacturing",
            "job_publisher": "LinkedIn",
            "job_id": "sli4eini4GgAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Security Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/security-data-engineer-at-koch-industries-3675614617",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5519,
            "job_description": "Your Job\n\nThe Koch Cyber Security Team is seeking a Security Data Engineer to join our global cyber security team. As a member of this team your primary duties will be to attend to the health and support of our SIEM tool and work closely with our analyst to ensure that data is available to them when needed.\n\nOur Team\n\nThe Koch Cyber Security team is a dynamic and proactive force, fueled by an unwavering commitment to Koch's vision for value creation. With a relentless drive, we tackle cyber threats head-on, always ready to protect our stakeholders from any potential harm. Our team members are trailblazers, spearheading transformational efforts in areas such as Incident Response, Automation, exposure management, awareness, and the ever-evolving cyber landscape. We thrive on challenges and constantly seek innovative solutions to safeguard our organization and its interests.\n\nWhat You Will Do\n\u2022 Oversee the ingestion and normalization of new data sources.\n\u2022 Maintain data availability\n\u2022 Detect and remediate any drop in data ingestion\n\u2022 Support, maintain and improve infrastructure for the data collection tools overall health\n\u2022 Respond to customer inquiries surrounding the collection of data\n\u2022 Work closely with Cyber Security team to ensure that analyst have access to the data they need and that it\u2019s presented in a clear and concise manner\n\u2022 Stay up to date with the latest trends and technologies in data engineering and cloud infrastructure management and applying them to our data collection tool stack\n\u2022 Actively seek ways to improve our current data collection tool stack.\n\nWho You Are (Basic Qualifications)\n\u2022 Experience supporting a SIEM tool\n\u2022 Experience deploying Infrastructure in cloud environments\n\u2022 Experience building and troubleshooting data pipelines.\n\nWhat Will Put You Ahead\n\u2022 Experience with Global Information\\Cyber Security Teams\n\u2022 Experience with Machine Learning\n\u2022 Experience with Multiple Operating Systems\n\u2022 Knowledge with PowerShell, JavaScript, or Python Scripting\n\u2022 Windows Or Linux Administration Experience\n\u2022 SOAR Platform Experience\n\u2022 Familiar with CIM date compliance\n\nAt Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate\u2019s knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.\n\nHiring Philosophy\n\nAll Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.\n\nWho We Are\n\nAs a Koch company, Koch Global Services (KGS) creates solutions spanning technology, human resources, finance, project management and anything else our businesses need. With locations in India, Mexico, Poland and the United States, our employees have the opportunity to make a global impact.\n\nAt Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.\n\nOur Benefits\n\nOur goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.\n\nEqual Opportunities\n\nEqual Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690317872,
            "job_posted_at_datetime_utc": "2023-07-25T20:44:32.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "retirement_savings",
                "dental_coverage",
                "paid_time_off"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=sli4eini4GgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-24T20:44:32.000Z",
            "job_offer_expiration_timestamp": 1692909872,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience supporting a SIEM tool",
                    "Experience deploying Infrastructure in cloud environments",
                    "Experience building and troubleshooting data pipelines",
                    "Experience with Global Information\\Cyber Security Teams",
                    "Experience with Machine Learning",
                    "Experience with Multiple Operating Systems",
                    "Knowledge with PowerShell, JavaScript, or Python Scripting",
                    "Windows Or Linux Administration Experience",
                    "SOAR Platform Experience",
                    "Familiar with CIM date compliance"
                ],
                "Responsibilities": [
                    "Oversee the ingestion and normalization of new data sources",
                    "Maintain data availability",
                    "Detect and remediate any drop in data ingestion",
                    "Support, maintain and improve infrastructure for the data collection tools overall health",
                    "Respond to customer inquiries surrounding the collection of data",
                    "Work closely with Cyber Security team to ensure that analyst have access to the data they need and that it\u2019s presented in a clear and concise manner",
                    "Stay up to date with the latest trends and technologies in data engineering and cloud infrastructure management and applying them to our data collection tool stack",
                    "Actively seek ways to improve our current data collection tool stack"
                ],
                "Benefits": [
                    "Any compensation range provided for a role is an estimate determined by available market data",
                    "Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15112200",
            "job_onet_job_zone": "4",
            "job_naics_code": "32519",
            "job_naics_name": "Other Basic Organic Chemical Manufacturing"
        },
        {
            "employer_name": "Ryan, LLC",
            "employer_logo": "http://companies.naukri.com/ryan-jobs/wp-content/uploads/sites/2082/2015/02/product.jpg",
            "employer_website": "http://www.ryan.com",
            "employer_company_type": null,
            "job_publisher": "Glassdoor",
            "job_id": "8SyHUOO43CsAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer, Data Engineering",
            "job_apply_link": "https://www.glassdoor.com/job-listing/senior-data-engineer-data-engineering-ryan-llc-JV_IC1139977_KO0,37_KE38,46.htm?jl=1008660428279",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.566,
            "job_description": "The Senior Data Engineer is responsible for the identifying, developing, and maintaining the technologies that enable the efficient flow of data throughout the organization. This role requires an enterprise mindset to build out robust, high-performance technology.\nDuties and Responsibilities, aligned with Key Results:\n\nPeople\n\u2022 Use a variety of programming languages and tools to develop, test, and maintain data pipelines within the Platform Reference Architecture.\n\u2022 Working directly with management, product teams and practice personnel to understand their platform data requirements\n\u2022 Maintaining a positive work atmosphere by behaving and communicating in a manner that encourages productive interactions with customers, co-workers and supervisors\n\u2022 Developing and engaging with team members by creating a motivating work environment that recognizes, holds team members accountable, and rewards strong performance\n\u2022 Fostering an innovative, inclusive and diverse team environment, promoting positive team culture, encouraging collaboration and self-organization while delivering high quality solutions\n\nClient\n\u2022 Collaborating on an Agile team to design, develop, test, implement and support highly scalable data solutions\n\u2022 Collaborating with product teams and clients to deliver robust cloud-based data solutions that drive tax decisions and provide powerful experiences\n\u2022 Analyzing user feedback and activity and iterate to improve the services and user experience\n\nValue\n\u2022 Securing data in alignment with internal information and data security policies, best practices and client requirements\n\u2022 Creating and implementing robust cloud-based data solutions that scale effectively, and provide powerful experiences for both internal teams and clients\n\u2022 Performing unit tests and conducting reviews with other team members to make sure solutions and code are rigorously designed, elegantly coded and effectively tuned for performance\n\u2022 Staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities and mentoring other members of the engineering community\n\u2022 Perform other duties as assigned\n\nEducation and Experience:\n\nBachelor\u2019s and/or Master\u2019s degree in a related field.\n\u2022 7+ years of experience developing data technologies.\n\u2022 7+ years of experience deploying ETL solutions in production environments.\n\u2022 7+ years of experience with cloud-based data services, preferably in AWS or Azure.\n\u2022 7+ years of experience developing Python, Scala, Java, .Net or similar solutions in a backend or data wrangling capacity.\n\u2022 7+ years of experience in mixed Windows/Linux environments.\n\nAdditional Required Skills and Experience:\n\u2022 Results-proven track record of exceeding goals and evidence of the ability to consistently make good decisions through a combination of analysis, experience and judgment\n\u2022 Fluency in one or more databases, preferably relational and NoSQL is a plus.\n\u2022 Experience with distributed data platforms is a plus.\n\u2022 Exposure to AI/ML pipelines is preferred.\n\u2022 Experience deploying, monitoring, and maintaining data pipelines in production environments\n\nComputer Skills:\n\nTo perform this job successfully, an individual must have intermediate knowledge of Microsoft Project, Word, Excel, Access, PowerPoint, Outlook, and Internet navigation and research.\n\nSupervisory Responsibilities:\n\nRequires supervisory responsibilities, including training employees, assigning work, and assuring quality throughout any deliverables\n\nWork Environment:\n\u2022 Standard indoor working environment.\n\u2022 Occasional long periods of sitting while working at computer.\n\u2022 Position requires regular interaction with employees at all levels of the Firm and interface with external vendors as necessary.\n\u2022 Independent travel requirement: As Needed\n\nRemote position\n\u2022 For Denver, CO-based roles, the base salary hiring range for this position is\n\n$135,000 - $165,000.\n\u2022 For New York, NY-based roles, the base salary hiring range for this position is\n\n$155,000-$185,000.\n\u2022 For Bellevue, WA- based roles, the base salary hiring range for this position is\n\n$140,750-$173,000.\n\u2022 For Carlsbad, Glendale, Irvine, Los Angelos, Sacramento, and San Diego, CA-based roles, the base salary hiring range for this position is $140,750-$173,000.\n\u2022 For Oakland and San Jose, CA-based roles, the base salary hiring range for this position is $155,000-$185,000.\n\u2022 The Company makes offers based on many factors, including qualifications and experience",
            "job_is_remote": true,
            "job_posted_at_timestamp": 1689984000,
            "job_posted_at_datetime_utc": "2023-07-22T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=8SyHUOO43CsAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 135000,
            "job_max_salary": 165000,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "Bachelor\u2019s and/or Master\u2019s degree in a related field",
                    "7+ years of experience deploying ETL solutions in production environments",
                    "7+ years of experience with cloud-based data services, preferably in AWS or Azure",
                    "7+ years of experience developing Python, Scala, Java, .Net or similar solutions in a backend or data wrangling capacity",
                    "7+ years of experience in mixed Windows/Linux environments",
                    "Results-proven track record of exceeding goals and evidence of the ability to consistently make good decisions through a combination of analysis, experience and judgment",
                    "Experience deploying, monitoring, and maintaining data pipelines in production environments",
                    "To perform this job successfully, an individual must have intermediate knowledge of Microsoft Project, Word, Excel, Access, PowerPoint, Outlook, and Internet navigation and research"
                ],
                "Responsibilities": [
                    "This role requires an enterprise mindset to build out robust, high-performance technology",
                    "Use a variety of programming languages and tools to develop, test, and maintain data pipelines within the Platform Reference Architecture",
                    "Working directly with management, product teams and practice personnel to understand their platform data requirements",
                    "Maintaining a positive work atmosphere by behaving and communicating in a manner that encourages productive interactions with customers, co-workers and supervisors",
                    "Developing and engaging with team members by creating a motivating work environment that recognizes, holds team members accountable, and rewards strong performance",
                    "Fostering an innovative, inclusive and diverse team environment, promoting positive team culture, encouraging collaboration and self-organization while delivering high quality solutions",
                    "Collaborating on an Agile team to design, develop, test, implement and support highly scalable data solutions",
                    "Collaborating with product teams and clients to deliver robust cloud-based data solutions that drive tax decisions and provide powerful experiences",
                    "Analyzing user feedback and activity and iterate to improve the services and user experience",
                    "Creating and implementing robust cloud-based data solutions that scale effectively, and provide powerful experiences for both internal teams and clients",
                    "Performing unit tests and conducting reviews with other team members to make sure solutions and code are rigorously designed, elegantly coded and effectively tuned for performance",
                    "Staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities and mentoring other members of the engineering community",
                    "Perform other duties as assigned",
                    "Requires supervisory responsibilities, including training employees, assigning work, and assuring quality throughout any deliverables",
                    "Occasional long periods of sitting while working at computer"
                ],
                "Benefits": [
                    "For Oakland and San Jose, CA-based roles, the base salary hiring range for this position is $155,000-$185,000"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Wipro Limited",
            "employer_logo": "https://www.wipro.com/content/dam/nexus/en/wipro-logo-new-og-502x263.jpg",
            "employer_website": "http://www.wipro.com",
            "employer_company_type": "Computer Services",
            "job_publisher": "OPTnation",
            "job_id": "qIeZ_iz27YwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.optnation.com/data-engineer-job-in-dallas-tx-view-jobid-29161",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6901,
            "job_description": "Responsibilities Design data integrations and data quality framework using AWS Cloud technologies. Develop and maintain scalable data pipelines and ETL processes using Python. Write unit/integration tests contribute to engineering wiki and document work. SQL Performance & Tuning Implement processes to monitor data quality ensuring high availability and high accuracy of production data. Provide operational support for the data pipeline and perform data analysis required to troubleshoot data related issues and the resolution. Collaborate with analytics and business teams to build data models that feed business intelligence tools to foster data-driven decision making across the organization. Work closely with the business users product managers analysts and off-shore team (India) to develop strategy for long term data architecture and the development for the business solution. Requirements BS or MS degree in Computer Science or a related technical field 0-2 years of experience in the Information Technology field.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1677196800,
            "job_posted_at_datetime_utc": "2023-02-24T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=qIeZ_iz27YwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "python",
                "AWS",
                "ETL",
                "Data modeling",
                "MySQL"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 120000,
            "job_max_salary": 140000,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "Requirements BS or MS degree in Computer Science or a related technical field 0-2 years of experience in the Information Technology field"
                ],
                "Responsibilities": [
                    "Responsibilities Design data integrations and data quality framework using AWS Cloud technologies",
                    "Develop and maintain scalable data pipelines and ETL processes using Python",
                    "Write unit/integration tests contribute to engineering wiki and document work",
                    "SQL Performance & Tuning Implement processes to monitor data quality ensuring high availability and high accuracy of production data",
                    "Provide operational support for the data pipeline and perform data analysis required to troubleshoot data related issues and the resolution",
                    "Collaborate with analytics and business teams to build data models that feed business intelligence tools to foster data-driven decision making across the organization",
                    "Work closely with the business users product managers analysts and off-shore team (India) to develop strategy for long term data architecture and the development for the business solution"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541511",
            "job_naics_name": "Custom Computer Programming Services"
        },
        {
            "employer_name": "Randstad USA",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Monster",
            "job_id": "6VJKKZ1oBBkAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Sr. Data Engineer",
            "job_apply_link": "https://www.monster.com/job-openings/sr-data-engineer-dallas-tx--37e67443-4bdb-4d63-a605-9f6778fa8531",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.6571,
            "job_description": "job summary:\n\u2022 strong in real time & batch pipelines in big data technologies (i.e. Spark/Kafka/Cassandra/Hadoop/Hive/Elasticsearch)\n\u2022 Proficient in RESTful Services, Java, Scala, Spring Boot/Play Framework, RDBMS, NoSql, Python\n\u2022 Proficient in development of scalable cloud native microservices\n\u2022 Proficient with Designing and building APIs\n\nlocation: Dallas, Texas\njob type: Contract\nsalary: $70 - 75 per hour\nwork hours: 8am to 4pm\neducation: No Degree Required\n\nresponsibilities:\n\nstrong in real time & batch pipelines in big data technologies\n\u2022 (i.e. Spark/Kafka/Cassandra/Hadoop/Hive/Elasticsearch)\n\nProficient in RESTful Services, Java, Scala, Spring Boot/Play Framework, RDBMS, NoSql, Python\n\u2022 Proficient in development of scalable cloud native microservices\n\u2022 Proficient with Designing and building APIs\n\nAzure and or GCP exposure and experience\n\nqualifications:\n\u2022 Experience level: Experienced\n\u2022 Minimum 7 years of experience\n\u2022 Education: No Degree Required\n\nskills:\n\u2022 Hadoop (7 years of experience is required)\n\u2022 Java (7 years of experience is required)\n\nEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.\n\nAt Randstad, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants. If you require a reasonable accommodation to make your application or interview experience a great one, please contact HRsupport@randstadusa.com.\n\nPay offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc. In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility).\n\nFor certain assignments, Covid-19 vaccination and/or testing may be required by Randstad's client or applicable federal mandate, subject to approved medical or religious accommodations. Carefully review the job posting for details on vaccine/testing requirements or ask your Randstad representative for more information.\n\nAbout the Company:\nRandstad USA\n\nRandstad was founded in 1960 by Frits Goldschmeding. We've never let go of his passion or the values that he established. By staying true to those fundamentals, we've expanded to represent more than 90 percent of the HR services market.\n\nWe provide outsourcing, staffing, consulting and workforce solutions within the areas of engineering, accounting and finance, healthcare, human resources, IT, legal, life sciences, manufacturing and logistics, office and administration and sales and marketing. We can\u2019t wait to tell you all about it.\n\nOur mission is to be a world leader in matching demand for, and supply of, labor and HR services. We believe in the value of work as a unifying force that shapes society for the better. We live by the core values established early in our company's history: to know, serve and trust, striving for perfection and simultaneous promotion of all interests.\n\nCompany Size:\n10,000 employees or more\n\nIndustry:\nStaffing/Employment Agencies\n\nFounded:\n1960\n\nWebsite:\nhttps://www.randstadusa.com/",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416000,
            "job_posted_at_datetime_utc": "2023-07-27T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "health_insurance",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=6VJKKZ1oBBkAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Proficient in RESTful Services, Java, Scala, Spring Boot/Play Framework, RDBMS, NoSql, Python",
                    "Proficient in development of scalable cloud native microservices",
                    "Proficient with Designing and building APIs",
                    "Azure and or GCP exposure and experience",
                    "Experience level: Experienced",
                    "Minimum 7 years of experience",
                    "Hadoop (7 years of experience is required)"
                ],
                "Responsibilities": [
                    "work hours: 8am to 4pm"
                ],
                "Benefits": [
                    "salary: $70 - 75 per hour",
                    "Pay offered to a successful candidate will be based on several factors including the candidate's education, work experience, work location, specific job duties, certifications, etc",
                    "In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility)"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Cognizant Technology Solutions",
            "employer_logo": "https://www.cognizant.com/content/dam/cognizant/en_us/dotcom/logos/COG-Logo-2022.svg",
            "employer_website": "http://www.cognizant.com",
            "employer_company_type": "Computer Services",
            "job_publisher": "Ladders",
            "job_id": "Hxm5v2wJMHUAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer- Databricks",
            "job_apply_link": "https://www.theladders.com/job/senior-data-engineer-databricks-cognizant-dallas-tx_64139874",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5671,
            "job_description": "Senior Data Engineer- DataBricks\n\nLocation: Dallas, TX (Hybrid/Onsite)\n\nCognizant (NASDAQ: CTSH) is a leading provider of information technology, consulting, and business process outsourcing services, dedicated to helping the world's leading companies build stronger businesses. Headquartered in Teaneck, New Jersey (U.S.). Cognizant is a member of the NASDAQ-100, the S&P 500, the Forbes Global 1000, and the Fortune 500 and we are among the top performing and fastest growing companies in the world.\n\nPlease note, this role is not able to offer visa transfer or sponsorship now or in the future\n\nPractice - AIA - Artificial Intelligence and Analytics\n\nQualification:\n\u2022 Bachelors in science , engineering or equivalent\n\u2022 10+ years of experience building, deploying, and designing data solutions: ETL, data warehousing, or Big Data.\n\u2022 3-6 Years of experience working with DataBricks.\n\u2022 Experience working with Azure Data stack; Data Lake, Data factory, DataBricks, Synapse, etc.\n\nResponsibility:\n\u2022 Provide input to cloud engineer for the design and implementation of data management and/or architecture solutions.\n\u2022 Partner with cloud engineer and ML engineer to develop and evolve the concept of data ops.\n\u2022 Design implement and deploy data loaders to load data into the Engineering Sandbox.\n\u2022 Assist in pulling filtering tagging joining parsing and normalizing data sets for use.\n\u2022 Work with the analytics translator data quality analyst and IT to resolve data quality issues.\n\u2022 Experience in designing and implementing large scale data loading manipulation processing analysis and exploration solutions.\n\u2022 Deep technical expertise with pulling and massaging data understanding of first/third party data.\n\u2022 Experience in R Python Modeling Big Data etc. with focus on AI/ML techniques\n\u2022 Advanced SQL skills and understanding of data management principles and processes.\n\nMust Have Skills\n\u2022 Databricks\n\nGood To Have Skills\n\u2022 PySpark\n\u2022 Azure Databricks\n\u2022 Azure Data Factory\n\u2022 eCommerce\n\nSalary and Other Compensation:\n\nThis position is also eligible for Cognizant\u2019s discretionary annual incentive program, based on performance and subject to the terms of Cognizant\u2019s applicable plans.\n\nBenefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:\n\u2022 \u00b7 Medical/Dental/Vision/Life Insurance\n\u2022 \u00b7 Paid holidays plus Paid Time Off\n\u2022 \u00b7 401(k) plan and contributions\n\u2022 \u00b7 Long-term/Short-term Disability\n\u2022 \u00b7 Paid Parental Leave\n\u2022 \u00b7 Employee Stock Purchase Plan\n\nDisclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.\n\nCognizant is an Equal Opportunity Employer M/F/D/V. Cognizant is committed to ensuring that all current and prospective associates are afforded equal opportunities and treatment and a work environment free of harassment.\n\nCognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.\n\n#LI-IR1 #CB #Ind123\n\nEmployee Status : Full Time Employee\n\nShift : Day Job\n\nTravel : No\n\nJob Posting : Jun 30 2023\n\nAbout Cognizant\n\nCognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead wiApplicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.\n\nCognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.\n\nIf you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, with your request and contact information.\n\nBe aware of fraudulent offers!!\n\nCognizant does not charge anything at any stage of the recruitment process and has not authorized any agencies or partners or individuals to charge any fee at any stage of the recruitment process. In case you receive offers that you suspect are fraudulent,By this, we want to avoid and prevent unaware prospective candidates from falling victim to these scams.\n\nKindly note that any payment made to either individuals or agencies for gaining employment at Cognizant, will be at your own risk & volition and Cognizant cannot be held accountable for the same.\nGet notified for similar jobs\n\nSign up to receive job alerts\n\nEnter Email address\n\nSUBMIT",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688352222,
            "job_posted_at_datetime_utc": "2023-07-03T02:43:42.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "dental_coverage",
                "retirement_savings",
                "paid_time_off",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Hxm5v2wJMHUAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 120,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 100000,
            "job_max_salary": 150000,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "Bachelors in science , engineering or equivalent",
                    "10+ years of experience building, deploying, and designing data solutions: ETL, data warehousing, or Big Data",
                    "3-6 Years of experience working with DataBricks",
                    "Experience working with Azure Data stack; Data Lake, Data factory, DataBricks, Synapse, etc",
                    "PySpark",
                    "eCommerce"
                ],
                "Responsibilities": [
                    "Provide input to cloud engineer for the design and implementation of data management and/or architecture solutions",
                    "Partner with cloud engineer and ML engineer to develop and evolve the concept of data ops",
                    "Design implement and deploy data loaders to load data into the Engineering Sandbox",
                    "Assist in pulling filtering tagging joining parsing and normalizing data sets for use",
                    "Work with the analytics translator data quality analyst and IT to resolve data quality issues",
                    "Experience in designing and implementing large scale data loading manipulation processing analysis and exploration solutions",
                    "Deep technical expertise with pulling and massaging data understanding of first/third party data",
                    "Advanced SQL skills and understanding of data management principles and processes"
                ],
                "Benefits": [
                    "This position is also eligible for Cognizant\u2019s discretionary annual incentive program, based on performance and subject to the terms of Cognizant\u2019s applicable plans",
                    "Medical/Dental/Vision/Life Insurance",
                    "Paid holidays plus Paid Time Off",
                    "401(k) plan and contributions",
                    "Long-term/Short-term Disability",
                    "Paid Parental Leave",
                    "Employee Stock Purchase Plan",
                    "Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541511",
            "job_naics_name": "Custom Computer Programming Services"
        },
        {
            "employer_name": "Infinity Consulting Solutions",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTuUR1-jQ4Y1CJaEdnrJ5J24Ybu94wWwoqmUKpK&s=0",
            "employer_website": "http://www.infinity-cs.com",
            "employer_company_type": null,
            "job_publisher": "Lensa",
            "job_id": "0uC1tfLfQNoAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://lensa.com/data-engineer-jobs/dallas/jd/b8d9c2125bf7388601b5a0f327748a49",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4713,
            "job_description": "ICS, a Korn Ferry company, is looking for a Data Engineer, who is a SQL and SSIS expert, to join our growing Financial Services' client's team in the Dallas/Fort Worth, TX area! This position will sit on the Data Services Team and be responsible for sourcing data and loading it into the enterprise data warehouse across multiple projects in the organization. The ideal candidate will be a self-starter and a team player!\n\nWhat You'll Do:\n\u2022 Communicating with Technology teams and business owners to develop, test, and implement reports for internal and external users.\n\u2022 The developer will be responsible for the design, development, and deployment of data extracts, transformations, and loads.\n\u2022 ETL development will utilize the Microsoft SQL platform, specifically SQL Server Integration Services (SSIS)\n\u2022 Experience with various reporting methods, including SSRS.\n\u2022 Develop reports based on defined requirements.\n\u2022 Responsible for analyzing the business requirements for data file requests, designing, developing, and deploying fully automated ETL jobs to produce data files\n\u2022 Write advanced SQL statements to retrieve data from tables and stored procedures\n\u2022 Use known education principles and stay up-to-date on new reporting methods and techniques.\n\u2022 Partner with internal stakeholders and liaise with experts regarding instructional design\n\nSkills Required:\n\u2022 Experience with MS SQL Server and T-SQL code development\n\u2022 Experience with ETL - SSIS\n\u2022 Experience in building stored procedures, functions, merges and Ad Hoc scripts\n\u2022 Experience with data management and understanding of data structures, database design\n\u2022 Ability to collaborate and communicate effectively across functional groups\n\u2022 Ability to independently perform database development, testing and implementation with little supervision\n\u2022 Ability to effectively communicate technical issues and resolve problems\n\u2022 Experience with C# or Python a plus\n\u2022 Visual studio to create SSIS package, stored procedure, code review\n\nTITLE: Data Engineer\n\nClient Industry: Financial Services, Industry\n\nLocation: Remote in the Dallas/Fort Worth area\n\nPay Rate: $50-60/hour",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689437824,
            "job_posted_at_datetime_utc": "2023-07-15T16:17:04.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=0uC1tfLfQNoAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-14T16:17:04.000Z",
            "job_offer_expiration_timestamp": 1692029824,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": [
                "Reading Comprehension",
                "Active Listening",
                "Writing",
                "Speaking",
                "Critical Thinking",
                "Active Learning",
                "Monitoring",
                "Social Perceptiveness",
                "Coordination",
                "Complex Problem Solving",
                "Programming",
                "Judgment and Decision Making",
                "Systems Analysis",
                "Systems Evaluation"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Experience with MS SQL Server and T-SQL code development",
                    "Experience with ETL - SSIS",
                    "Experience in building stored procedures, functions, merges and Ad Hoc scripts",
                    "Experience with data management and understanding of data structures, database design",
                    "Ability to collaborate and communicate effectively across functional groups",
                    "Ability to independently perform database development, testing and implementation with little supervision",
                    "Ability to effectively communicate technical issues and resolve problems",
                    "Visual studio to create SSIS package, stored procedure, code review"
                ],
                "Responsibilities": [
                    "This position will sit on the Data Services Team and be responsible for sourcing data and loading it into the enterprise data warehouse across multiple projects in the organization",
                    "The ideal candidate will be a self-starter and a team player!",
                    "Communicating with Technology teams and business owners to develop, test, and implement reports for internal and external users",
                    "The developer will be responsible for the design, development, and deployment of data extracts, transformations, and loads",
                    "ETL development will utilize the Microsoft SQL platform, specifically SQL Server Integration Services (SSIS)",
                    "Experience with various reporting methods, including SSRS",
                    "Develop reports based on defined requirements",
                    "Responsible for analyzing the business requirements for data file requests, designing, developing, and deploying fully automated ETL jobs to produce data files",
                    "Write advanced SQL statements to retrieve data from tables and stored procedures",
                    "Use known education principles and stay up-to-date on new reporting methods and techniques",
                    "Partner with internal stakeholders and liaise with experts regarding instructional design"
                ],
                "Benefits": [
                    "Pay Rate: $50-60/hour"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06 Database Architects"
            ]
        },
        {
            "employer_name": "Salesforce",
            "employer_logo": "https://www.sfdcstatic.com/common/assets/img/logo-company-large.png",
            "employer_website": "http://www.salesforce.com",
            "employer_company_type": "Information",
            "job_publisher": "Dallas, TX - Geebo",
            "job_id": "hmPiuV0MhssAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Associate Data Engineer, Enterprise Data Warehouse at Salesforce in Dallas, TX",
            "job_apply_link": "https://dallas-tx.geebo.com/jobs-online/view/id/1051796261-associate-data-engineer-enterprise-/",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.4366,
            "job_description": "Job Details Business Technology blazes the trail of enterprise IT. Built on the foundation of our core values, we own more than the traditional IT components with a heavy focus on working closely with our business partners for amazing outcomes. Our goal is to deliver technology that is centered around our business and our collective success. We oversee technology strategy, Salesforce on Salesforce, customer and partner enablement, applications engineering, infrastructure, collaboration, enterprise operations, architecture, and program enablement. We own the world's foremost Salesforce implementation and enable our global Ohana to do their best work by leveraging our platform. The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, Customer success and Employee Success. This platform also includes management of a Big Data Hadoop platform to store unstructured application log data that is generated by the Salesforce product offerings, this data is predominately used by the data science teams. Team also leads all aspects of rolling out Salesforce's analytics tool Wave to internal business partners. The Salesforce IT EDW team is looking for an experienced BI designer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com. This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.\nResponsibilities:\n? Experience in designing and development of analytical solutions including Salesforce analytics solution - Wave and Tableau ? Contribute towards designing data models, ETL mappings and associated objects of analytical solutions. ? Review solution design with Lead members and ensure that the defined EDW standards and framework are followed. ? Review and validate logical and physical design to ensure alignment with the defined solution architecture. ? Create/review technical documentation for all new and modified objects. ? Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly. ? Follow standard practises for QA in the organization ? Support QA, UAT and performance testing phases of the development cycle. ? Understand and incorporate required security framework in the developed data model and ETL objects. ? Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes. ? Evaluate, determine root cause and resolve production issues. ? Work closely with other IT development groups to deliver coordinated software solutions Required Skills:\n? 1+year of experience working as a BI technical resource in a customer-focused IT EDW team ? 2+years of experience in the data warehousing domain as a technical resource ? Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools. ?Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture. ? Good knowledge of SQL and relational database models. ? Hands-on experience creating Unix shell scripts ? Data visualization tool experience like Tableau ? Good understanding of data modeling ? Basic working experience in an agile environment ? Excellent team player able to work with virtual and global across functional teams at all levels. ? Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines. ? Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences. ? Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role. Desired Skills:\n? Salesforce, Data Warehousing, customer success and Data Analysis ? Working experience on Data science or Python scripting is a plus ? Working experience of snowflake ? Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690502400,
            "job_posted_at_datetime_utc": "2023-07-28T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=hmPiuV0MhssAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-04T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1691107200,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 24,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 20,
            "job_max_salary": 28,
            "job_salary_currency": "USD",
            "job_salary_period": "HOUR",
            "job_highlights": {
                "Qualifications": [
                    "This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel",
                    "1+year of experience working as a BI technical resource in a customer-focused IT EDW team ?",
                    "2+years of experience in the data warehousing domain as a technical resource ?",
                    "Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools",
                    "Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture",
                    "Good knowledge of SQL and relational database models",
                    "Hands-on experience creating Unix shell scripts ?",
                    "Data visualization tool experience like Tableau ?",
                    "Good understanding of data modeling ?",
                    "Basic working experience in an agile environment ?",
                    "Excellent team player able to work with virtual and global across functional teams at all levels",
                    "Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines",
                    "Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences",
                    "Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role",
                    "Salesforce, Data Warehousing, customer success and Data Analysis ?",
                    "Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements",
                    "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"
                ],
                "Responsibilities": [
                    "Contribute towards designing data models, ETL mappings and associated objects of analytical solutions",
                    "Review solution design with Lead members and ensure that the defined EDW standards and framework are followed",
                    "Review and validate logical and physical design to ensure alignment with the defined solution architecture",
                    "Create/review technical documentation for all new and modified objects",
                    "Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly",
                    "Follow standard practises for QA in the organization ?",
                    "Support QA, UAT and performance testing phases of the development cycle",
                    "Understand and incorporate required security framework in the developed data model and ETL objects",
                    "Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes",
                    "Evaluate, determine root cause and resolve production issues",
                    "Work closely with other IT development groups to deliver coordinated software solutions Required Skills:"
                ],
                "Benefits": [
                    "$80K -- $100K"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "511210",
            "job_naics_name": "Software Publishers"
        },
        {
            "employer_name": "H-E-B",
            "employer_logo": "https://www.heb.com/img/header/logo.png",
            "employer_website": "http://www.heb.com",
            "employer_company_type": "Retail",
            "job_publisher": "ZipRecruiter",
            "job_id": "F0BzDvq8V2EAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer, Dallas, Austin, or San Antonio, TX",
            "job_apply_link": "https://www.ziprecruiter.com/c/H-E-B/Job/Senior-Data-Engineer,-Dallas,-Austin,-or-San-Antonio,-TX/-in-Dallas,TX?jid=1e6c47e6fd30e3b3",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6602,
            "job_description": "Overview\n\nH-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion. Described by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace. H-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.\nResponsibilities\n\nSince H-E-B Digital Technology's inception, we've been investing heavily in our customers' digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital--we're using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. If you enjoy taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.Our team is made up of data engineers with various levels of experience. The primary focus is current customer data which our team consolidates, cleanses, formats and validates for various stakeholders within our Digital team. As a Data Engineer you'll:Design and build a modern data warehouse in the cloud\n\nEnhance data collection procedures to build analytic systems.\n\nWork as an Agile partner and participate in back log refinement, sprint planning, review, and retrospective.\n\nCoach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed.\n\nContribute to overall system design, architecture, security, scalability, reliability, and performance of applications.Support the build and deployment pipeline and when necessary, both diagnose and solve production support issues.\n\nWork with Product, Design, and QA to deliver world-class digital experiences.\n\nGet the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team's architecture.\n\nApply understanding to help improve the cloud infrastructure that powers our high-performance, consumer-scale site and mobile apps.Who You Are:3-5+ years of experience with ETL, Data Modeling, Data Warehousing, and working with large-scale datasets (enterprise experience is a plus!)\n\n3-5+ years of hands-on experience in multiple modern programming languages such as Python or Java\n\n2+ years of experience leveraging DevOps principals such as CI/CD and using tools like Git, Jenkins, etc.\n\nStrong working knowledge of Data Engineering and associated tools and technologies like Apache, Spark, Databricks, Python, SQL, and data lake concepts.\n\nExperience with Argo Informatica\n\nWorking experience with AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3, etc.\n\nBachelor's degree in Computer Engineering, Computer Science or related discipline, Master's Degree preferred\n\nAn equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above.\n\nIt's a Plus if you have:\n\nExperience working with other public cloud technologies AWS, GCP\n\nHands on experience with data virtualization technologies: CIS (Tibco), Denodo, or SQL 2019 Virtualization\n\nExperience building test automation to ensure data quality and accuracy\n\nProficient in data modeling (specifically RDS, PostgreSQL, Oracle)\n\nDATA3232\nEmployment Type: FULL_TIME",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1627369200,
            "job_posted_at_datetime_utc": "2021-07-27T07:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=F0BzDvq8V2EAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693008000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "3-5+ years of hands-on experience in multiple modern programming languages such as Python or Java",
                    "2+ years of experience leveraging DevOps principals such as CI/CD and using tools like Git, Jenkins, etc",
                    "Strong working knowledge of Data Engineering and associated tools and technologies like Apache, Spark, Databricks, Python, SQL, and data lake concepts",
                    "Experience with Argo Informatica",
                    "Working experience with AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3, etc",
                    "An equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above",
                    "Experience working with other public cloud technologies AWS, GCP",
                    "Hands on experience with data virtualization technologies: CIS (Tibco), Denodo, or SQL 2019 Virtualization",
                    "Experience building test automation to ensure data quality and accuracy",
                    "Proficient in data modeling (specifically RDS, PostgreSQL, Oracle)"
                ],
                "Responsibilities": [
                    "As a Data Engineer you'll:Design and build a modern data warehouse in the cloud",
                    "Enhance data collection procedures to build analytic systems",
                    "Work as an Agile partner and participate in back log refinement, sprint planning, review, and retrospective",
                    "Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed",
                    "Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications",
                    "Support the build and deployment pipeline and when necessary, both diagnose and solve production support issues",
                    "Work with Product, Design, and QA to deliver world-class digital experiences",
                    "Get the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team's architecture",
                    "Apply understanding to help improve the cloud infrastructure that powers our high-performance, consumer-scale site and mobile apps"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06: Database Architects"
            ],
            "job_naics_code": "445110",
            "job_naics_name": "Supermarkets and Other Grocery (except Convenience) Stores"
        },
        {
            "employer_name": "Dice",
            "employer_logo": "https://s24.q4cdn.com/133441296/files/images/brands/dice.png",
            "employer_website": null,
            "employer_company_type": "Information",
            "job_publisher": "LinkedIn",
            "job_id": "GZboUoqtmI4AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Big Data Engineer/Data Engineer(Only W2 or Self Corp.)",
            "job_apply_link": "https://www.linkedin.com/jobs/view/big-data-engineer-data-engineer-only-w2-or-self-corp-at-dice-3674337930",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5631,
            "job_description": "Dice is the leading career destination for tech experts at every stage of their careers. Our client, Newt Global, is seeking the following. Apply via Dice today!\n\nBig Data Engineer /Data Engineer (Only W2 or Self Corp.)\n\nLocation: Dallas, TX(Hybrid)\n\n12 Months+\n\nRequired:\n\n8 plus year's experience\n\nExpert on PySpark\n\nExpert in Python and Spark\n\nNice to have Banking experience.\n\nBig Data Engineer/Data Engineer(Only W2 or Self Corp.)",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690213856,
            "job_posted_at_datetime_utc": "2023-07-24T15:50:56.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=GZboUoqtmI4AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-23T15:50:56.000Z",
            "job_offer_expiration_timestamp": 1692805856,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "8 plus year's experience",
                    "Nice to have Banking experience"
                ]
            },
            "job_job_title": "Engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "519130",
            "job_naics_name": "Internet Publishing and Broadcasting and Web Search Portals"
        },
        {
            "employer_name": "CoreLogic",
            "employer_logo": "https://www.corelogic.com/wp-content/uploads/sites/4/2021/06/Featured_FB_OG-corelogic-logo_1200x1200.png",
            "employer_website": "http://www.corelogic.com",
            "employer_company_type": "Information",
            "job_publisher": "Talent.com",
            "job_id": "ziqGUCDHBPwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior data engineer",
            "job_apply_link": "https://www.talent.com/view?id=8e0ac49a262d",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4889,
            "job_description": "Description\n\nCoreLogic is seeking a Senior Data Engineer who is passionate about all things data and has the desire to turn data processing into a competitive advantage.\n\nEmployees in this role need to be well versed with innovation and trends in the world of data processing which can include data management, data quality and data curation.\n\nIf you like being at the forefront and for your work to be impactful, this is the position you want. This is a highly visible role with direct revenue impact as data is our product.\n\nThis role also provides a tremendous opportunity to learn while making an impactful contribution to the success of the company.\n\nJob Responsibilities :\n\nProcess complex data sets, leverage technologies used to process these disparate data sets and understand the correlation as well as patterns that exist between the data sets.\n\nWork closely with other teams to develop solutions to meet business requirements, perform assessments, POC\u2019s and establish an execution or development plan.\n\nForecast technical challenges and develop implementation strategies.\n\nIdentify enabling technologies and develop solutions based on identified technologies as required.\n\nPrepare and present white papers and proposals.\n\nEmploy lateral thinking to develop innovative solutions to meet business needs.\n\nCommunicate complex technical ideas in a straightforward and compelling way.\n\nJob Qualifications :\n\n8 years of relevant IT experience and 3 years of experience with Java coding\n\nBachelor's degree in Engineering, Computer Science, Information Technology, related discipline or significant industry experience\n\nExcellent coding skills with expertise in Java, REST APIs, SQL, and micro-services\n\nExperience with Apache Spark / Beam, Google Cloud Dataflow, MySQL, Big Query, Airflow, Kafka, Kubernetes, and Anthos\n\nExperience with development & operations in cloud native technologies (GCP preferred)\n\nBroad knowledge of open-source libraries & packages\n\nWell versed with big data concepts and data engineering principles or techniques\n\nExcellent communication and organization skills\n\nMust have a go-getter attitude and a willingness to pick up new technologies and programming languages\n\nMust be a team player with a creative mindset who's always looking for novel solutions to challenging problems and open to learning as well as sharing knowledge with others\n\nPreferred Qualifications :\n\nWorking knowledge of Python, DBT, React.JS & Angular.JS\n\nProfessional certifications\n\nKnowledge of Cloud architecture, networking and protocols\n\nAnalytical mindset and good problem-solving skills\n\nGood organizational skills\n\nSomeone who enjoys having fun and maintaining work life balance while working on exciting projects\n\nSomeone who's interested in working at a company that believes in mandatory development of its staff and has a culture of promoting from within\n\nAnnual Pay Range : 97,200 - 130,000 USD\n\n97,200 - 130,000 USD\n\nCoreLogic benefits information can be found here : . Qualifications, locations and experience of the individual ultimately selected for the position may impact the final actual offered compensation, which may vary from any posted range.\n\nCoreLogic's Diversity Commitment :\n\nCoreLogic is fully committed to employing a diverse workforce and creating an inclusive work environment that embraces everyone\u2019s unique contributions, experiences and values.\n\nWe offer an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package.\n\nWe are better together when we support and recognize our differences.\n\nEOE AA M / F / Veteran / Disability :\n\nCoreLogic is an Equal Opportunity / Affirmative Action employer committed to attracting and retaining the best-qualified people available, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, disability or status as a veteran of the Armed Forces, or any other basis protected by federal, state or local law.\n\nCoreLogic maintains a Drug-Free Workplace.\n\nPlease apply on our website for consideration.\n\nPrivacy Policy -\n\nBy providing your telephone number, you agree to receive automated (SMS) text messages at that number from CoreLogic regarding all matters related to your application and, if you are hired, your employment and company business.\n\nMessage & data rates may apply. You can opt out at any time by responding STOP or UNSUBSCRIBING and will automatically be opted out company-wide.\n\nConnect with us on social media! Click on the quicklinks below to find out more about our company and associates.\n\nLast updated : 2023-07-27",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416000,
            "job_posted_at_datetime_utc": "2023-07-27T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ziqGUCDHBPwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-31T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693440000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 96,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "8 years of relevant IT experience and 3 years of experience with Java coding",
                    "Bachelor's degree in Engineering, Computer Science, Information Technology, related discipline or significant industry experience",
                    "Excellent coding skills with expertise in Java, REST APIs, SQL, and micro-services",
                    "Experience with Apache Spark / Beam, Google Cloud Dataflow, MySQL, Big Query, Airflow, Kafka, Kubernetes, and Anthos",
                    "Broad knowledge of open-source libraries & packages",
                    "Well versed with big data concepts and data engineering principles or techniques",
                    "Excellent communication and organization skills",
                    "Must have a go-getter attitude and a willingness to pick up new technologies and programming languages",
                    "Must be a team player with a creative mindset who's always looking for novel solutions to challenging problems and open to learning as well as sharing knowledge with others",
                    "Working knowledge of Python, DBT, React.JS & Angular.JS",
                    "Professional certifications",
                    "Knowledge of Cloud architecture, networking and protocols",
                    "Analytical mindset and good problem-solving skills",
                    "Good organizational skills",
                    "Someone who enjoys having fun and maintaining work life balance while working on exciting projects",
                    "Someone who's interested in working at a company that believes in mandatory development of its staff and has a culture of promoting from within"
                ],
                "Responsibilities": [
                    "Process complex data sets, leverage technologies used to process these disparate data sets and understand the correlation as well as patterns that exist between the data sets",
                    "Work closely with other teams to develop solutions to meet business requirements, perform assessments, POC\u2019s and establish an execution or development plan",
                    "Forecast technical challenges and develop implementation strategies",
                    "Identify enabling technologies and develop solutions based on identified technologies as required",
                    "Prepare and present white papers and proposals",
                    "Employ lateral thinking to develop innovative solutions to meet business needs",
                    "Communicate complex technical ideas in a straightforward and compelling way"
                ],
                "Benefits": [
                    "Annual Pay Range : 97,200 - 130,000 USD",
                    "CoreLogic benefits information can be found here : . Qualifications, locations and experience of the individual ultimately selected for the position may impact the final actual offered compensation, which may vary from any posted range",
                    "We offer an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1243.00"
            ],
            "job_naics_code": "518210",
            "job_naics_name": "Data Processing, Hosting, and Related Services"
        },
        {
            "employer_name": "IT Engagements,Inc.",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Glassdoor",
            "job_id": "pB9FEDB1j4sAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Sr Cloud Data Engineer",
            "job_apply_link": "https://www.glassdoor.com/job-listing/sr-cloud-data-engineer-it-engagements-inc-JV_IC1139977_KO0,22_KE23,41.htm?jl=1008579705781",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5566,
            "job_description": "Greeting from IT Engagements\u2026!\n\nIT Engagements is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. We have an immediate opening for the below position with our premium clients.\n\nJob:- Sr Cloud Data Engineer\n\nLocation:- McLean VA or Dallas, TX (Onsite)\nDuration:- 6+ Months Contract to hire\n\nExp: 8 years+\n\nJob Description\n\nMust Haves:\n\u2022 Building data lake platform in AWS (Glue, DynamoDB S3, EMR)\n\u2022 Python\n\u2022 Scala\n\u2022 Understand data engineering, data governance, data lineage\n\u2022 Linux Shell Scripting\n\nReally Nice to Have:\n\u2022 Snowflake\n\u2022 Data warehousing\n\u2022 Kubernetes\n\u2022 Hadoop infrastructure\n\nRequirements:\n\u2022 Experience with Big Data technologies such as Hadoop/Hive/Spark specific to AWS related services is a plus.\n\u2022 Expertise in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.\n\u2022 Sound knowledge of distributed systems and data architecture - design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.\n\u2022 5+ years of work experience with building Data Pipelines, Data Processing, Data Modeling, and Data Architecture.\n\u2022 Experience operating very large data warehouses or data lakes. (Snowflake, etc)\n\u2022 Excellent skills in writing and optimizing SQL.\n\u2022 Knowledge of Engineering and Operational Excellence using standard methodologies.\n\u2022 Knowledge of IT, service-oriented architectures, software development life cycles, or information security platforms and applications.\n\u2022 Minimum 5+ years of experience in software development.\n\u2022 3+ years of related industry experience in an enterprise environment.\n\u2022 5+ years of data engineering experience.\n\u2022 Scala / Python, pySpark(Boto, Boto3, etc.)/ Spark experience.\n\u2022 Delta lake, delta table and lakehouse architecture\n\u2022 Datewarehouse Experience (Snowflake)\n\u2022 Experience with lambda, EMR, SQS, DynamoDB, Glue, Stepfunctions, etc.\n\u2022 Linux and shell scripting.\n\nKnowledge of:\n\u2022 Kubernetes.\n\u2022 Formal design patterns and industry best-practices.\n\u2022 2+ years of experience with requirements, design, implementation, integration, and testing for data and analytics integration.\n\u2022 2+ years of experience across a variety of technologies such databases, directory services, application servers, network infrastructures, Linux operating systems, and an understanding of fundamental security and data flows within these components.\n\u2022 Excellent verbal and written communication skills.\n\u2022 Self-motivated, driven, and creative individual.\n\u2022 Scaling systems and microservices.\n\u2022 Familiarity with CI/CD processes\n\u2022 Code coverage analysis / static analysis tools.\n\u2022 Agile programming processes and methodologies such as Scrum.\n\u2022 Scheduling tools like Autosys , ControlM.\n\u2022 Informatica IICS , Talend\n\nThankyou\n\nDivya Kumari\n\nTechnical Recruiter\n\ndivya(at)itengagements(dot)com\n\nJob Types: Full-time, Contract\n\nPay: From $65.00 per hour\n\nExperience level:\n\u2022 8 years\n\nSchedule:\n\u2022 8 hour shift\n\nExperience:\n\u2022 Informatica: 1 year (Preferred)\n\u2022 SQL: 1 year (Preferred)\n\u2022 Data warehouse: 1 year (Preferred)\n\nWork Location: On the road\n\nSpeak with the employer\n+91 7864080647",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688947200,
            "job_posted_at_datetime_utc": "2023-07-10T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=pB9FEDB1j4sAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Building data lake platform in AWS (Glue, DynamoDB S3, EMR)",
                    "Understand data engineering, data governance, data lineage",
                    "Linux Shell Scripting",
                    "Data warehousing",
                    "Kubernetes",
                    "Expertise in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies",
                    "Sound knowledge of distributed systems and data architecture - design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures",
                    "5+ years of work experience with building Data Pipelines, Data Processing, Data Modeling, and Data Architecture",
                    "Experience operating very large data warehouses or data lakes",
                    "Knowledge of Engineering and Operational Excellence using standard methodologies",
                    "Knowledge of IT, service-oriented architectures, software development life cycles, or information security platforms and applications",
                    "Minimum 5+ years of experience in software development",
                    "3+ years of related industry experience in an enterprise environment",
                    "Scala / Python, pySpark(Boto, Boto3, etc.)/ Spark experience",
                    "Experience with lambda, EMR, SQS, DynamoDB, Glue, Stepfunctions, etc",
                    "Formal design patterns and industry best-practices",
                    "2+ years of experience with requirements, design, implementation, integration, and testing for data and analytics integration",
                    "2+ years of experience across a variety of technologies such databases, directory services, application servers, network infrastructures, Linux operating systems, and an understanding of fundamental security and data flows within these components",
                    "Excellent verbal and written communication skills",
                    "Self-motivated, driven, and creative individual",
                    "Scaling systems and microservices",
                    "Familiarity with CI/CD processes",
                    "Code coverage analysis / static analysis tools",
                    "Agile programming processes and methodologies such as Scrum",
                    "Scheduling tools like Autosys , ControlM",
                    "Informatica IICS , Talend",
                    "8 years"
                ],
                "Responsibilities": [
                    "Delta lake, delta table and lakehouse architecture"
                ],
                "Benefits": [
                    "Pay: From $65.00 per hour"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Costco Wholesale",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQMEXpjzxSXP_hEYETqQMZmfop0Uwnb1a8puH1-&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "JobzMall",
            "job_id": "7-6zJm6_MhYAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.jobzmall.com/costco-wholesale/job/data-engineer-251",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5613,
            "job_description": "At Costco Wholesale, we offer an exciting opportunity for a Data Engineer to join our team and make a real difference. We are looking for a highly organized and creative individual who can develop and maintain our data-driven systems.The successful candidate will have a strong technical background and the ability to think strategically about our data architecture. You must have a Bachelor's degree in Computer Science, Software Engineering, Information Systems, Data Science or a related field and at least two years of experience in working with data architectures, ETL pipelines, and data warehousing.We are looking for an individual who is passionate about data and has the ability to analyze and interpret complex datasets. You should have excellent problem-solving skills, be comfortable working with multiple stakeholders, and have a keen eye for detail. If you are motivated, organized, and an excellent communicator, we would love to hear from you!\n\nCostco Wholesale is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1676932949,
            "job_posted_at_datetime_utc": "2023-02-20T22:42:29.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=7-6zJm6_MhYAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T23:59:59.000Z",
            "job_offer_expiration_timestamp": 1693094399,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Sql <br>Python <br>Business Intelligence <br>Data Analysis <br>Data Quality <br>Communication <br>Conflict Resolution <br>Leadership <br>Negotiation <br>Time management <br>Data Security <br>Big Data <br>Data Modeling <br>ETL <br>Hadoop <br>Data warehousing <br>Data Visualization <br>Cloud Computing <br>Interpersonal Skills <br>creativity <br>Teamwork <br>Data migration <br>Data governance <br>Adaptability <br>Problem-SolvingCommunication <br>Conflict Resolution <br>Leadership <br>Negotiation <br>Time management <br>Interpersonal Skills <br>creativity <br>Teamwork <br>Adaptability <br>Problem-Solving"
            ],
            "job_required_education": {
                "postgraduate_degree": true,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "The successful candidate will have a strong technical background and the ability to think strategically about our data architecture",
                    "You must have a Bachelor's degree in Computer Science, Software Engineering, Information Systems, Data Science or a related field and at least two years of experience in working with data architectures, ETL pipelines, and data warehousing.We are looking for an individual who is passionate about data and has the ability to analyze and interpret complex datasets",
                    "You should have excellent problem-solving skills, be comfortable working with multiple stakeholders, and have a keen eye for detail",
                    "If you are motivated, organized, and an excellent communicator, we would love to hear from you!"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Walmart",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Walmart_logo.svg/2560px-Walmart_logo.svg.png",
            "employer_website": "https://www.walmart.com",
            "employer_company_type": "Retail",
            "job_publisher": "Dallas, TX - Geebo",
            "job_id": "fciHxK29OeAAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer III",
            "job_apply_link": "https://dallas-tx.geebo.com/jobs-online/view/id/1139475167-data-engineer-iii-/",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.404,
            "job_description": "What you'll do at Position Summary What you'll do At Walmart, we help people save money, so they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We can't do that without the best talent - talent that is innovative, curious, and driven to create exceptional experiences for our customers. Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail ? With the sheer scale of Walmart's environment comes the biggest of big data sets. As a Walmart Data Engineer, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on . You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers' lives. You'll make an impact by:\nDesign, develop and build database to power Big Data analytical systems. Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies. Build robust and scalable applications using SQL, Scala/Python and Spark. Create real time data streaming and processing using Kafka and/or Spark streaming. Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure Engage with architects and senior technical leads to create and enhance complex software components. Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud. Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion. Develop proof-of-concept prototype with fast iteration and experimentation. Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager Perform continuous integration and deployment using Jenkins and Git You'll sweep us off our feet if 3\nyears of experience with 1\nyears of Big data development experience Experience in HDFS, Hive, Hive UDF's, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix. Demonstrates expertise in writing complex, highly optimized queries across large data sets Retail experience and knowledge of commercial data is a huge plus Experience with BI Tool Tableau or Looker is a plu The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process. About Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That's what we do at Walmart Global Tech. We're a team of 15,000\nsoftware engineers, data scientists and service professionals within Walmart, the world's largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. We're virtual Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting. Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We'll collaborate in person at a regular cadence and with purpose. Benefits & Perks:\nBeyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions - while being inclusive of all people. Who We Are Join Walmart and your work could help over 275 million global customers live better every week. Yes, we are the Fortune #1 company. But you ' ll quickly find we ' re a company who wants you to feel comfortable bringing your whole self to work. A career at Walmart is where the world's most complex challenges meet a kinder way of life. Our mission spreads far beyond the walls of our stores. Join us and you'll discover why we are a world leader in diversity and inclusion, sustainability, and community involvement. From day one, you ' ll be empowered and equipped to do the best work of your life. careers.walmart.com Minimum Qualifications Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1:\nBachelor's degree in Computer Science and 2 years' experience in software engineering or related field. Option 2:\n4 years' experience in software engineering or related field. Option 3:\nMaster's degree in Computer Science. Preferred Qualifications Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Master's degree in Computer Science or related field and 2 years' experience in software engineering or related field Primary Location 603 MUNGER AVE STE 400, DALLAS, TX 75202, United States of America About Walmart At Walmart, we help people save money so they can live better. This mission serves as the foundation for every decision we make, from responsible sourcing to sustainability-and everything in between. As a Walmart associate, you will play an integral role in shaping the future of retail, tech, merchandising, finance and hundreds of other industries-all while affecting the lives of millions of customers all over the world. Here, your work makes an impact every day. What are you waiting for? Walmart, Inc. is an Equal Opportunity Employer- By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, abilities, ideas and opinions- while being inclusive of all people. All the benefits you need for you and your family Multiple health plan options, including vision & dental plans for you & dependents Financial benefits including 401(k), stock purchase plans, life insurance and more Associate discounts in-store and online Education assistance for Associate and dependents Parental Leave Pay during military service Paid Time off - to include vacation, sick, parental Short-term and long-term disability for when you can't work because of injury, illness, or childbirth Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to specific plan or program terms. For information about benefits and eligibility, see One.Walmart.com/Benefits . Frequently asked questions On average, how long does it take to fill out an application? On average, it takes 45-60 minutes to complete your application for the first time. Subsequent applications will take less time to apply as our system saves some of your application information. Please note that some positions require the completion of assessments in order to receive consideration for that role. Those would take additional time. Can I change my application after submitting? No, you cannot change your application after submitting, so please make sure that everything is finalized before you hit the submit button. How do you protect my personal information? Processing of information on paper is minimal, and Walmart processes application information using an applicant tracking system (ATS). Access to the data within the ATS is restricted to authorized personnel, and the system itself is held to high security standards by Walmart. What are the recommended Internet Browsers for applying for open roles? Internet Explorer 8.0\nFirefox 4.0\nSafari 4.0\nChrome 12+\nSalary Range:\n$80K -- $100K\nMinimum Qualification\nData Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416000,
            "job_posted_at_datetime_utc": "2023-07-27T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "paid_time_off",
                "retirement_savings",
                "health_insurance",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=fciHxK29OeAAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-03T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1691020800,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 48,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 20,
            "job_max_salary": 28,
            "job_salary_currency": "USD",
            "job_salary_period": "HOUR",
            "job_highlights": {
                "Qualifications": [
                    "years of experience with 1",
                    "years of Big data development experience Experience in HDFS, Hive, Hive UDF's, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix",
                    "Demonstrates expertise in writing complex, highly optimized queries across large data sets Retail experience and knowledge of commercial data is a huge plus Experience with BI Tool Tableau or Looker is a plu The above information has been designed to indicate the general nature and level of work performed in the role",
                    "Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications"
                ],
                "Responsibilities": [
                    "You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way",
                    "You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers' lives",
                    "Design, develop and build database to power Big Data analytical systems",
                    "Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies",
                    "Build robust and scalable applications using SQL, Scala/Python and Spark",
                    "Create real time data streaming and processing using Kafka and/or Spark streaming",
                    "Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure Engage with architects and senior technical leads to create and enhance complex software components",
                    "Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud",
                    "Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion",
                    "Develop proof-of-concept prototype with fast iteration and experimentation",
                    "Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager Perform continuous integration and deployment using Jenkins and Git You'll sweep us off our feet if 3"
                ],
                "Benefits": [
                    "Beyond competitive pay, you can receive incentive awards for your performance",
                    "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more",
                    "All the benefits you need for you and your family Multiple health plan options, including vision & dental plans for you & dependents Financial benefits including 401(k), stock purchase plans, life insurance and more Associate discounts in-store and online Education assistance for Associate and dependents Parental Leave Pay during military service Paid Time off - to include vacation, sick, parental Short-term and long-term disability for when you can't work because of injury, illness, or childbirth Eligibility requirements apply to some benefits and may depend on your job classification and length of employment",
                    "Benefits are subject to change and may be subject to specific plan or program terms",
                    "$80K -- $100K"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "452990",
            "job_naics_name": "All Other General Merchandise Stores"
        },
        {
            "employer_name": "Sphinix Solutions",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQd1Iu8RoMwh5xJq8yGk6lyzpIamO--IiysvtJC&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Built In",
            "job_id": "gbgzIjy0vDoAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Sr Data Engineer (Dallas, TX)",
            "job_apply_link": "https://builtin.com/job/data/sr-data-engineer/361150",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5966,
            "job_description": "Sr Data Engineer - 5 Positions\n\nCall Notes:\n\nTop Skills:\n\nData Engineers with -\n\n3-5 years of genuine experience ( will not count the academic experience in this)\n\nPython\n\nSpark/Pyspark/ Scala\n\nSpring Boot\n\nJava\n\nAWS or any other Cloud\n\nLooking for hands on resources.\n\nData Pipeline, data analysis\n\nThis is a hands on up and running Senior Level role.\n\nTeam Info:\n\nData space\n\nRisk management called - TDGRM\n\nRole Info:\n\nBuilding controls for tech and data environment\n\nThe team/ hiring will be distributed based on these skills -\n\u2022 Python with Pyspark and AWS - 1st track\n\u2022 Java with Spark or Scala and AWS- 2nd track\n\nMinimum Requirements:\n\u2022 At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python\n\u2022 At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc)\n\u2022 At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps\n\u2022 At least 3 years of experience with SQL and Shell Scripting experience\n\u2022 At least 2 years of experience with software design and must have an understanding of cross systems usage and impact\n\nNice-to-Have qualifications:\n\u2022 2+ years of experience working with Dimensional Data Model and pipelines in relation with the same\n\u2022 2+ years' experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service\n\u2022 2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL\n\u2022 Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)\n\u2022 Hands on design experience with data pipelines, joining data between structured and unstructured data",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1635462453,
            "job_posted_at_datetime_utc": "2021-10-28T23:07:33.000Z",
            "job_city": null,
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.707874,
            "job_longitude": -96.92091,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=gbgzIjy0vDoAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "3-5 years of genuine experience ( will not count the academic experience in this)",
                    "Python with Pyspark and AWS - 1st track",
                    "Java with Spark or Scala and AWS- 2nd track",
                    "At least 3 years of experience developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python",
                    "At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc)",
                    "At least 3 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps",
                    "At least 3 years of experience with SQL and Shell Scripting experience",
                    "At least 2 years of experience with software design and must have an understanding of cross systems usage and impact",
                    "2+ years of experience working with Dimensional Data Model and pipelines in relation with the same",
                    "2+ years' experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service",
                    "2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL",
                    "Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)",
                    "Hands on design experience with data pipelines, joining data between structured and unstructured data"
                ],
                "Responsibilities": [
                    "This is a hands on up and running Senior Level role",
                    "Building controls for tech and data environment",
                    "The team/ hiring will be distributed based on these skills -"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "BuzzClan Private Limited",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Indeed",
            "job_id": "Zl5bZRbb03QAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.indeed.com/viewjob?jk=e185e49f149ae1b0",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.6156,
            "job_description": "-Job Title: Data Engineer /Data Analyst\nLocation: Dallas, TX\n\nJob Overview\nShall apply specialized knowledge in writing high-performance SQL\nqueries and data engineering solutions that moves, cleans, and loads data to differing systems. The\nperson will interact with our clients and differing roles within IT to gain understanding of the business\nenvironment, technical context, and strategic direction. This person will also be responsible for\ngenerating documentation as needed; conform to security and quality standards; and stay current on\nemerging trends. Teams supported include: Application Engineering, Data Platform Engineering, Data\nEngineering, Decision Science, and Business Analytics\nA successful candidate for this position, you must:\n? Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions\n? Proactively identify and assist in solving recurring data quality or data availability issues.\n? Have experience in monitoring, troubleshooting, and resolving ETL issues.\n? Be able to develop high performance data queries, stored procedures and/or functional code for data related ad-hoc reporting and/or ETL batch triage reasons.\n? Understand how to profile code, queries, programming objects and optimize performance\n? Aspire to be efficient, thorough, and proactive\n\nResponsibilities and Duties\n? Monitors, supports, triages data pipelines and ETL tasks that ingest, move, transform, and integrate data in a secure and performant manner.\n? Prepares necessary SQL scripts to perform data manipulation in key systems to address data related application and reporting needs.\n? Explores new technologies and data processing methods to increase efficiency, performance, and flexibility to proactively address recurring data related issues.\n? Document requirements and translate into proper system requirements specifications using high-maturity methods, processes, and tools.\n? Designs, prepares, and executes unit tests.\n? Participates in cross-functional teams.\n? Represents team to clients.\n? Demonstrates technical leadership and exerts influence outside of immediate team.\n? Develops innovative team solutions to complex problems.\n? Contributes to strategic direction for teams.\n? Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. SSIS, Azure ADF).\n? Integrates technical expertise and business understanding to create superior solutions for clients.\n? Consults with team members and other organizations, clients and vendors on complex issues.\n? Work on Special projects as requested\n? Performs other duties as assigned\n\nQualifications\n? Bachelor\u2019s degree in IT preferred. Equivalent combination of education and experience may be substituted in lieu of degree.\n? 2 to 4 years of Microsoft SSIS package development experience including experience with Microsoft Visual Studio\n? 1+ years of experience working with enterprise data warehouses\n? 5+ years of experience in SQL and be able to write complex logic using SQL as part of ETL, and use SQL effectively to perform complex data analysis and discovery\n? 1+ years of experience building reports with SSRS.\n? Exposure to Azure and Azure Data Factory\n? Exposure to an Enterprise Data Lake\n? Demonstrate strong organization skills and detail-oriented\n? Experience with CMD shell and PowerShell\n? Experience with large-scale, complex data environments\n? Ability to self-motivate and meet deadlines\n? Intense desire to learn\n? Ability to express complex technical concepts effectively, both verbally and in writing\n? Ability to multi-task in a fast-paced, changing environment\n? Ability to maintain confidentiality\n\nJob Types: Full-time, Contract, Temporary\n\nSalary: Up to $80,000.00 per year\n\nBenefits:\n\u2022 401(k)\n\u2022 Dental insurance\n\u2022 Flexible schedule\n\u2022 Health insurance\n\u2022 Paid time off\n\u2022 Tuition reimbursement\n\u2022 Vision insurance\n\nExperience level:\n\u2022 4 years\n\nSchedule:\n\u2022 8 hour shift\n\nAbility to commute/relocate:\n\u2022 Dallas, TX 75240: Reliably commute or planning to relocate before starting work (Required)\n\nExperience:\n\u2022 Informatica: 1 year (Preferred)\n\u2022 SQL: 1 year (Preferred)\n\u2022 Data warehouse: 1 year (Preferred)\n\nWork Location: In person",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1680704545,
            "job_posted_at_datetime_utc": "2023-04-05T14:22:25.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "paid_time_off",
                "health_insurance",
                "retirement_savings",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Zl5bZRbb03QAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Equivalent combination of education and experience may be substituted in lieu of degree",
                    "2 to 4 years of Microsoft SSIS package development experience including experience with Microsoft Visual Studio",
                    "1+ years of experience working with enterprise data warehouses",
                    "5+ years of experience in SQL and be able to write complex logic using SQL as part of ETL, and use SQL effectively to perform complex data analysis and discovery",
                    "1+ years of experience building reports with SSRS",
                    "Exposure to Azure and Azure Data Factory",
                    "Exposure to an Enterprise Data Lake",
                    "Demonstrate strong organization skills and detail-oriented",
                    "Experience with CMD shell and PowerShell",
                    "Experience with large-scale, complex data environments",
                    "Ability to self-motivate and meet deadlines",
                    "Intense desire to learn",
                    "Ability to express complex technical concepts effectively, both verbally and in writing",
                    "Ability to multi-task in a fast-paced, changing environment",
                    "Ability to maintain confidentiality",
                    "Dallas, TX 75240: Reliably commute or planning to relocate before starting work (Required)"
                ],
                "Responsibilities": [
                    "queries and data engineering solutions that moves, cleans, and loads data to differing systems",
                    "person will interact with our clients and differing roles within IT to gain understanding of the business",
                    "This person will also be responsible for",
                    "generating documentation as needed; conform to security and quality standards; and stay current on",
                    "Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions",
                    "Proactively identify and assist in solving recurring data quality or data availability issues",
                    "Have experience in monitoring, troubleshooting, and resolving ETL issues",
                    "Be able to develop high performance data queries, stored procedures and/or functional code for data related ad-hoc reporting and/or ETL batch triage reasons",
                    "Understand how to profile code, queries, programming objects and optimize performance",
                    "Monitors, supports, triages data pipelines and ETL tasks that ingest, move, transform, and integrate data in a secure and performant manner",
                    "Prepares necessary SQL scripts to perform data manipulation in key systems to address data related application and reporting needs",
                    "Explores new technologies and data processing methods to increase efficiency, performance, and flexibility to proactively address recurring data related issues",
                    "Document requirements and translate into proper system requirements specifications using high-maturity methods, processes, and tools",
                    "Designs, prepares, and executes unit tests",
                    "Participates in cross-functional teams",
                    "Represents team to clients",
                    "Demonstrates technical leadership and exerts influence outside of immediate team",
                    "Develops innovative team solutions to complex problems",
                    "Contributes to strategic direction for teams",
                    "Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g",
                    "Integrates technical expertise and business understanding to create superior solutions for clients",
                    "Consults with team members and other organizations, clients and vendors on complex issues",
                    "Work on Special projects as requested",
                    "Performs other duties as assigned"
                ],
                "Benefits": [
                    "Salary: Up to $80,000.00 per year",
                    "401(k)",
                    "Dental insurance",
                    "Flexible schedule",
                    "Health insurance",
                    "Paid time off",
                    "Tuition reimbursement",
                    "Vision insurance"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Employer Direct Healthcare, LLC",
            "employer_logo": null,
            "employer_website": "http://www.edhc.com",
            "employer_company_type": null,
            "job_publisher": "Monster",
            "job_id": "9Z1mzqAMpa8AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.monster.com/job-openings/data-engineer-dallas-tx--85381867-01d8-4e40-85d5-6de644c3e7c7",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.6695,
            "job_description": "Responsible for developing complex logic to analyze, prepare, and discover insights within relational databases for innovative healthcare solutions provider. Specific duties include: (1) architecting database schema and other programmatic objects within database to store and secure data; (2) developing routines using SQL Server Integration Services, Azure Data Factory, and other ETL toolsets to ingest, transform, move, prepare, and extract data from different data stores; (3) utilizing SQL, Databricks, and other analytics tools to derive insights from data sets; (4) utilizing data visualization toolsets such as SQL Server Reporting Services and Microsoft Power BI to produce reports and data visualization for business partners; and (5) working data engineering team to conduct peer review and code analysis for data SDLC process.\n\nMinimum Requirements: B.S. in Computer Science, Electrical Engineering, Electronics Engineering, or closely related field; five years of IT development or related experience, including specific experience using Microsoft SQL server database to write complex logic and data analysis; knowledge of data warehouses, Microsoft SSIS package development, and building reports with SSR; and proficiency with large-scale, complex data environments, Microsoft Power BI, Azure and Azure Data Factory, Enterprise Data Lake, CMD shell and PowerShell.\n\nInterested candidates should submit a resume & cover letter to HR, Employer Direct Healthcare, LLC, 2100 Ross Avenue, Suite 1900, Dallas, TX 75201.\n\nAbout the Company:\nEmployer Direct Healthcare, LLC",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688688000,
            "job_posted_at_datetime_utc": "2023-07-07T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=9Z1mzqAMpa8AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-06T20:38:56.000Z",
            "job_offer_expiration_timestamp": 1691354336,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Minimum Requirements: B.S. in Computer Science, Electrical Engineering, Electronics Engineering, or closely related field; five years of IT development or related experience, including specific experience using Microsoft SQL server database to write complex logic and data analysis; knowledge of data warehouses, Microsoft SSIS package development, and building reports with SSR; and proficiency with large-scale, complex data environments, Microsoft Power BI, Azure and Azure Data Factory, Enterprise Data Lake, CMD shell and PowerShell"
                ],
                "Responsibilities": [
                    "Responsible for developing complex logic to analyze, prepare, and discover insights within relational databases for innovative healthcare solutions provider",
                    "Specific duties include: (1) architecting database schema and other programmatic objects within database to store and secure data; (2) developing routines using SQL Server Integration Services, Azure Data Factory, and other ETL toolsets to ingest, transform, move, prepare, and extract data from different data stores; (3) utilizing SQL, Databricks, and other analytics tools to derive insights from data sets; (4) utilizing data visualization toolsets such as SQL Server Reporting Services and Microsoft Power BI to produce reports and data visualization for business partners; and (5) working data engineering team to conduct peer review and code analysis for data SDLC process"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Mindlance",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTkhYZMlIJNU1lj96ZEzQYztcEocOhFFTGmJ_nF&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Lensa",
            "job_id": "JH3OAdcicPwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Lead Data Engineer",
            "job_apply_link": "https://lensa.com/lead-data-engineer-jobs/dallas/jd/e4b6a0390857a679def8b8fc111a2057",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.475,
            "job_description": "Job Description\n\nDuration:\nPossibility of extensions\n\nLocation:\nDallas, TX & Miami, FL (Hybrid)\n2 days/week - between Monday through Thursday\n\nPosition Responsibilities:\nPrincipal level engineering background technically but won't be spending most of their time hands on and need to be comfortable managing the process without getting hands-on\nMore delegating because they will manage 2 teams - architects are doing the modeling so these leads will be working with the business most\nNeed to be able to manage off-shore resources\nEach team is going to have 3 data engineers and 2 QA engineers and each lead will be responsible for two of these off-shore teams\nHighly regulatory industry experience would be a huge advantage\n\nPosition Qualifications:\nSnowflake and python - high level experience in the data engineering/science space\nFor ETL -- Into snowflake using Kafka - in snowflake its python and IDMC\nExperience doing large enterprise data migrations like they are doing\n\nEducation/Certifications Required/Preferred:\nAny kind of degree is a plus - builds perspective and shows ability to adapt and learn but feels like you can also get that perspective different ways so not a must have if they are otherwise great\n\n\"Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of - Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.\"",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689265152,
            "job_posted_at_datetime_utc": "2023-07-13T16:19:12.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=JH3OAdcicPwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-12T16:19:12.000Z",
            "job_offer_expiration_timestamp": 1691857152,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Reading Comprehension",
                "Active Listening",
                "Writing",
                "Speaking",
                "Critical Thinking",
                "Active Learning",
                "Monitoring",
                "Social Perceptiveness",
                "Coordination",
                "Complex Problem Solving",
                "Programming",
                "Judgment and Decision Making",
                "Systems Analysis",
                "Systems Evaluation"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Principal level engineering background technically but won't be spending most of their time hands on and need to be comfortable managing the process without getting hands-on",
                    "Need to be able to manage off-shore resources",
                    "Each team is going to have 3 data engineers and 2 QA engineers and each lead will be responsible for two of these off-shore teams",
                    "Highly regulatory industry experience would be a huge advantage",
                    "Snowflake and python - high level experience in the data engineering/science space",
                    "For ETL -- Into snowflake using Kafka - in snowflake its python and IDMC",
                    "Experience doing large enterprise data migrations like they are doing"
                ],
                "Responsibilities": [
                    "More delegating because they will manage 2 teams - architects are doing the modeling so these leads will be working with the business most"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06 Database Architects"
            ]
        },
        {
            "employer_name": "Sam's Club",
            "employer_logo": "https://scene7.samsclub.com/is/image/samsclub/sc_logo_blue?fmt=png-alpha&resMode=sharp2&op_usm=1.75,0.3,2,0&wid=200",
            "employer_website": "http://www.samsclub.com",
            "employer_company_type": "Retail",
            "job_publisher": "Walmart Careers",
            "job_id": "GjPMRB4MIFMAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Manager I, Data Engineering",
            "job_apply_link": "https://careers.walmart.com/us/jobs/WD1527986-senior-manager-i-data-engineering",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6831,
            "job_description": "Position Summary...\n\nWhat you'll do...\n\nAs a Senior Manager I, Data Engineer you are managing a large global Engineering team building scalable, highly available, enterprise-grade data flows. You will drive the execution of multiple data engineering project that unlocks the potential of data for the business. You will lead and inspire your teams while effectively prioritizing work and maintaining a strong customer focus to create business value while putting processes in-place to ensure successful outcomes. You will find ways to deliver incremental value while solving tactical problems strategically. Quantifying opportunities and measuring the quality, performance, and business impact of our solutions and use data to inform prioritization and operations.\n\nAbout Team:\nSam's Club is our membership warehouse club, a business model that provides our members with high-quality products at prices that are unrivaled by traditional retail. Sam's Club provides a carefully curated assortment of items, as well as developing and leading technologies and services such as Scan & Go, Club Pickup, and home delivery service in select markets. Sam's Club also provides travel, auto purchasing, pharmacy, optical, hearing aid centers, tire and battery centers, and a portfolio of business operations support services.\n\nWhat you'll do:\n\u2022 Lead a team of Data engineers to design, build, test and deploy cutting edge solutions at scale, impacting millions of customers worldwide drive value from data.\n\u2022 Interact with Sam's Club engineering teams across geographies to leverage expertise and contribute to the tech community.\n\u2022 Engage with Product Management and Business to drive the agenda, set your priorities and deliver awesome product features to keep platform ahead of market scenarios.\n\u2022 Identify right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forums.\n\u2022 Develop and/or Contribute to add features that enable adoption of data across Sam\u2019s Club.\n\u2022 Deploy and monitor products on Cloud platforms.\n\u2022 Develop and implement best-in-class monitoring processes to enable data applications meet SLAs.\n\u2022 Guide the team technically for end-to-end solution Lifecyle.\n\nWhat you'll bring:\n\u2022 3 - 5 Years of Experience leading a high performing data engineering team.\n\u2022 8 - 10 years of Big data development experience.\n\u2022 2 -3 Years of Experience in GCP/Azure cloud platforms.\n\u2022 2 -3 years of experience building streaming pipeline using spark streaming or similar.\n\u2022 Demonstrates up-to-date expertise in Data Engineering, complex data pipeline development.\n\u2022 Architect, Design, develop, implement and tune distributed data processing pipelines that process large volume of data; focusing on scalability, low -latency, and fault-tolerance in every system built.\n\u2022 Exposure to Data Governance ( Data Quality, Metadata Management, Security, etc.).\n\u2022 Experience with Java, Scala and/or Python to write data pipelines and data processing layers.\n\u2022 Demonstrates expertise in writing complex, highly-optimized queries across large data sets.\n\u2022 Proven working expertise with Big Data Technologies Spark Scala/PySpark, and SQL.\n\u2022 Knowledge and experience in Kafka, Storm, Druid and Presto.\n\u2022 Architect, Design, develop, implement, and tune distributed data processing pipelines that process large volume of data; focusing on scalability, low -latency, and fault-tolerance in every system built.\n\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That\u2019s what we do at Walmart Global Tech. We\u2019re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world\u2019s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work:\nWe use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.\n\nBenefits:\nBenefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer \u2013 By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions \u2013 while being inclusive of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\n\nAt Sam's Club, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.\n\nYou will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .\n\nLive Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\n\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. For information about benefits and eligibility, see One.Walmart at https://bit.ly/3iOOb1J .\n\nThe annual salary range for this position is $112,000.00-$192,000.00\n\nAdditional compensation includes annual or quarterly performance incentives.\n\nAdditional compensation for certain positions may also include:\n\n- Regional Pay Zone (RPZ) (based on location)\n\n- Stock equity incentives\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor\u2019s degree in Computer Science and 4 years' experience in software engineering or related field. Option 2: 6 years\u2019 experience in software engineering or related field. Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field.\n3 years' experience in data engineering, database engineering, business intelligence, or business analytics.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master\u2019s degree in Computer Science or related field and 4 years' experience in software engineering or related field, Supervisory\n\nPrimary Location...\n603 MUNGER AVE STE 400, DALLAS, TX 75202, United States of America",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1686268800,
            "job_posted_at_datetime_utc": "2023-06-09T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "dental_coverage",
                "paid_time_off",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=GjPMRB4MIFMAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "3 - 5 Years of Experience leading a high performing data engineering team",
                    "8 - 10 years of Big data development experience",
                    "2 -3 Years of Experience in GCP/Azure cloud platforms",
                    "2 -3 years of experience building streaming pipeline using spark streaming or similar",
                    "Demonstrates up-to-date expertise in Data Engineering, complex data pipeline development",
                    "Exposure to Data Governance ( Data Quality, Metadata Management, Security, etc.)",
                    "Experience with Java, Scala and/or Python to write data pipelines and data processing layers",
                    "Demonstrates expertise in writing complex, highly-optimized queries across large data sets",
                    "Proven working expertise with Big Data Technologies Spark Scala/PySpark, and SQL",
                    "Knowledge and experience in Kafka, Storm, Druid and Presto",
                    "Option 1: Bachelor\u2019s degree in Computer Science and 4 years' experience in software engineering or related field",
                    "Option 3: Master's degree in Computer Science and 2 years' experience in software engineering or related field",
                    "Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master\u2019s degree in Computer Science or related field and 4 years' experience in software engineering or related field, Supervisory"
                ],
                "Responsibilities": [
                    "As a Senior Manager I, Data Engineer you are managing a large global Engineering team building scalable, highly available, enterprise-grade data flows",
                    "You will drive the execution of multiple data engineering project that unlocks the potential of data for the business",
                    "You will lead and inspire your teams while effectively prioritizing work and maintaining a strong customer focus to create business value while putting processes in-place to ensure successful outcomes",
                    "You will find ways to deliver incremental value while solving tactical problems strategically",
                    "Lead a team of Data engineers to design, build, test and deploy cutting edge solutions at scale, impacting millions of customers worldwide drive value from data",
                    "Interact with Sam's Club engineering teams across geographies to leverage expertise and contribute to the tech community",
                    "Engage with Product Management and Business to drive the agenda, set your priorities and deliver awesome product features to keep platform ahead of market scenarios",
                    "Identify right open source tools to deliver product features by performing research, POC/Pilot and/or interacting with various open source forums",
                    "Develop and/or Contribute to add features that enable adoption of data across Sam\u2019s Club",
                    "Deploy and monitor products on Cloud platforms",
                    "Develop and implement best-in-class monitoring processes to enable data applications meet SLAs",
                    "Guide the team technically for end-to-end solution Lifecyle"
                ],
                "Benefits": [
                    "Benefits: Beyond our great compensation package, you can receive incentive awards for your performance",
                    "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more",
                    "Health benefits include medical, vision and dental coverage",
                    "Financial benefits include 401(k), stock purchase and company-paid life insurance",
                    "Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting",
                    "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
                    "You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
                    "It will meet or exceed the requirements of paid sick leave laws, where applicable",
                    "Tuition, books, and fees are completely paid for by Walmart",
                    "The annual salary range for this position is $112,000.00-$192,000.00",
                    "Additional compensation includes annual or quarterly performance incentives",
                    "Additional compensation for certain positions may also include:",
                    "Regional Pay Zone (RPZ) (based on location)",
                    "Stock equity incentives"
                ]
            },
            "job_job_title": "Senior manager",
            "job_posting_language": "en",
            "job_onet_soc": "11302100",
            "job_onet_job_zone": "4",
            "job_naics_code": "452990",
            "job_naics_name": "All Other General Merchandise Stores"
        },
        {
            "employer_name": "Capco",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Capco_logo.svg/800px-Capco_logo.svg.png",
            "employer_website": "http://www.capco.com",
            "employer_company_type": "Consulting",
            "job_publisher": "Salary.com",
            "job_id": "3KBM7t96SksAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.salary.com/job/capco/data-engineer/j202301250117332463849",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6827,
            "job_description": "Data Engineer\n\nAbout the team:\n\nCapco\u2019s Data Team helps our clients transform every aspect of their business. We are highly skilled at formulating data strategy, defining business and technology initiatives across the data management lifecycle, and aligning multi-year strategic roadmaps with client\u2019s business goals. As digital technologies advance and regulations tighten, today\u2019s consumers \u2013 and, therefore, today\u2019s businesses \u2013 are becoming more aware of the importance of good quality data. We work to establish holistic ways to effectively manage data through the modern data supply chain and facilitate consumption through analytics, modelling, AI, machine learning, dashboarding, and reporting.\n\nAbout the Job:\n\nAs a member of our Data Team, you will work across Capco\u2019s different domains and solution offerings to help break down large problems, develop approaches and solutions. As a Data Engineer, you will create analytics reporting and provide data-driven strategic insights, trends, and perspective to help drive transformation for our clients.\n\nWhat You\u2019ll Get to Do:\n\u2022 Work on hard problems with smart people\n\u2022 Be highly motivated, result-oriented, and take pride in being a problem solver\n\u2022 Work with new technology, focus on using the right tool for the job, rather than any sticky preference for a tool or technology\n\u2022 Learn and share knowledge across our engineering teams, so we can continue to iterate and improve\n\u2022 Write reusable, testable, and efficient code as needed\n\u2022 Design and implement of low-latency, high-availability, and performant database systems\n\u2022 Work on implementation of data pipelines/data warehouses/data marts/ODSs/Lakehouses\n\u2022 Collaborate and work on integration of data storage solutions\n\u2022 Focus on performance tuning, improvement, balancing, usability and automation\n\nWhat You\u2019ll Bring with You:\n\u2022 6 years of demonstrated hands on data engineering experience-- preferably in consulting or financial services\n\u2022 Success working in ANY ONE of the following Data Engineering/Management technology areas, including but not limited to:\n\u2022 Experience with ETL tools with data tracking\n\u2022 Experience developing ETL Data Pipelines and Data Model solutions for integrating new data sets into a data platform from different sources.\n\u2022 Experience implementing enterprise systems\n\u2022 Experience with Big Data (Hadoop, MongoDB, Exadata)\n\u2022 Experience developing real time ETL solutions (Kafka, Kinesis, etc)\n\u2022 Strong knowledge of RDBS systems (Oracle preferred)\n\u2022 Cloud Data Platforms across Azure and/or AWS is a plus\n\u2022 Strong data modelling skills\n\u2022 Experience developing complex SQL queries and PL/SQL\n\u2022 PL/SQL development experience with enterprise software systems\n\u2022 Experience developing Shell scripts and Python programs to automate tasks\n\u2022 Knowledge of BI Solutions (MS Power BI, Tableau, Qlik, Alteryx, etc.)\n\nWhy Capco?\n\nA career at Capco is a chance to help reshape the competitive landscape in financial services. We launch new banks, transform existing ones, and help our clients navigate complex change. As consultants, we work on the front-end business design all the way through to technology implementation.\n\nWe are the largest Financial Services focused consultancy in the world, serving everyone from global banks to emerging FinTechs, from strategy through digital transformation, design, business consulting, data and analytics, cyber, cloud, technology architecture, and engineering.\n\nCapco is a young and growing firm. We maintain an entrepreneurial spirit and growth mindset and have minimal bureaucracy. We have no internal silos that get in the way of your career opportunities or ability to focus on our clients and make a difference to the business. We offer the opportunity for everyone to learn rapidly, take on tough challenges, and get promoted quickly. We take pride in our creative, collaborative, diverse, and inclusive culture, where everyone can #BYAW.\n\nWe offer highly competitive benefits, including medical, dental and vision insurance, a 401(k) plan, tuition reimbursement, and a work culture focused on innovation and creation of lasting value for our clients and employees.\n\nReady to take the Next Step?\n\nIf this sounds like you, we would love to hear from you. This is an opportunity to make a difference and contribute to a highly successful company with a significant growth trajectory.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1684454400,
            "job_posted_at_datetime_utc": "2023-05-19T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "health_insurance",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=3KBM7t96SksAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-11-18T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1700265600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 72,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "6 years of demonstrated hands on data engineering experience-- preferably in consulting or financial services",
                    "Success working in ANY ONE of the following Data Engineering/Management technology areas, including but not limited to:",
                    "Experience with ETL tools with data tracking",
                    "Experience developing ETL Data Pipelines and Data Model solutions for integrating new data sets into a data platform from different sources",
                    "Experience implementing enterprise systems",
                    "Experience with Big Data (Hadoop, MongoDB, Exadata)",
                    "Experience developing real time ETL solutions (Kafka, Kinesis, etc)",
                    "Strong data modelling skills",
                    "Experience developing complex SQL queries and PL/SQL",
                    "PL/SQL development experience with enterprise software systems",
                    "Experience developing Shell scripts and Python programs to automate tasks",
                    "Knowledge of BI Solutions (MS Power BI, Tableau, Qlik, Alteryx, etc.)"
                ],
                "Responsibilities": [
                    "As a Data Engineer, you will create analytics reporting and provide data-driven strategic insights, trends, and perspective to help drive transformation for our clients",
                    "Work on hard problems with smart people",
                    "Learn and share knowledge across our engineering teams, so we can continue to iterate and improve",
                    "Write reusable, testable, and efficient code as needed",
                    "Design and implement of low-latency, high-availability, and performant database systems",
                    "Work on implementation of data pipelines/data warehouses/data marts/ODSs/Lakehouses",
                    "Collaborate and work on integration of data storage solutions",
                    "Focus on performance tuning, improvement, balancing, usability and automation"
                ],
                "Benefits": [
                    "We offer highly competitive benefits, including medical, dental and vision insurance, a 401(k) plan, tuition reimbursement, and a work culture focused on innovation and creation of lasting value for our clients and employees"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541613",
            "job_naics_name": "Marketing Consulting Services"
        },
        {
            "employer_name": "CTG",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRMA_-RRfem6FTx7bf27YkAaVBdXBLZdJXlPOgn&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Nexxt",
            "job_id": "t_bwv82knGAAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer - Legacy Data Conversion",
            "job_apply_link": "https://www.nexxt.com/jobs/data-engineer-legacy-data-conversion-dallas-tx-2545869795-job.html?aff=2ED44C72-8FD2-4B5D-BC54-2F623E88BE26",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4993,
            "job_description": "Data Engineer - Legacy Data Conversion\n\nUnited States\n\nNew\n\nHealthcare IT\n\nJul 13, 2023Post Date\n\n23200980Requisition #\n\nApply for JobShare this JobSign Up for Job Alerts\n\nCTG is seeking an experienced Data Engineer for a remote opportunity!\n\nJob responsibilities will include\n\u2022 Develop and maintain data extracts and data models\n\u2022 Tools used include SSMS SSAS / SSMS SSIS, ETL and ELT processes\n\u2022 On-call rotations exist for this team, and would be every 3-4 weeks.\n\nAfter an initial Knowledge Transfer period of 2 to 4 weeks at the client site (tentative), work will be performed remotely, with the exception of a major outage.\n\nREQUIRED SKILLS (primary and nice-to-have): (Required means that the client only wants to see candidates who have these skills).\n\n3-5 years\u2019 experience in SSIS, ETL and ELT processes are required.\n\n3-5 years\u2019 experience in understanding data models and extracts\n\nUnderstanding in the construct of data cubes\n\nFamiliarity with Data Lineage and Data Governance\n\nOptional/Preferred experience:\n\nexposure to healthcare and systems, preferably Epic or Cerner\n\nexposure to Dimensional Insights\n\nwould also be familiarity working in healthcare IT database systems\n\nworking with Cerner Millennium data\n\ntroubleshooting Tableau Dashboards\n\nfamiliarity with Star Schema database design\n\nfamiliarity with python scripting\n\nHealthCatalyst\n\nDimensional Insights\n\nPlease email ~~~ if interested\n\nCTG is a leading provider of digital transformation solutions and services that accelerate clients' project momentum and achievement of their desired IT and business outcomes. Our vision is to be an indispensable partner to our clients and the preferred career destination for digital and technology experts. CTG has operations in North America, South America, Western Europe, and India. For more information, visit ~~~.\n\nOur culture is a direct result of the people who work at CTG, the values we hold, and the actions we take. In other words, our people are the culture. It's a living, breathing thing that is renewed every day through the ways we engage with each other, our clients, and our communities. Part of our mission is to cultivate a workplace that attracts and develops the best people, reflected by our recognition as a Great Place to Work-certified company across many of our global operations.\n\nCTG will consider for employment all qualified applicants including those with criminal histories in a manner consistent with the requirements of all applicable local, state, and federal laws.\n\nCTG is an Equal Opportunity and Affirmative Action Employer. CTG will assure equal opportunity and consideration to all applicants and employees in recruitment, selection, placement, training, benefits, compensation, promotion, transfer, and release of individuals without regard to race, creed, religion, color, national origin, sex, sexual orientation, gender identity and gender expression, age, disability, marital or veteran status, citizenship status, or any other discriminatory factors as required by law. Our Affirmative Action program serves to promote occupational equality and diversity through good faith efforts. CTG is fully committed to promoting employment opportunities for members of protected classes.\n\nAdditional Information\n\u2022 Job Function: Application Management & Support\n\u2022 Education Level: Bachelor's Degree (\u00b116 years)\n\u2022 Work Remote: Yes\n\u2022 Travel: Yes, 5 % of the Time",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689356309,
            "job_posted_at_datetime_utc": "2023-07-14T17:38:29.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=t_bwv82knGAAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "REQUIRED SKILLS (primary and nice-to-have): (Required means that the client only wants to see candidates who have these skills)",
                    "3-5 years\u2019 experience in SSIS, ETL and ELT processes are required",
                    "3-5 years\u2019 experience in understanding data models and extracts",
                    "Understanding in the construct of data cubes",
                    "Familiarity with Data Lineage and Data Governance",
                    "exposure to healthcare and systems, preferably Epic or Cerner",
                    "exposure to Dimensional Insights",
                    "would also be familiarity working in healthcare IT database systems",
                    "working with Cerner Millennium data",
                    "familiarity with Star Schema database design",
                    "familiarity with python scripting",
                    "Education Level: Bachelor's Degree (\u00b116 years)"
                ],
                "Responsibilities": [
                    "Develop and maintain data extracts and data models",
                    "Tools used include SSMS SSAS / SSMS SSIS, ETL and ELT processes",
                    "On-call rotations exist for this team, and would be every 3-4 weeks",
                    "After an initial Knowledge Transfer period of 2 to 4 weeks at the client site (tentative), work will be performed remotely, with the exception of a major outage",
                    "troubleshooting Tableau Dashboards"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Anblicks",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "ZipRecruiter",
            "job_id": "BTtXgAW3LgEAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.ziprecruiter.com/c/Anblicks/Job/Data-Engineer/-in-Dallas,TX?jid=6f97aba4bed93265",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7224,
            "job_description": "Data Engineer\n\nDesign and develop data pipeline models to extract, transform and load data from heterogenous source systems onto common data repository/data lake for provisioning data to various downstream teams to help them build their reports and dashboards for various analytics purposes and derive key business insights to enhance the user/customer satisfaction. Use Python, PySpark, Hive QL, Spark, Snowflake, BitBucket, Autosys, Jenkins CI/CD, Tableau, JuPyter, and ShellScripting to design, develop, extract, load, build, and code data models and frameworks. Gather requirements from stakeholders and identify the source data systems that provide transactional data. Build reusable framework and code scripts using python, spark, shell and yaml to automate data pipelines on top of distributed Hadoop cluster. Perform unit and regression testing to make sure developed data pipeline framework meets data quality and integrity standards. Generate key performance indicators metrics from data stored on reporting layer which is built by creating views on top of data objects in data lake and communicate with business teams in regular intervals when any deviations observed in data. May require travel/relocation to unanticipated work locations within the USA.\n\nRequire Master of Science in Computer Science/Engineering, computer/management Information Systems/Technology, or related field and 1 year of experience in the job offered, software engineer/developer, data architect/engineer, Hadoop developer/associate/consultant, or related field.\n\nPlease mail your resume to Maruthi Technologies, Inc, 14911 Quorum Drive, Suite 390, Dallas, TX 75254\n\nEmployment Type: FULL_TIME",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690441200,
            "job_posted_at_datetime_utc": "2023-07-27T07:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=BTtXgAW3LgEAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-27T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693094400,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 12,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "May require travel/relocation to unanticipated work locations within the USA",
                    "Require Master of Science in Computer Science/Engineering, computer/management Information Systems/Technology, or related field and 1 year of experience in the job offered, software engineer/developer, data architect/engineer, Hadoop developer/associate/consultant, or related field"
                ],
                "Responsibilities": [
                    "Design and develop data pipeline models to extract, transform and load data from heterogenous source systems onto common data repository/data lake for provisioning data to various downstream teams to help them build their reports and dashboards for various analytics purposes and derive key business insights to enhance the user/customer satisfaction",
                    "Use Python, PySpark, Hive QL, Spark, Snowflake, BitBucket, Autosys, Jenkins CI/CD, Tableau, JuPyter, and ShellScripting to design, develop, extract, load, build, and code data models and frameworks",
                    "Gather requirements from stakeholders and identify the source data systems that provide transactional data",
                    "Build reusable framework and code scripts using python, spark, shell and yaml to automate data pipelines on top of distributed Hadoop cluster",
                    "Perform unit and regression testing to make sure developed data pipeline framework meets data quality and integrity standards",
                    "Generate key performance indicators metrics from data stored on reporting layer which is built by creating views on top of data objects in data lake and communicate with business teams in regular intervals when any deviations observed in data"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-2041.00: Statisticians"
            ]
        },
        {
            "employer_name": "Amazon.com Services, Inc.",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Dallas, TX - Geebo",
            "job_id": "YWCs__E_bzUAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer",
            "job_apply_link": "https://dallas-tx.geebo.com/jobs-online/view/id/761054315-senior-data-engineer-/",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.4224,
            "job_description": "Are you interested in innovating to deliver a world-class level of service to Amazon's Vendors and Selling Partners?We are looking for a talented Senior Data Engineer to help build/enhance the data landscape that shapes decision making and recommendations driving further value for our Customers, Vendors, and Selling Partners.\nYou will help drive data architecture across many large datasets, perform exploratory data analysis, implement new data pipelines that feed into or from critical data systems at Amazon, and your insights will impact millions of Customers, Vendors and Sellers who communicate with each other for mission-critical use cases every day.\nAs a Senior Data Engineer, you will develop new data engineering patterns that leverage new cloud architectures, and will extend or migrate existing data pipelines to the architectures as needed.\nYou will also be assisting with integrating the Redshift platform as our primary processing platform to create the curated Amazon.\ncom data model for the enterprise to leverage.\nYou will be responsible for designing and implementing the complex ETL pipelines in data warehouse platform and other BI solutions to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making at Amazon.\ncom.\nEstimated Salary: $20 to $28 per hour based on qualifications.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690416000,
            "job_posted_at_datetime_utc": "2023-07-27T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=YWCs__E_bzUAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-03T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1691020800,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": false,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 20,
            "job_max_salary": 28,
            "job_salary_currency": "USD",
            "job_salary_period": "HOUR",
            "job_highlights": {
                "Responsibilities": [
                    "You will help drive data architecture across many large datasets, perform exploratory data analysis, implement new data pipelines that feed into or from critical data systems at Amazon, and your insights will impact millions of Customers, Vendors and Sellers who communicate with each other for mission-critical use cases every day",
                    "As a Senior Data Engineer, you will develop new data engineering patterns that leverage new cloud architectures, and will extend or migrate existing data pipelines to the architectures as needed",
                    "You will also be assisting with integrating the Redshift platform as our primary processing platform to create the curated Amazon"
                ],
                "Benefits": [
                    "Estimated Salary: $20 to $28 per hour based on qualifications"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "JPMorgan Chase",
            "employer_logo": "https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=11465191261",
            "employer_website": "http://www.jpmorganchase.com",
            "employer_company_type": "Finance",
            "job_publisher": "Built In",
            "job_id": "mOyDU6ACctkAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior Data Engineer- AWS (Dallas, TX)",
            "job_apply_link": "https://builtin.com/job/data/senior-data-engineer-aws/1898846",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6987,
            "job_description": "Job Description\nIf you're interested in making significant contributions to your team and business, you're heading the right direction, but even more so- with our Data & Analytics team, the difference we make is to help provide tools and resources that give senior management what they need to make decisions that will keep JP Morgan Chase ahead of the competition through leveraging data across Chase to build advantages for the businesses while providing value and protection for customers.\nJob summary\nThe Data Analytics and Reporting (DART) team's mission is to provide high-quality dashboards & insights that executive leadership will use to monitor KPIs, steer the business, and make investment decisions. This team will constantly balance the need for speed with the importance of providing accurate results, to enable leadership to make informed decisions with agility & precision. The position will be part of a team that will take a loosely-defined idea or problem statement, find & acquire the relevant data, design & build the dashboards, develop the automated workflows, and document the details. You will also help to develop frameworks to ensure cross-platform data & logic consistency and optimization. They will write code to enrich and transform data sets to produce output that will help to advance the DART teams objectives and impact to the broader organization.\nJob responsibilities\n\u2022 Perform user acceptance testing and deliver demos to stakeholders by SQL queries or Python scripts.\n\u2022 Perform data analysis to define / support model development including metadata and data dictionary documentation that will enable data analysis and analytical exploration\n\u2022 Participate in strategic projects and provide ideas and inputs on ways to leverage quantitative analytics to generate actionable business insights and/or solutions to influence business strategies and identify opportunities to grow\n\u2022 Partners closely with business partners to identify impactful projects, influence key decisions with data, and ensure client satisfaction\n\nRequired qualifications, capabilities, and skills\n\u2022 Scripting skills e.g. Python, R and experience with tools such as Alteryx or similar tool\n\u2022 Strong understanding of data and database methodologies as well as hands on Oracle and/or AWS Cloud experience\n\u2022 Bachelor's degree in a relevant quantitative field (e.g. Statistics, Economics, Finance, Business Analytics, Mathematics, Engineering, Computer Science, Information Technology)\n\u2022 5+ years of industry experience in business analytics roles (e.g., marketing analytics, product analytics, business insights)\n\u2022 5+ years of work experience across broad range of analytics platforms, languages, and tools (SAS, SQL, Spark and Python, Unix, Excel Pivot etc.); hands-on experience using big data platform required\n\u2022 Strong understanding of CI/CD Pipelines in a globally distributed environment using Git, Bit-Bucket, Jenkins, Spinnaker, etc.\n\u2022 Experience with the entire Software Development Life Cycle (SDLC) including planning, analysis, development and testing of new applications and enhancements to existing application.\n\nPreferred qualifications, capabilities, and skills\n\u2022 Master's degree or Bachelor degree with 5+ years experience in a relevant quantitative field (e.g. Statistics, Economics, Finance, Business Analytics, Mathematics, Engineering, Computer Science or any related fields involving significant quantitative research & data analytics)\n\u2022 Strong communicator who's able to convey complex information in an understandable, compelling, and persuasive manner to business partners\n\u2022 Results-oriented with a strong attention to details\n\u2022 Strong knowledge in quantitative methods for business analytics; proficiency in critical thinking and problem solving\n\nAbout Us\nChase is a leading financial services firm, helping nearly half of America's households and small businesses achieve their financial goals through a broad range of financial products. Our mission is to create engaged, lifelong relationships and put our customers at the heart of everything we do. We also help small businesses, nonprofits and cities grow, delivering solutions to solve all their financial needs.\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.\nThe health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the \"WELL Health-Safety Rating\" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.\nAs a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.\nWe offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location. For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions. We also offer a range of benefits and programs to meet employee needs, based on eligibility. These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more. Additional details about total compensation and benefits will be provided during the hiring process.\nEqual Opportunity Employer/Disability/Veterans\nAbout the Team\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We're proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions - all while ranking first in customer satisfaction.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688270440,
            "job_posted_at_datetime_utc": "2023-07-02T04:00:40.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "health_insurance",
                "retirement_savings",
                "dental_coverage"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=mOyDU6ACctkAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Scripting skills e.g. Python, R and experience with tools such as Alteryx or similar tool",
                    "Strong understanding of data and database methodologies as well as hands on Oracle and/or AWS Cloud experience",
                    "Bachelor's degree in a relevant quantitative field (e.g. Statistics, Economics, Finance, Business Analytics, Mathematics, Engineering, Computer Science, Information Technology)",
                    "5+ years of industry experience in business analytics roles (e.g., marketing analytics, product analytics, business insights)",
                    "5+ years of work experience across broad range of analytics platforms, languages, and tools (SAS, SQL, Spark and Python, Unix, Excel Pivot etc.); hands-on experience using big data platform required",
                    "Strong understanding of CI/CD Pipelines in a globally distributed environment using Git, Bit-Bucket, Jenkins, Spinnaker, etc",
                    "Experience with the entire Software Development Life Cycle (SDLC) including planning, analysis, development and testing of new applications and enhancements to existing application"
                ],
                "Responsibilities": [
                    "The Data Analytics and Reporting (DART) team's mission is to provide high-quality dashboards & insights that executive leadership will use to monitor KPIs, steer the business, and make investment decisions",
                    "This team will constantly balance the need for speed with the importance of providing accurate results, to enable leadership to make informed decisions with agility & precision",
                    "The position will be part of a team that will take a loosely-defined idea or problem statement, find & acquire the relevant data, design & build the dashboards, develop the automated workflows, and document the details",
                    "You will also help to develop frameworks to ensure cross-platform data & logic consistency and optimization",
                    "They will write code to enrich and transform data sets to produce output that will help to advance the DART teams objectives and impact to the broader organization",
                    "Perform user acceptance testing and deliver demos to stakeholders by SQL queries or Python scripts",
                    "Perform data analysis to define / support model development including metadata and data dictionary documentation that will enable data analysis and analytical exploration",
                    "Participate in strategic projects and provide ideas and inputs on ways to leverage quantitative analytics to generate actionable business insights and/or solutions to influence business strategies and identify opportunities to grow",
                    "Partners closely with business partners to identify impactful projects, influence key decisions with data, and ensure client satisfaction"
                ],
                "Benefits": [
                    "We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location",
                    "For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions",
                    "We also offer a range of benefits and programs to meet employee needs, based on eligibility",
                    "These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "523920",
            "job_naics_name": "Portfolio Management"
        },
        {
            "employer_name": "Ad Hoc Team",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTLqIeSWzqeEd9GPo6i2Vz7XXH8GRDbuUwF3NLz&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Glassdoor",
            "job_id": "B9mwS3vp2lgAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer (Remote)",
            "job_apply_link": "https://www.glassdoor.com/job-listing/data-engineer-remote-ad-hoc-dc-JV_IC1139977_KO0,20_KE21,30.htm?jl=1008732513836",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.573,
            "job_description": "This is a fully remote position.\n\nWork on things that matter\n\nAd Hoc is a digital services company that helps the federal government better serve people. Our teams use modern, agile methods to design and engineer government systems that connect Veterans with services, bring affordable health care to millions of people, and support important programs like Head Start. And as we work to make critical government services intuitive, accessible, and human-centered, we're also changing how the government thinks about and uses technology. If you thrive on change, want to help close the gap between consumer expectations and government services, and can see the possibilities in ambiguity, then we want you here with us.\n\nWhat matters most\n\nAd Hoc operates according to our commitment to inclusivity, acceptance, accountability, and humility. We aren't heroes. We believe in missions larger than our individual selves and leave our egos at the door, learn from our mistakes, and iterate in order to better serve the people in our country. We prioritize building teams that represent the diversity of the people our government serves. We love the challenge of government-size projects. We want to bring skills to federal agencies, help them better meet the needs of their users, and close the gap between consumer expectations and government.\n\nBuilt for a remote life\n\nAd Hoc is remote-first and remote-always. We've designed our culture, communications, and tools to support a nationwide distributed team since the beginning. Being remote by design allows Ad Hoc to be thoughtful and intentional about creating diverse teams and supporting them with a work environment that fits their lives. With a generous PTO policy and Slack channels for every interest (from bird watching to space nerds to parenting) our culture embraces the things happening in your life. Maybe you need to adjust your schedule to care for your family or take a bike ride. At Ad Hoc, that's embraced.\n\nWhat you'll do\n\nData Engineers are responsible for working with the systems and infrastructure that enable data storage, processing, and analysis. They work closely with data scientists and analysts to ensure that data is properly collected, organized, enriched, refined, and available for analysis. They are the critical connectors between the teams that maintain existing legacy systems, and the data analysts and data scientists that will use aggregated data for analysis, reporting, and predictive analytics.\n\u2022 Shipping software that impacts the lives of millions of people\n\u2022 Using modern programming languages and frameworks to build scalable services that gracefully integrate with legacy systems\n\u2022 Building and working with APIs to support both the digital services we deliver as well as third-party usage\n\u2022 Helping us continuously, iteratively improve\n\nWhat we hope you'll bring\n\u2022 A minimum of four (4) years of professional software development experience\n\u2022 AWS experience\n\u2022 Understanding of ETL/ELT processes and tooling\n\u2022 Understanding of database technologies - setup/ maintenance / data loads / etc. (not data modeling)\n\u2022 Redshift experience preferred\n\u2022 Understand system security\n\u2022 API design and implementation\n\u2022 GIT and DevOps release process\n\u2022 Python or Scala, Python is preferred for ETL\n\u2022 Some experience with older file systems / file based processes such as MOVEit\n\u2022 Some experience with Mulesoft\n\u2022 Experience with agile software development practices emphasizing agility, flexibility, and iterative development\n\nMore than that, our ideal candidate wants to contribute to work that is bigger than themselves and wants to make a difference collaborating with their team. They care deeply about building better products, better relationships, and better trust in each interaction people have with their government. They believe in intuitive, easy-to-use government services. They collaborate well with designers, stakeholders, and other teams. They mentor and guide more junior engineers. They're human-centered.\n\nAnd if you don't check every box on the list? That doesn't mean you can't help us in our mission to deliver critical government services. Talk to us!\n\nSome basic requirements\n\u2022 All work must be conducted within the U.S., excluding U.S. territories. Some federal contracts require U.S. citizenship to be eligible for employment.\n\u2022 You must be legally authorized to work in the U.S now and in the future without sponsorship.\n\u2022 As a government contractor, you may be required to obtain a public trust security clearance.\n\u2022 Bachelor's Degree in a technical field is preferred\n\u2022 4 years of professional software development\n\u2022 Our technical screening involves completing a homework assignment that is then graded blind to remove bias. We do not do tricky, unreliable whiteboarding tests. You can read more about our homework here.\n\nLearn more about engineering at Ad Hoc.\n\nBenefits\n\u2022 Company-subsidized Health, Dental, and Vision Insurance\n\u2022 Use What You Need Vacation Policy\n\u2022 401K with employer match\n\u2022 Paid parental leave after one year of service\n\nAd Hoc LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, sex, sexual orientation, gender identity or expression, religion, age, pregnancy, disability, work-related injury, covered veteran status, political ideology, marital status, or any other factor that the law protects from employment discrimination.\n\nIn support of the Colorado Equal Pay Transparency Act, and others like it across the country, Ad Hoc job descriptions feature the starting range we reasonably expect to pay to candidates who would join our team with little to no need for training on the responsibilities we've outlined above. Actual compensation is influenced by a wide range of factors including but not limited to skill set, level of experience, and responsibility. The range of starting pay for this role is $101,570 - $136,994 and information on benefits offered is here. Our recruiters will be happy to answer any questions you may have, and we look forward to learning more about your salary requirements.\n\njob reference: 2015",
            "job_is_remote": true,
            "job_posted_at_timestamp": 1688083200,
            "job_posted_at_datetime_utc": "2023-06-30T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "paid_time_off",
                "dental_coverage",
                "health_insurance",
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=B9mwS3vp2lgAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 48,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 101570,
            "job_max_salary": 136994,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "We want to bring skills to federal agencies, help them better meet the needs of their users, and close the gap between consumer expectations and government",
                    "A minimum of four (4) years of professional software development experience",
                    "AWS experience",
                    "Understanding of ETL/ELT processes and tooling",
                    "Understanding of database technologies - setup/ maintenance / data loads / etc",
                    "(not data modeling)",
                    "Understand system security",
                    "Some experience with older file systems / file based processes such as MOVEit",
                    "Some experience with Mulesoft",
                    "Experience with agile software development practices emphasizing agility, flexibility, and iterative development",
                    "All work must be conducted within the U.S., excluding U.S. territories",
                    "Some federal contracts require U.S. citizenship to be eligible for employment",
                    "You must be legally authorized to work in the U.S now and in the future without sponsorship",
                    "As a government contractor, you may be required to obtain a public trust security clearance"
                ],
                "Responsibilities": [
                    "Data Engineers are responsible for working with the systems and infrastructure that enable data storage, processing, and analysis",
                    "They work closely with data scientists and analysts to ensure that data is properly collected, organized, enriched, refined, and available for analysis",
                    "They are the critical connectors between the teams that maintain existing legacy systems, and the data analysts and data scientists that will use aggregated data for analysis, reporting, and predictive analytics",
                    "Shipping software that impacts the lives of millions of people",
                    "Using modern programming languages and frameworks to build scalable services that gracefully integrate with legacy systems",
                    "Building and working with APIs to support both the digital services we deliver as well as third-party usage",
                    "Helping us continuously, iteratively improve"
                ],
                "Benefits": [
                    "Company-subsidized Health, Dental, and Vision Insurance",
                    "Use What You Need Vacation Policy",
                    "401K with employer match",
                    "Paid parental leave after one year of service",
                    "The range of starting pay for this role is $101,570 - $136,994 and information on benefits offered is here"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "McKinsey & Company",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTe9SJcOULyOseFld0yxO8Tun4CwnLPCamc7rqf&s=0",
            "employer_website": null,
            "employer_company_type": "Consulting",
            "job_publisher": "JobzMall",
            "job_id": "4cTj1j-ZIEwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.jobzmall.com/mckinsey-company/job/data-engineer-113",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.5696,
            "job_description": "Your main expertise and responsibility will be data identification and extraction. Leveraging proprietary and third-party data-extraction tools (e.g. Snaplogic), you will identify spend relevant data from general ledger, purchase order, material & supplier master and other relevant tables within ERP or data/business warehouse applications. You will conduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools. With your experience in data engineering and data models, you will build sustainable data pipelines and adjust them for special needs. You will conduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools. With your experience in data engineering and data models, you will build sustainable data pipelines and adjust them for special needs.\n\nMcKinsey & Company is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1610146599,
            "job_posted_at_datetime_utc": "2021-01-08T22:56:39.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=4cTj1j-ZIEwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T23:59:59.000Z",
            "job_offer_expiration_timestamp": 1693094399,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Java <br>Time Management <br>Process Improvement <br>Verbal communication <br>VBA <br>Tableau <br>Alteryx <br>Macros <br>PowerShell <br>Detail Oriented <br>Prioritizing skills <br>written communication <br>Adaptability <br>Spend analysis <br>Data visualization and reporting toolsProcess Improvement <br>Verbal communication <br>Detail Oriented <br>Prioritizing skills <br>written communication <br>Adaptability"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Responsibilities": [
                    "Your main expertise and responsibility will be data identification and extraction",
                    "Leveraging proprietary and third-party data-extraction tools (e.g. Snaplogic), you will identify spend relevant data from general ledger, purchase order, material & supplier master and other relevant tables within ERP or data/business warehouse applications",
                    "You will conduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools",
                    "With your experience in data engineering and data models, you will build sustainable data pipelines and adjust them for special needs"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "54161",
            "job_naics_name": "Management Consulting Services"
        },
        {
            "employer_name": "The Beneficient Company Group USA LLC",
            "employer_logo": "https://www.mg21.com/wp-content/uploads/2022/09/The-Beneficient-Company-Group.png",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Lensa",
            "job_id": "vQeezN_YDuwAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer/Analyst",
            "job_apply_link": "https://lensa.com/data-engineeranalyst-jobs/dallas/jd/d9df6f49861354a487aa4b420188497a",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4711,
            "job_description": "Position Summary:\n\nBen\u2019s Data Engineer/Analyst will be responsible for designing and implementing ETL, Storage, and Reporting solutions. Ben requires a technical background working with regulated environments, demonstrated Data Management skills, and a self-motivated attitude.\n\nEssential Functions:\n\u2022 Manage data design and the creation of database architecture and data repositories\n\u2022 Facilitate Data ETL for customer facing and internal processes\n\u2022 Work with departments throughout Ben that are creating data or ingesting data to build and maintain a documented set of information life cycles; serve as a knowledge resource, it must have a clear understanding of our processes and the resulting data\n\u2022 Ensure that data projects and milestones are met\n\u2022 Follow PMO/Release/Change Management policies/procedures\n\u2022 Follow and participate in all IT projects and initiatives to effectively develop and maintain an understanding of existing and new data flows\n\u2022 Interface with Project Managers, Developers, and Production support to keep processes, databases, and reports kept up to date with all changes\n\u2022 Engage stakeholders to understand and document business requirements, translate them into technical requirements, and then deliver the solution\n\u2022 Install processes for auditing data warehouse transfers, storage, and data quality\n\u2022 In-depth understanding of Python programming/objects\n\u2022 Work with stakeholders across the company that are responsible for data governance and drive the business to consensus.\n\u2022 Support release management in deployment efforts\n\u2022 Develop data flow diagrams (technical and functional)\n\u2022 Single source of truth analysis\nKnowledge, Skills, Abilities and Competencies:\n\u2022 Experience with various ETL tools in an audited environment.\n\u2022 Experience working with source control (git)\n\u2022 Experience with Data Warehouse technologies\n\u2022 Experience with enterprise reporting packages\n\u2022 Technical bachelor\u2019s degree or equivalent experience required\n\u2022 5+ years of IT industry experience required\n\u2022 5+ years of database (SQL) experience required\n\u2022 4+ year of ETL experience required\n\u2022 3+ years of Python experience required\nQualifications:\n\u2022 2+ years Snowflake experience preferred\n\u2022 3+ years of reporting experience preferred (Tableau)\n\u2022 1+ years cloud experience (AWS Lambda, S3, SQS, CloudFormation preferred)\n\u2022 Matillion experience preferred\n\u2022 Experience with CI/CD Pipelines preferred\n\u2022 Experience with monitoring and audit tools preferred\n\nIdeal Candidate Skills:\n\nSQL, ETL, Python, Snowflake, Matillion, Tableau, AWS Lambda, AWS S3, AWS SQS, AWS CloudFormation\n\nWork Model:\n\nAt present, the Ben I.T. Application Development Team has a hybrid work model that includes 2 in-office days and 3 work-from-home days per week as the rule, with rare exceptions announced in advance.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, disability, sexual orientation, national origin or any other category protected by law.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688343552,
            "job_posted_at_datetime_utc": "2023-07-03T00:19:12.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=vQeezN_YDuwAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-02T00:19:12.000Z",
            "job_offer_expiration_timestamp": 1690935552,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": [
                "Reading Comprehension",
                "Active Listening",
                "Writing",
                "Speaking",
                "Critical Thinking",
                "Active Learning",
                "Monitoring",
                "Social Perceptiveness",
                "Coordination",
                "Complex Problem Solving",
                "Programming",
                "Judgment and Decision Making",
                "Systems Analysis",
                "Systems Evaluation"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Ben requires a technical background working with regulated environments, demonstrated Data Management skills, and a self-motivated attitude",
                    "Experience with various ETL tools in an audited environment",
                    "Experience working with source control (git)",
                    "Experience with Data Warehouse technologies",
                    "Experience with enterprise reporting packages",
                    "Technical bachelor\u2019s degree or equivalent experience required",
                    "5+ years of database (SQL) experience required",
                    "4+ year of ETL experience required",
                    "3+ years of Python experience required",
                    "SQL, ETL, Python, Snowflake, Matillion, Tableau, AWS Lambda, AWS S3, AWS SQS, AWS CloudFormation"
                ],
                "Responsibilities": [
                    "Ben\u2019s Data Engineer/Analyst will be responsible for designing and implementing ETL, Storage, and Reporting solutions",
                    "Manage data design and the creation of database architecture and data repositories",
                    "Facilitate Data ETL for customer facing and internal processes",
                    "Work with departments throughout Ben that are creating data or ingesting data to build and maintain a documented set of information life cycles; serve as a knowledge resource, it must have a clear understanding of our processes and the resulting data",
                    "Ensure that data projects and milestones are met",
                    "Follow PMO/Release/Change Management policies/procedures",
                    "Follow and participate in all IT projects and initiatives to effectively develop and maintain an understanding of existing and new data flows",
                    "Interface with Project Managers, Developers, and Production support to keep processes, databases, and reports kept up to date with all changes",
                    "Engage stakeholders to understand and document business requirements, translate them into technical requirements, and then deliver the solution",
                    "Install processes for auditing data warehouse transfers, storage, and data quality",
                    "In-depth understanding of Python programming/objects",
                    "Work with stakeholders across the company that are responsible for data governance and drive the business to consensus",
                    "Support release management in deployment efforts",
                    "Develop data flow diagrams (technical and functional)",
                    "Single source of truth analysis"
                ]
            },
            "job_job_title": null,
            "job_posting_language": "en",
            "job_onet_soc": "43911100",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1199.06 Database Architects"
            ]
        },
        {
            "employer_name": "H-E-B",
            "employer_logo": "https://www.heb.com/img/header/logo.png",
            "employer_website": "http://www.heb.com",
            "employer_company_type": "Retail",
            "job_publisher": "Talent.com",
            "job_id": "rLA-OhanVgEAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Senior data engineer",
            "job_apply_link": "https://www.talent.com/view?id=d7d2d7817df4",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5006,
            "job_description": "Overview\n\nH-E-B is one of the largest, independently owned food retailers in the nation operating over 420+ stores throughout Texas and Mexico, with annual sales generating over $34 billion.\n\nDescribed by industry experts as a daring innovator and smart competitor, H-E-B has led the way with creative new concepts, outstanding service and a commitment to diversity in our workforce, workplace and marketplace.\n\nH-E-B offers a wealth of career opportunities to our 145,000+ Partners (employees), competitive compensation and benefits program and comprehensive training that lead to successful careers.\n\nResponsibilities\n\nSince H-E-B Digital Technology's inception, we've been investing heavily in our customers' digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes.\n\nThis is an exciting time to join H-E-B Digital we're using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience.\n\nIf you enjoy taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.\n\nOur team is made up of data engineers with various levels of experience. The primary focus is current customer data which our team consolidates, cleanses, formats and validates for various stakeholders within our Digital team.\n\nAs a Data Engineer you'll :\n\nDesign and build a modern data warehouse in the cloud\n\nEnhance data collection procedures to build analytic systems.\n\nWork as an Agile partner and participate in back log refinement, sprint planning, review, and retrospective.\n\nCoach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed.\n\nContribute to overall system design, architecture, security, scalability, reliability, and performance of applications.\n\nSupport the build and deployment pipeline and when necessary, both diagnose and solve production support issues.\n\nWork with Product, Design, and QA to deliver world-class digital experiences.\n\nGet the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team's architecture.\n\nWho You Are :\n\n3-5+ years of experience with ETL, Data Modeling, Data Warehousing, and working with large-scale datasets (enterprise experience is a plus!)\n\n3-5+ years of hands-on experience in multiple modern programming languages such as Python or Java\n\n2+ years of experience leveraging DevOps principals such as CI / CD and using tools like Git, Jenkins, etc.\n\nStrong working knowledge of Data Engineering and associated tools and technologies like Apache, Spark, Databricks, Python, SQL, and data lake concepts.\n\nExperience with Argo Informatica\n\nWorking experience with AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3, etc.\n\nBachelor\u2019s degree in Computer Engineering, Computer Science or related discipline, Master\u2019s Degree preferred\n\nAn equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above.\n\nIt\u2019s a Plus if you have :\n\nExperience working with other public cloud technologies AWS, GCP\n\nHands on experience with data virtualization technologies : CIS (Tibco), Denodo, or SQL 2019 Virtualization\n\nExperience building test automation to ensure data quality and accuracy\n\nProficient in data modeling (specifically RDS, PostgreSQL, Oracle)\n\nDATA3232\n\nOptions\n\nLast updated : 2023-07-24",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690156800,
            "job_posted_at_datetime_utc": "2023-07-24T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=rLA-OhanVgEAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-16T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1692144000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "3-5+ years of hands-on experience in multiple modern programming languages such as Python or Java",
                    "2+ years of experience leveraging DevOps principals such as CI / CD and using tools like Git, Jenkins, etc",
                    "Strong working knowledge of Data Engineering and associated tools and technologies like Apache, Spark, Databricks, Python, SQL, and data lake concepts",
                    "Experience with Argo Informatica",
                    "Working experience with AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3, etc",
                    "An equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above",
                    "Experience working with other public cloud technologies AWS, GCP",
                    "Hands on experience with data virtualization technologies : CIS (Tibco), Denodo, or SQL 2019 Virtualization",
                    "Experience building test automation to ensure data quality and accuracy",
                    "Proficient in data modeling (specifically RDS, PostgreSQL, Oracle)"
                ],
                "Responsibilities": [
                    "Design and build a modern data warehouse in the cloud",
                    "Enhance data collection procedures to build analytic systems",
                    "Work as an Agile partner and participate in back log refinement, sprint planning, review, and retrospective",
                    "Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed",
                    "Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications",
                    "Support the build and deployment pipeline and when necessary, both diagnose and solve production support issues",
                    "Work with Product, Design, and QA to deliver world-class digital experiences"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1243.00"
            ],
            "job_naics_code": "445110",
            "job_naics_name": "Supermarkets and Other Grocery (except Convenience) Stores"
        },
        {
            "employer_name": "Logic20/20, Inc.",
            "employer_logo": "https://res.cloudinary.com/crunchbase-production/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/ffwrq37xnvyhjt1nwrdm",
            "employer_website": "http://www.logic2020.com",
            "employer_company_type": null,
            "job_publisher": "LinkedIn",
            "job_id": "U3j2JDYImI0AAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Cloud Data Engineer",
            "job_apply_link": "https://www.linkedin.com/jobs/view/cloud-data-engineer-at-logic20-20-inc-3676014190",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5875,
            "job_description": "Company Description\n\nLogic20/20 invests in being a \u201cBest Company to Work For,\u201d where talented people come together to deliver exceptional solutions. We bring clarity, insights, and optimization to enterprise organizations spanning technology, telecommunications, utilities, healthcare, and more.\n\nWe thrive as One Team, built on values:\n\u2022 We Foster a Culture of We by prioritizing connection and collaboration.\n\u2022 We Drive toward Excellence by investing in professional growth and cultivating thought leadership.\n\u2022 We Act with Integrity by doing the right thing and bringing our best selves to the table.\n\nTo make it all possible, we\u2019ve created programs, resources, and benefits that promote connection and help you evolve your career.\n\nJob Description\n\nAs a senior or lead Data Engineer joining Logic20/20's Advanced Analytics practice, you'll be supporting a team at one of the largest utility companies in California creating data pipelines to land data in an AWS data lake, and working with data transformation and integration to support future analytics. You will improve risk modeling that will allow the client to better understand the integrity of their pipelines for improved customer safety.\n\nHear more about these efforts\u202fas Jeff Lovington shares his experience working in Data Science and Machine Learning for the Energy & Utilities sector.\n\nAbout The Team\n\nThe Logic20/20 Advanced Analytics team is where skilled professionals in data engineering, data science, and visual analytics join forces to build simple solutions for complex data problems. We make it look like magic, but for us, it\u2019s all in a day\u2019s work. As part of our team, you\u2019ll collaborate on projects that help clients spin their data into a high-performance asset, all while enjoying the company of kindred spirits who are as committed to your success as you are. And when you\u2019re ready to level up in your career, you\u2019ll have access to the training, the project opportunities, and the mentorship to get you where you want to go.\n\n\u201cWe build an environment where we really operate as one team, building up each other\u2019s careers and capabilities.\u201d \u2013 Adam Cornille, Senior Director, Advanced Analytics\n\nQualifications\n\nMust have:\n\u2022 5+ years of cloud data engineering experience\n\u2022 Strong experience designing and developing cloud ELT and data pipelines with AWS, Python, SQL, Athena\n\u2022 Must also have experience with the following technology: CLI, Glue, RDS, Pandas, KMS, S3\n\u2022 Experience with developing and operating CI/CD pipelines and other DataOps fundamentals\n\u2022 Strong understanding of data modeling, data pipelines, data lakes\n\u2022 Excellent foundation of consulting skills: analytical, written and verbal communication, and presentation skills\n\u2022 Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule\u202f\n\u2022 Bachelor's degree in Computer Science, Engineering, or a related field\n\nPreferred:\n\u2022 Experience with creating custom-built apps on a large scale\n\u2022 Experience working in the utility industry\n\nAdditional Information\n\nAll your information will be kept confidential according to EEO guidelines.\n\nCompensation range for senior data engineer: $125,000 - $175,000 annually\n\nCompensation range for lead data engineer: $145,000 - $195,000 annually\n\nAbout Logic20/20\n\nTo learn more about Logic20/20, please visit: https://www.logic2020.com/careers/life-at-logic\n\nCore Values\n\nAt Logic20/20, we are guided by three core values: Drive toward Excellence, Act with Integrity & Foster a Culture of We. These values were generated and agreed upon by our employees\u2014and they help us pursue our goal of being one of the best companies to work for and to work with. Learn more at https://www.logic2020.com/company/our-values.\n\nLogic20/20 Benefits\n\nWhy Logic20/20? It\u2019s our goal to be one of the best companies to work for. One piece of the puzzle is an evolving set of benefits that extend past medical, dental, and 401(k).\n\nYou will have\n\u2022 Career Development \u2013 A built-in program from day 1, providing a mentor and individually-directed training opportunities, plus access to leaders across the company\n\u2022 PTO, Paid Holidays, & Voluntary Leave \u2013 Worry-free time off to recharge and pursue your personal goals\n\u2022 Community & Committees \u2013 As part of our \u201cCulture of We,\u201d Logic20/20 invests in providing many social, interest, and learning opportunities\n\u2022 Recognition \u2013 From peer recognition, swag, and the chance to win a once-in-a-lifetime type of award, we make your Logic20/20 journey stand out\n\u2022 Referral Programs & Bonuses \u2013 Employee, project, and sales referral programs with paid incentives\n\nEqual Opportunity Statement\n\nWe believe that people should be celebrated: for their talents, ideas, and skills, but most of all, for what makes them unique. We prohibit harassment and/or discrimination based on age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status, or any other basis as protected by federal, state, or local law.\n\nTo learn more about our DE&I initiatives, please visit: https://www.logic2020.com/company/diversity-equity-inclusion\n\nPrivacy Policy\n\nDuring the recruitment and hiring process, we gather, process, and store some of your personal data. We consider data privacy a priority. For further information, please view our company privacy policy.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690329600,
            "job_posted_at_datetime_utc": "2023-07-26T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "paid_time_off",
                "dental_coverage",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=U3j2JDYImI0AAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-25T22:19:11.000Z",
            "job_offer_expiration_timestamp": 1693001951,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "5+ years of cloud data engineering experience",
                    "Strong experience designing and developing cloud ELT and data pipelines with AWS, Python, SQL, Athena",
                    "Must also have experience with the following technology: CLI, Glue, RDS, Pandas, KMS, S3",
                    "Experience with developing and operating CI/CD pipelines and other DataOps fundamentals",
                    "Strong understanding of data modeling, data pipelines, data lakes",
                    "Excellent foundation of consulting skills: analytical, written and verbal communication, and presentation skills",
                    "Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule\u202f",
                    "Bachelor's degree in Computer Science, Engineering, or a related field"
                ],
                "Responsibilities": [
                    "You will improve risk modeling that will allow the client to better understand the integrity of their pipelines for improved customer safety"
                ],
                "Benefits": [
                    "Compensation range for senior data engineer: $125,000 - $175,000 annually",
                    "Compensation range for lead data engineer: $145,000 - $195,000 annually",
                    "Career Development \u2013 A built-in program from day 1, providing a mentor and individually-directed training opportunities, plus access to leaders across the company",
                    "PTO, Paid Holidays, & Voluntary Leave \u2013 Worry-free time off to recharge and pursue your personal goals",
                    "Community & Committees \u2013 As part of our \u201cCulture of We,\u201d Logic20/20 invests in providing many social, interest, and learning opportunities",
                    "Recognition \u2013 From peer recognition, swag, and the chance to win a once-in-a-lifetime type of award, we make your Logic20/20 journey stand out",
                    "Referral Programs & Bonuses \u2013 Employee, project, and sales referral programs with paid incentives"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Seamless.AI",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "WAVY Jobs",
            "job_id": "ggGlVmkBy6AAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Vice President of Data Engineering",
            "job_apply_link": "https://jobs.wavy.com/jobs/vice-president-of-data-engineering-dallas-texas/1073551209-2/",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5612,
            "job_description": "The Opportunity\n\nWe are seeking a highly experienced and talented Vice President of Data Engineering to join our fast-growing SaaS company. The ideal candidate will have a proven track record in building robust data pipelines and processes, along with a strong background in AWS, Python, and data consolidation techniques. Reporting directly to the CTO, this strategic role requires excellent communication and organization skills, as well as the ability to lead and mentor a team. The Vice President of Data Engineering will play a crucial role in driving our companys growth and scalability by managing budgets, reporting on ROI, and analyzing team and individual metrics.\n\nRole Responsibilities\n\u2022 Lead the design, development, implementation, and maintenance of scalable data pipelines and processes to support our rapidly growing SaaS platform.\n\u2022 Oversee the consolidation, validation, and transformation of data from various sources into a unified and reliable format.\n\u2022 Collaborate closely with cross-functional teams to understand business requirements and provide data solutions that meet their needs.\n\u2022 Manage and mentor a team of data engineers, fostering a collaborative and high-performance work environment.\n\u2022 Develop and enforce best practices for data engineering, including coding standards, documentation, and quality assurance processes.\n\u2022 Stay up-to-date with industry trends, emerging technologies, and advancements in data engineering techniques, and assess their potential impact on our business.\n\u2022 Effectively communicate complex technical concepts to non-technical stakeholders, including executives and other department heads.\n\u2022 Take ownership of the data engineering budget, including resource allocation, vendor management, and cost optimization.\n\u2022 Analyze and report on the ROI of data engineering initiatives, identifying areas for improvement and recommending actionable strategies.\n\u2022 Track team and individual performance metrics, providing constructive feedback and implementing performance improvement plans as necessary.\n\nCandidate Requirements\n\u2022 Bachelors or masters degree in computer science, engineering, or a related field.\n\u2022 Proven experience in a similar role, preferably in a fast-growing SaaS company.\n\u2022 Strong expertise in building scalable data pipelines using AWS services such as S3, Glue, Redshift, and Lambda.\n\u2022 Proficiency in Python programming and related frameworks/libraries for data engineering tasks.\n\u2022 Solid understanding of data consolidation and validation techniques, including data cleansing, deduplication, and data integrity checks.\n\u2022 Excellent communication and interpersonal skills, with the ability to collaborate effectively with both technical and non-technical stakeholders.\n\u2022 Strong organizational skills and the ability to manage multiple priorities in a fast-paced environment.\n\u2022 Demonstrated leadership experience, with the ability to mentor and develop a high-performing team.\n\u2022 Analytical mindset with a focus on data-driven decision-making.\n\u2022 Experience managing budgets and reporting on financial and performance metrics.\n\u2022 Strong business acumen and the ability to align data engineering strategies with company goals and objectives.\n\nSeamless.AI has been delivering the worlds best sales leads since 2015. Our product is the first real time, B2B search engine helping sales teams maximize revenue, increase sales, and easily acquire their total addressable market using artificial intelligence. We have been recognized as one of Ohio's fastest growing companies and won 2020 Best Places to Work and LinkedIn's Top 50 Tech Startups in 2020 and 2022. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Visa Sponsorship is not included in our hiring package. Applicants will need to be authorized to work in the U.S.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690295159,
            "job_posted_at_datetime_utc": "2023-07-25T14:25:59.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ggGlVmkBy6AAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": null,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "The ideal candidate will have a proven track record in building robust data pipelines and processes, along with a strong background in AWS, Python, and data consolidation techniques",
                    "Bachelors or masters degree in computer science, engineering, or a related field",
                    "Proven experience in a similar role, preferably in a fast-growing SaaS company",
                    "Strong expertise in building scalable data pipelines using AWS services such as S3, Glue, Redshift, and Lambda",
                    "Proficiency in Python programming and related frameworks/libraries for data engineering tasks",
                    "Solid understanding of data consolidation and validation techniques, including data cleansing, deduplication, and data integrity checks",
                    "Excellent communication and interpersonal skills, with the ability to collaborate effectively with both technical and non-technical stakeholders",
                    "Strong organizational skills and the ability to manage multiple priorities in a fast-paced environment",
                    "Demonstrated leadership experience, with the ability to mentor and develop a high-performing team",
                    "Analytical mindset with a focus on data-driven decision-making",
                    "Experience managing budgets and reporting on financial and performance metrics",
                    "Strong business acumen and the ability to align data engineering strategies with company goals and objectives"
                ],
                "Responsibilities": [
                    "Reporting directly to the CTO, this strategic role requires excellent communication and organization skills, as well as the ability to lead and mentor a team",
                    "The Vice President of Data Engineering will play a crucial role in driving our companys growth and scalability by managing budgets, reporting on ROI, and analyzing team and individual metrics",
                    "Lead the design, development, implementation, and maintenance of scalable data pipelines and processes to support our rapidly growing SaaS platform",
                    "Oversee the consolidation, validation, and transformation of data from various sources into a unified and reliable format",
                    "Collaborate closely with cross-functional teams to understand business requirements and provide data solutions that meet their needs",
                    "Manage and mentor a team of data engineers, fostering a collaborative and high-performance work environment",
                    "Develop and enforce best practices for data engineering, including coding standards, documentation, and quality assurance processes",
                    "Stay up-to-date with industry trends, emerging technologies, and advancements in data engineering techniques, and assess their potential impact on our business",
                    "Effectively communicate complex technical concepts to non-technical stakeholders, including executives and other department heads",
                    "Take ownership of the data engineering budget, including resource allocation, vendor management, and cost optimization",
                    "Analyze and report on the ROI of data engineering initiatives, identifying areas for improvement and recommending actionable strategies",
                    "Track team and individual performance metrics, providing constructive feedback and implementing performance improvement plans as necessary"
                ]
            },
            "job_job_title": "Data engineering",
            "job_posting_language": "en",
            "job_onet_soc": "11302100",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Parkland Health and Hospital System",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "ZipRecruiter",
            "job_id": "gEFo3LqOIkEAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer - PCHP",
            "job_apply_link": "https://www.ziprecruiter.com/c/Parkland-Health-and-Hospital-System/Job/Data-Engineer-PCHP/-in-Dallas,TX?jid=8029868922761f0f",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.7163,
            "job_description": "Interested in a career with both meaning and growth? Whether your abilities are in direct patient care or one of the many other areas of healthcare administration and support, everyone at Parkland works together to fulfill our mission: the health and well-being of individuals and communities entrusted to our care. By joining Parkland, you become part of a diverse healthcare legacy that's served our community for more than 125 years. Put your skills to work with us, seek opportunities to learn and join a talented team where patient care is more than a job. It's our passion.\n\nPrimary Purpose\n\nThe Parkland Community Health Plan's (PCHP's) Data Engineer is responsible for maintaining the data systems including business intelligence, ETL, and supporting backup strategies in order to provide PCHP with secure, dependable, and accurate data including data transfer, data integrity, and data storage responsibilities. The Data Engineer will collaborate with Database Administrators, server team, storage team, and other teams to plan maintenance activities and with PHCP's analytics team for report or universe deployments. The Data Engineer will also be involved in dashboard and report development activities.\n\nMinimum Specifications\n\nEducation\n\u2022 Bachelor's degree in computer science, management information systems, information technology, statistics, mathematics, or related discipline.\n\nExperience\n\u2022 Seven years of experience in maintaining business intelligence, data warehouse solutions, or ETL in a Run or Production environment.\n\u2022 Six years of experience troubleshooting ETL load related issues (SSIS or Data Solutions).\n\u2022 Six years of experience with ETL development and maintenance experience in a data warehouse environment.\n\u2022 Experience with systems engineering (hardware / software) capacity.\n\u2022 Experience with database or report portal tool administration is preferred.\n\u2022 Experience at a healthcare or managed care organization is preferred.\n\nEquivalent Education and/or Experience\n\u2022 Five (5) years of experience with healthcare data management in a health plan or managed care organization may be considered in lieu of a degree.\n\nCertification/Registration/Licensure\n\u2022 System Administration or Reporting Tool Administrative Certification is preferred. (i.e. Epic Cogito or Clarity, SAP Business Objects, Tibco Composite, Microsoft Certified Solutions Engineer (MCSE), Oracle Certified Professional (OCP), etc.)\n\u2022 PMP or other project management certificate or training is preferred.\n\nSkills or Special Abilities\n\u2022 Proficiency with ETL tool Build or Run activities.\n\u2022 Ability to create reports and/or build virtual data environments.\n\u2022 Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\n\u2022 Proficiency with Microsoft Office Excel, Word and Outlook is required; Access and PowerPoint are preferred.\n\u2022 Demonstrated critical thinking and troubleshooting skills accompanied by a high level of detail.\n\u2022 Demonstrated ability to plan and manage multiple processes and projects simultaneously.\n\u2022 High level of attention to detail.\n\u2022 Strong verbal and written communication skills.\n\u2022 Demonstrated ability to collaborate effectively and work as part of a team.\n\u2022 Independent worker and self-starter, having the ability to provide internal motivation and drive.\n\u2022 Proficiency with server or application patching, backups, scripting is preferred.\n\u2022 Understanding of SSIS and Apache NiFi is preferred.\n\u2022 Proficiency with Business Objects Administration is preferred.\n\nResponsibilities\n\u2022 Implements and maintains high-value business intelligence environments.\n\u2022 Maintains the data systems including business intelligence, ETL, and supporting backup strategies.\n\u2022 Has a strong understanding of all the tools within the environment, regardless of vendor, and quickly and efficiently triages, troubleshoots, and restores services during outages or service degradation.\n\u2022 Responsible for being on-call for Business Intelligence and ETL cycles.\n\u2022 Works with the Database Analyst and storage teams to ensure proper backups are taken, test back-ups periodically, and ensures that the system can be restored in the time of a disaster.\n\u2022 Proactively identifies areas for improvement in our Business Intelligence environment.\n\u2022 Documents all routine processes and cross-trains other team members.\n\u2022 Improves function, speed, and accuracy of data distribution methods.\n\u2022 Develops automated reports and dashboards.\n\nJob Accountabilities\n\u2022 Identifies ways to improve work processes and improve customer satisfaction. Makes recommendations to supervisor, implements, and monitors results as appropriate in support of the overall goals of PCHP.\n\u2022 Stays abreast of the latest developments, advancements, and trends in the field by attending seminars/workshops, reading professional journals, actively participating in professional organizations, and/or maintaining certification or licensure. Integrates knowledge gained into current work practices.\n\u2022 Maintains knowledge of applicable rules, regulations, policies, laws, and guidelines that impact the area. Develops effective internal controls designed to promote adherence with applicable laws, accreditation agency requirements, and customer requirements. Seeks advice and guidance as needed to ensure proper understanding.\n\nParkland Health and Hospital System prohibits discrimination based on age (40 or over), race, color, religion, sex (including pregnancy), sexual orientation, gender identity, gender expression, genetic information, disability, national origin, marital status, political belief, or veteran status. As part of our commitment to our patients and employees' wellness, Parkland Health is a tobacco and smoke-free campus.",
            "job_is_remote": true,
            "job_posted_at_timestamp": 1689724800,
            "job_posted_at_datetime_utc": "2023-07-19T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=gEFo3LqOIkEAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-26T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1693008000,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 84,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": true
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's degree in computer science, management information systems, information technology, statistics, mathematics, or related discipline",
                    "Seven years of experience in maintaining business intelligence, data warehouse solutions, or ETL in a Run or Production environment",
                    "Six years of experience troubleshooting ETL load related issues (SSIS or Data Solutions)",
                    "Six years of experience with ETL development and maintenance experience in a data warehouse environment",
                    "Experience with systems engineering (hardware / software) capacity",
                    "Equivalent Education and/or Experience",
                    "Five (5) years of experience with healthcare data management in a health plan or managed care organization may be considered in lieu of a degree",
                    "Epic Cogito or Clarity, SAP Business Objects, Tibco Composite, Microsoft Certified Solutions Engineer (MCSE), Oracle Certified Professional (OCP), etc.)",
                    "Proficiency with ETL tool Build or Run activities",
                    "Ability to create reports and/or build virtual data environments",
                    "Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy",
                    "Demonstrated critical thinking and troubleshooting skills accompanied by a high level of detail",
                    "Demonstrated ability to plan and manage multiple processes and projects simultaneously",
                    "High level of attention to detail",
                    "Strong verbal and written communication skills",
                    "Demonstrated ability to collaborate effectively and work as part of a team",
                    "Independent worker and self-starter, having the ability to provide internal motivation and drive"
                ],
                "Responsibilities": [
                    "The Parkland Community Health Plan's (PCHP's) Data Engineer is responsible for maintaining the data systems including business intelligence, ETL, and supporting backup strategies in order to provide PCHP with secure, dependable, and accurate data including data transfer, data integrity, and data storage responsibilities",
                    "The Data Engineer will collaborate with Database Administrators, server team, storage team, and other teams to plan maintenance activities and with PHCP's analytics team for report or universe deployments",
                    "The Data Engineer will also be involved in dashboard and report development activities",
                    "Implements and maintains high-value business intelligence environments",
                    "Has a strong understanding of all the tools within the environment, regardless of vendor, and quickly and efficiently triages, troubleshoots, and restores services during outages or service degradation",
                    "Responsible for being on-call for Business Intelligence and ETL cycles",
                    "Works with the Database Analyst and storage teams to ensure proper backups are taken, test back-ups periodically, and ensures that the system can be restored in the time of a disaster",
                    "Proactively identifies areas for improvement in our Business Intelligence environment",
                    "Documents all routine processes and cross-trains other team members",
                    "Improves function, speed, and accuracy of data distribution methods",
                    "Develops automated reports and dashboards",
                    "Identifies ways to improve work processes and improve customer satisfaction",
                    "Makes recommendations to supervisor, implements, and monitors results as appropriate in support of the overall goals of PCHP",
                    "Stays abreast of the latest developments, advancements, and trends in the field by attending seminars/workshops, reading professional journals, actively participating in professional organizations, and/or maintaining certification or licensure",
                    "Maintains knowledge of applicable rules, regulations, policies, laws, and guidelines that impact the area",
                    "Develops effective internal controls designed to promote adherence with applicable laws, accreditation agency requirements, and customer requirements",
                    "Seeks advice and guidance as needed to ensure proper understanding"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-2041.00: Statisticians"
            ]
        },
        {
            "employer_name": "InnoCore Solutions, Inc.",
            "employer_logo": "https://media.licdn.com/dms/image/C4E0BAQFf1XwUvZbQ-g/company-logo_200_200/0/1548364987002?e=2147483647&v=beta&t=t_toT-cPqBtXgu6AeDd8iCaKvZwqKgEFADBR8TlzNZE",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Dice.com",
            "job_id": "VEeZH6OmNmcAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Sr. ETL Migration Data Engineer",
            "job_apply_link": "https://www.dice.com/job-detail/e4fea931-6de0-41bf-8fd6-df33a15a3e9a",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.6635,
            "job_description": "Responsibilities:\n\u2022 Responsible for migrating an on-premises ETL(IBM DataStage) to Azure Databricks\n\u2022 Implement data privacy and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools\n\u2022 Perform multiple aspects involved in the development lifecycle design, cloud engineering (infrastructure, network, security, and administration)\n\u2022 Implement batch and streaming data pipelines using cloud technologies\n\u2022 Develop and support data privacy and governance related frameworks and help other teams implement them for compliance\n\nRequirements:\n\u2022 8 years of software solution development using Agile, and DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions\n\u2022 2 years of cloud development and data lake experience including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps, and Power BI\n\u2022 5 years of experience with Python, PySpark, Spark, Unix, SQL\n\u2022 Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake\n\u2022 Tools: DataStage, Informatica EDC/Axon, BigID\n\u2022 Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers\n\nEducation:\n\u2022 Bachelor's degree in Computer Science/Engineering or related field",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1688390886,
            "job_posted_at_datetime_utc": "2023-07-03T13:28:06.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=VEeZH6OmNmcAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-27T15:29:20.000Z",
            "job_offer_expiration_timestamp": 1693150160,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Azure EventHub",
                "Azure Data Factory",
                "Azure Databricks",
                "Azure DevOps",
                "Azure Blob Storage",
                "Azure Data Lake",
                "Azure Power Apps",
                "Power BI",
                "SQL",
                "DataStage"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "8 years of software solution development using Agile, and DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions",
                    "2 years of cloud development and data lake experience including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps, and Power BI",
                    "5 years of experience with Python, PySpark, Spark, Unix, SQL",
                    "Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake",
                    "Tools: DataStage, Informatica EDC/Axon, BigID",
                    "Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers",
                    "Bachelor's degree in Computer Science/Engineering or related field"
                ],
                "Responsibilities": [
                    "Responsible for migrating an on-premises ETL(IBM DataStage) to Azure Databricks",
                    "Implement data privacy and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools",
                    "Perform multiple aspects involved in the development lifecycle design, cloud engineering (infrastructure, network, security, and administration)",
                    "Implement batch and streaming data pipelines using cloud technologies",
                    "Develop and support data privacy and governance related frameworks and help other teams implement them for compliance"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113300",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Sogeti",
            "employer_logo": "https://www.sogeti.be/Static/img/sogeti-logo.svg",
            "employer_website": "http://www.sogeti.com",
            "employer_company_type": "Consulting",
            "job_publisher": "Built In",
            "job_id": "yMUxVNy49foAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Remote Data Engineer (Dallas, TX)",
            "job_apply_link": "https://builtin.com/job/data/remote-data-engineer/162680",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6028,
            "job_description": "Sogeti is a leading provider of professional technology services, specializing in Application Management, Infrastructure Management and High-Tech Engineering. Sogeti offers cutting-edge solutions around Testing, Business Intelligence, Mobility, Cloud and Security, combining world class methodologies and the global delivery model, Rightshore\u00ae. Sogeti brings together more than 20,000 professionals in 15 countries and is present in over 100 locations in Europe, the US and India. Sogeti is a wholly-owned subsidiary of Cap Gemini S.A., listed on the Paris Stock Exchange.\n\nAt Sogeti USA, we are committed to building a long and enduring relationship with our employees and to creating an environment that rewards and empowers. Our mission is to constantly exceed our employees' expectations in the same way that we strive to exceed our clients' expectations. We offer an environment that celebrates innovation and helps you to achieve a good balance between your professional and personal life. We strive to be an employer of choice!\n\nWhat You'll Do\n\u2022 Build well-managed data solutions, tools, and capabilities to enable self-service frameworks for data consumers.\n\u2022 Understand, facilitate, and influence domain-driven design processes.\n\u2022 Recommend improvements to data architectures to maximize business capabilities.\n\u2022 Build data pipelines andservices for AWS, Azure, Kafka.\n\u2022 Develop deploymentpackages and automate testing (Jenkins, Git).\n\u2022 Work collaboratively with others within and outside of the engineering function to achieve goals, improve processes, and optimize system design (Agile, Scrum team).\nMinimum education qualification\n\u2022 Bachelor's Degree in Computer Science, Computer Engineering, MIS or related field.\nThe ideal candidate will have:\n\u2022 Bachelor of Science in Computer Science or technical equivalent; Master's preferred\n\u2022 Knowledge of industry leading data management practices\n\u2022 Knowledge of cloudplatforms such as Azure, AWS\n\u2022 5+ years using modern BItools (Power BI, Tableau, Qlik, MicroStrategy)\n\u2022 Experience with ETL/ELTtools (Informatica, Talend)\n\u2022 5 years of experiencewith modern, various databases & data store tools (Snowflake, AWS Redshift,RDS, DocumentDB, S3)\n\u2022 Strong experience withApache Kafka, Kubernetes, Terraform\n\u2022 Experience with corecompetencies in Data Structures, Rest/SOAP APIs, JSON.\n\u2022 High degree of initiative and capacity to lead multiple priorities of significant scope in a fast-paced environment\n\u2022 Knowledge of various data architecture and technology solutions\n\u2022 Expert level in SQL\n\u2022 Experience with commonsoftware engineering tools (e.g., Git, JIRA, Confluence)\n\u2022 Strong experience withdata modeling, data lineage, data warehousing and data marts\n\u2022 Additional skillsinclude: Matillion, Azure Data Factory, Azure Synapse, AWS Lambda, S3, Glue,EMR, Redshift, Dynamo DB, Aurora, Athena, and Hadoop\n\nThe benefits our employees enjoy:\n\u2022 401(k) Savings Plan- Matched 150% up to 6%. (Our 401k is in the top 1% of 401(k) plans offered in the US!)\n\u2022 Medical/Prescription/Dental/Vision Coverage!\n\u2022 Low-premium and deductible. Plan with free preventive care.\n\u2022 $12,000 in Tuition Reimbursement\n\u2022 100% Company-paid mobile phone plan\n\u2022 Personal Time Off (PTO)- Ensuring a balance of work and home life",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1629846938,
            "job_posted_at_datetime_utc": "2021-08-24T23:15:38.000Z",
            "job_city": null,
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.707874,
            "job_longitude": -96.92091,
            "job_benefits": [
                "retirement_savings",
                "dental_coverage",
                "health_insurance",
                "paid_time_off"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=yMUxVNy49foAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": true,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's Degree in Computer Science, Computer Engineering, MIS or related field",
                    "Knowledge of industry leading data management practices",
                    "Knowledge of cloudplatforms such as Azure, AWS",
                    "5+ years using modern BItools (Power BI, Tableau, Qlik, MicroStrategy)",
                    "Experience with ETL/ELTtools (Informatica, Talend)",
                    "5 years of experiencewith modern, various databases & data store tools (Snowflake, AWS Redshift,RDS, DocumentDB, S3)",
                    "Apache Kafka, Kubernetes, Terraform",
                    "Experience with corecompetencies in Data Structures, Rest/SOAP APIs, JSON",
                    "High degree of initiative and capacity to lead multiple priorities of significant scope in a fast-paced environment",
                    "Knowledge of various data architecture and technology solutions",
                    "Expert level in SQL",
                    "Experience with commonsoftware engineering tools (e.g., Git, JIRA, Confluence)",
                    "Strong experience withdata modeling, data lineage, data warehousing and data marts"
                ],
                "Responsibilities": [
                    "Build well-managed data solutions, tools, and capabilities to enable self-service frameworks for data consumers",
                    "Understand, facilitate, and influence domain-driven design processes",
                    "Recommend improvements to data architectures to maximize business capabilities",
                    "Build data pipelines andservices for AWS, Azure, Kafka",
                    "Develop deploymentpackages and automate testing (Jenkins, Git)",
                    "Work collaboratively with others within and outside of the engineering function to achieve goals, improve processes, and optimize system design (Agile, Scrum team)"
                ],
                "Benefits": [
                    "401(k) Savings Plan- Matched 150% up to 6%",
                    "(Our 401k is in the top 1% of 401(k) plans offered in the US!)",
                    "Medical/Prescription/Dental/Vision Coverage!",
                    "Low-premium and deductible",
                    "Plan with free preventive care",
                    "$12,000 in Tuition Reimbursement",
                    "100% Company-paid mobile phone plan",
                    "Personal Time Off (PTO)- Ensuring a balance of work and home life"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "541613",
            "job_naics_name": "Marketing Consulting Services"
        },
        {
            "employer_name": "RIT solutions Inc",
            "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/4/49/Rochester_Institute_of_Technology_Seal_%282018%29.svg",
            "employer_website": "http://www.rit.edu",
            "employer_company_type": "Education",
            "job_publisher": "Techfetch",
            "job_id": "248UdFpME3EAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Sr Data Engineer",
            "job_apply_link": "https://www.techfetch.com/job-description/sr-data-engineer-dallas-tx-j3576490",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4578,
            "job_description": "Duration: 12+ month\nVisa: GC/USC\nMust have a valid Linkedin profile...\nMust be based in Dallas or Raleigh- (travel on-site one week per quarter)\n- Oracle - experience writing SQL queries in Oracle\n- 10+ years in Database Development\n- Experience with Informatica or similar ETL tool\n- Hands-on experience with AWS\n- Experience with Python or similar scripting language",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1676937600,
            "job_posted_at_datetime_utc": "2023-02-21T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=248UdFpME3EAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2024-02-21T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1708473600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 120,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Oracle",
                "Data Engineer",
                "ETL",
                "Informatica",
                "Python",
                "SQL"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Must have a valid Linkedin profile..",
                    "Must be based in Dallas or Raleigh- (travel on-site one week per quarter)",
                    "10+ years in Database Development",
                    "Experience with Informatica or similar ETL tool",
                    "Hands-on experience with AWS",
                    "Experience with Python or similar scripting language"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_naics_code": "61",
            "job_naics_name": "Education"
        },
        {
            "employer_name": "Docyt",
            "employer_logo": "https://res.cloudinary.com/crunchbase-production/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/v1464377633/hffpio7eh8djgtpcteyg.png",
            "employer_website": "http://docyt.com",
            "employer_company_type": null,
            "job_publisher": "Glassdoor",
            "job_id": "LWxudsMKTTMAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Big Data Engineer",
            "job_apply_link": "https://www.glassdoor.com/job-listing/big-data-engineer-docyt-JV_IC1139977_KO0,17_KE18,23.htm?jl=1008768999974",
            "job_apply_is_direct": true,
            "job_apply_quality_score": 0.573,
            "job_description": "Docyt, a fast-growing FinTech startup based in Silicon Valley, is seeking a highly motivated Big Data Engineer to join our team. The ideal candidate will be responsible for maintaining our data processing infrastructure and optimizing our data architecture, as well as contributing to the development and implementation of new data-driven solutions. At Docyt, we are passionate about empowering businesses to take control of their financial data using an AI-driven super app, and we're looking for a skilled engineer to help us continue to innovate in this exciting space.\n\nResponsibilities\n\u2022 Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse\n\u2022 Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines\n\u2022 Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques\n\u2022 Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities\n\u2022 Build and maintain data models and ensure data accuracy and consistency\n\u2022 Implement and manage data security measures, including backups and access controls\n\u2022 Participate in code reviews, providing constructive feedback to ensure code quality\n\nRequirements\n\u2022 Bachelor's degree in Computer Science, Engineering, or related field\n\u2022 At least 3 years of experience in big data engineering or related field\n\u2022 Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling\n\u2022 Proficient in at least one programming language, such as Python or Java\n\u2022 Experience with SQL and NoSQL databases\n\u2022 Familiarity with distributed computing frameworks, such as Hadoop or Spark\n\u2022 Understanding of data security and access control best practices\n\u2022 Strong problem-solving skills and ability to work independently and in a team environment\n\u2022 Excellent verbal and written communication skills\n\u2022 Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field\n\u2022 Experience with AWS and Docker is a plus.\n\nBenefits\n\u2022 Great growth potential at a fast-growing startup, we want you to grow with us!\n\u2022 Company-provided laptop and necessary hardware to ensure your setup for success.\n\u2022 Comprehensive health, dental and vision coverage.\n\u2022 Company-sponsored 401(k)\n\u2022 Inclusive and motivating work culture that values team collaboration.\n\nAbout Us\n\nDocyt, pronounced \u201cdocket\u201d, is a FinTech startup headquartered in Silicon Valley, that is passionately focused on giving businesses control of their financial data. While great strides have been made in sending and receiving payments, businesses still struggle to aggregate all their financial data, understand it, and use it to make well-informed, timely decisions. Docyt brings order to data chaos.\n\nDocyt is a super app that applies AI (artificial intelligence) across the entire accounting tech stack. Docyt digitizes financial data, automates both income and expense workflows, continuously reconciles QuickBooks\u00ae, and generates real-time financial statements. That explains what we do, but here\u2019s why it\u2019s important. A complete, accurate, real-time financial picture empowers businesses to make timely and smart decisions so their business can thrive.",
            "job_is_remote": true,
            "job_posted_at_timestamp": 1689811200,
            "job_posted_at_datetime_utc": "2023-07-20T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "dental_coverage",
                "retirement_savings",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=LWxudsMKTTMAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's degree in Computer Science, Engineering, or related field",
                    "At least 3 years of experience in big data engineering or related field",
                    "Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling",
                    "Proficient in at least one programming language, such as Python or Java",
                    "Experience with SQL and NoSQL databases",
                    "Familiarity with distributed computing frameworks, such as Hadoop or Spark",
                    "Understanding of data security and access control best practices",
                    "Strong problem-solving skills and ability to work independently and in a team environment",
                    "Excellent verbal and written communication skills",
                    "Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field"
                ],
                "Responsibilities": [
                    "Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse",
                    "Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines",
                    "Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques",
                    "Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities",
                    "Build and maintain data models and ensure data accuracy and consistency",
                    "Implement and manage data security measures, including backups and access controls",
                    "Participate in code reviews, providing constructive feedback to ensure code quality"
                ],
                "Benefits": [
                    "Great growth potential at a fast-growing startup, we want you to grow with us!",
                    "Company-provided laptop and necessary hardware to ensure your setup for success",
                    "Comprehensive health, dental and vision coverage",
                    "Company-sponsored 401(k)",
                    "Inclusive and motivating work culture that values team collaboration"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "ClifyX, INC",
            "employer_logo": null,
            "employer_website": "http://www.clifyx.com",
            "employer_company_type": null,
            "job_publisher": "Jobrapido.com",
            "job_id": "Z3RRrMfrr9IAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://us.jobrapido.com/jobpreview/2926831096",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.4464,
            "job_description": "Job Description\n1. (Data Engineer) - 1 open position\nJob Requirements: ETL/ AWS Glue\nRates : $85/hour\nLocation: Dallas, TX (Onsite)\nFulltime (Permanent) /Contract (SC)\nResponsibilities\n\u20225 years of experience in support, development, design, and implementation of technology solutions on large Data engineering initiatives - preferably in Financial Services\n\u2022Expertise in batch and file-based integrations Experience of performing ETL ELT processes from multiple sources\n\u2022Hands-on experience of working in AWS cloud environment and using AWS Glue for data integrations\n\u2022Experience of integrating with Snowflake eco-system and using of tools such as SnowSQL, SnowPipe etc\n\u2022Demonstrated application of skills and knowledge across digital technologies (AWS, Cloud, Data, App Architectures (APIs, Event Streaming), Automation) and with Agile delivery methods (Scrum, Kanban, DevOps)\n\u2022Strong Database and SQL Sk ills Required\n\u2022Strong Object-oriented programming Skills Required\n\u2022Strong knowledge in backend frameworks like Spring boot is Required\n\u2022Experience with some backend languages (Java, Python) Experience Required\n\u2022Experience with DevSecOps tools for CI CD Required\n\u2022Experience with Source Code management tools like Git Required\n\u2022Excellent communication skills Required\n\u2022Experience with using Message Queue and Kafka Preferred\n\u2022Experience with SRE Concepts & Tools Preferred\n\u2022Working in banking domain (Retail commercial) is Preferred",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1686787200,
            "job_posted_at_datetime_utc": "2023-06-15T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Z3RRrMfrr9IAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Expertise in batch and file-based integrations Experience of performing ETL ELT processes from multiple sources",
                    "Demonstrated application of skills and knowledge across digital technologies (AWS, Cloud, Data, App Architectures (APIs, Event Streaming), Automation) and with Agile delivery methods (Scrum, Kanban, DevOps)",
                    "Strong Database and SQL Sk ills Required",
                    "Strong Object-oriented programming Skills Required",
                    "Strong knowledge in backend frameworks like Spring boot is Required",
                    "Experience with some backend languages (Java, Python) Experience Required",
                    "Experience with DevSecOps tools for CI CD Required",
                    "Experience with Source Code management tools like Git Required",
                    "Excellent communication skills Required"
                ],
                "Responsibilities": [
                    "(Data Engineer) - 1 open position"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Advaana Staffing",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTPd-hps7yHu6ZmQPwa8QzKacpqKLbzwCNNoBVY&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "OPTnation",
            "job_id": "0OPIQUWCXxMAAAAAAAAAAA==",
            "job_employment_type": "CONTRACTOR",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.optnation.com/data-engineer-job-in-dallas-tx-view-jobid-28150",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.6919,
            "job_description": "Position Data Engineer\n\nLocation Dallas TX & NY.\n\nDuration 12+ Months\n\nMust Have Firmwide x 2 OR Data x 3\n\nJob Description Design and develop data ingest and transform processes Develop data models to provide standardized reporting solutions to the firm Develop automation governance and reporting solutions to provide firm and regulatory mandated controls Work as part of a global team using Agile software methodologies Partner with Marcus risk product acquisition and servicing teams Use Marcus data to drive change throughout the Marcus business Minimum 3 years of relevant professional experience Experience with SQL and relational databases Self-starter motivated and good communication skills Strong sense of ownership and driven to manage tasks to completion\n\nThanks & Regards\n\n--\n\nSandhya Sales Recruiter Direct +1 973 259 6437 Sandhya@advaana.com advaana.com\n\nAdvaana Inc. 12 Daniel Rd STE 314 Fairfield NJ 07004",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1677196800,
            "job_posted_at_datetime_utc": "2023-02-24T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=0OPIQUWCXxMAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": [
                "Data Engineer"
            ],
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": 50000,
            "job_max_salary": 140000,
            "job_salary_currency": "USD",
            "job_salary_period": "YEAR",
            "job_highlights": {
                "Qualifications": [
                    "Must Have Firmwide x 2 OR Data x 3",
                    "Job Description Design and develop data ingest and transform processes Develop data models to provide standardized reporting solutions to the firm Develop automation governance and reporting solutions to provide firm and regulatory mandated controls Work as part of a global team using Agile software methodologies Partner with Marcus risk product acquisition and servicing teams Use Marcus data to drive change throughout the Marcus business Minimum 3 years of relevant professional experience Experience with SQL and relational databases Self-starter motivated and good communication skills Strong sense of ownership and driven to manage tasks to completion"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "NFI",
            "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSqLoFpJateFKTAzYd2qBNoTh34L-AdTefNEqjD&s=0",
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "BeBee",
            "job_id": "VxiGjPFk0QAAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer II",
            "job_apply_link": "https://us.bebee.com/job/20230723-1100a4a283b734c77bfe515ca2401cf7",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5,
            "job_description": "At NFI, we offer innovative, integrated, and customized solutions that span the entire supply chain. Whatever our customer's challenge is, we have the knowledge, technology, scale, and commitment to help them solve it. It's simply what we do.\n\nThe NFI Data and Analytics team is looking for an enthusiastic Data Engineer to join our dynamic and growing team. As a Data Engineer, you will be responsible for developing and maintaining the data architecture and infrastructure that support our data-driven applications and processes. You will work closely with our data engineering team, IT Ops team, and business partners to ensure that data is appropriately collected, stored, processed, and analyzed. Guided by NFI's shared values, we work in an environment where collaboration, teamwork, respect, and openness are highly valued.\n\nEssential Duties & Responsibilities:\n\u2022 Develop and maintain scalable and efficient data pipelines, data warehouses, and databases.\n\u2022 Build large-scale batch and real-time data pipelines using state-of-the-art cloud technologies, such as Google Cloud, Azure, or AWS.\n\u2022 Build and manage data infrastructure, including data ingestion, processing, and storage.\n\u2022 Collaborate with cross-functional teams to ensure the quality and integrity of the data.\n\u2022 Implement data governance, security, and privacy measures.\n\u2022 Continuously optimize the data infrastructure to improve performance, reliability, and scalability\n\u2022 Conduct data modeling, profiling, and validation to ensure data accuracy and consistency.\n\u2022 Develop and maintain documentation on the data infrastructure, including data flow diagrams, architecture, and technical specifications.\n\u2022 Participate in code reviews and collaborate with other data engineers to maintain code quality and best practices.\n\nJob-Specific Requirements:\n\u2022 Bachelor's Degree in Computer Science, Information Systems, or a related discipline.\n\u2022 At least 3 years of experience as a Data Engineer or related role\n\u2022 Strong experience with database environments and (T) SQL.\n\u2022 Strong programming skills in languages such as Python, PySpark, and/or Java.\n\u2022 Experience with ETL tools and processes, such as SSIS or Azure Data Factory\n\u2022 Proficiency in data warehousing and data modeling\n\u2022 Solid understanding of data management principles, data quality, and data governance\n\u2022 Strong problem-solving skills and attention to detail\n\u2022 Experience with cloud computing platforms such as Google Cloud Platform, Azure, or AWS is a strong plus.\n\nExpected Competencies:\n\nFunctional Expertise: Possesses the skills and knowledge to perform essential functions efficiently and effectively.\n\u2022 Communication and Collaboration: Communicates openly and honestly. Follows through on commitments. Takes ownership and does not misrepresent information. Supports colleagues and team efforts.\n\u2022 Development: Takes an active role in self-development, seeking to grow job-related knowledge and skills. Empowers and challenges team members to reach their full potential.\n\u2022 Analysis and Decision Making: Uses all available resources to make good decisions. Knows when and how to partner with others when facing a problem.\n\u2022 Results Focus: Action-oriented. Assumes an appropriate level of accountability for goals, critical issues, and performance.\n\u2022 Managing Change and Continuous Improvement: Demonstrates an entrepreneurial mindset towards change. Takes risks, and creates new, and better ways for the organization to be successful.\n\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities\n\nThe contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1690073488,
            "job_posted_at_datetime_utc": "2023-07-23T00:51:28.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": null,
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=VxiGjPFk0QAAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-07-31T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1690761600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": true
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": true,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Bachelor's Degree in Computer Science, Information Systems, or a related discipline",
                    "At least 3 years of experience as a Data Engineer or related role",
                    "Strong experience with database environments and (T) SQL",
                    "Strong programming skills in languages such as Python, PySpark, and/or Java",
                    "Experience with ETL tools and processes, such as SSIS or Azure Data Factory",
                    "Proficiency in data warehousing and data modeling",
                    "Solid understanding of data management principles, data quality, and data governance",
                    "Strong problem-solving skills and attention to detail",
                    "Experience with cloud computing platforms such as Google Cloud Platform, Azure, or AWS is a strong plus",
                    "Functional Expertise: Possesses the skills and knowledge to perform essential functions efficiently and effectively"
                ],
                "Responsibilities": [
                    "As a Data Engineer, you will be responsible for developing and maintaining the data architecture and infrastructure that support our data-driven applications and processes",
                    "You will work closely with our data engineering team, IT Ops team, and business partners to ensure that data is appropriately collected, stored, processed, and analyzed",
                    "Develop and maintain scalable and efficient data pipelines, data warehouses, and databases",
                    "Build large-scale batch and real-time data pipelines using state-of-the-art cloud technologies, such as Google Cloud, Azure, or AWS",
                    "Build and manage data infrastructure, including data ingestion, processing, and storage",
                    "Collaborate with cross-functional teams to ensure the quality and integrity of the data",
                    "Implement data governance, security, and privacy measures",
                    "Continuously optimize the data infrastructure to improve performance, reliability, and scalability",
                    "Conduct data modeling, profiling, and validation to ensure data accuracy and consistency",
                    "Develop and maintain documentation on the data infrastructure, including data flow diagrams, architecture, and technical specifications",
                    "Participate in code reviews and collaborate with other data engineers to maintain code quality and best practices",
                    "Takes ownership and does not misrepresent information",
                    "Supports colleagues and team efforts",
                    "Development: Takes an active role in self-development, seeking to grow job-related knowledge and skills",
                    "Empowers and challenges team members to reach their full potential",
                    "Analysis and Decision Making: Uses all available resources to make good decisions",
                    "Managing Change and Continuous Improvement: Demonstrates an entrepreneurial mindset towards change"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        },
        {
            "employer_name": "Greystar Worldwide, LLC",
            "employer_logo": "https://mma.prnewswire.com/media/1761396/greystar_Logo.jpg?p=facebook",
            "employer_website": "https://www.greystar.com",
            "employer_company_type": "Real Estate",
            "job_publisher": "CareerBuilder",
            "job_id": "82l-Phq5MLUAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://www.careerbuilder.com/job/J3Q6GK6SQ68MDBSDJ23",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5483,
            "job_description": "At Greystar, we've launched a program aimed at bringing the real estate leasing experience for residents into the digital era.\n\nAs a data engineer, you will join the global Enterprise Data Organization and Apex Organization, which builds a resident-centric ecosystem of products that enable a 360 view of the prospect/resident to improve operational efficiency and resident satisfaction.\n\nYou will provide data capabilities and build out a common data model that supports 360 view of our prospect/resident powered by Azure SQL, Synapse data warehouse, and Microsoft Customer Insights. You will also work and support our industry-changing products and features designed to make shopping for an apartment more streamlined, e-commerce friendly, and efficient.\n\nWith your help, we will improve our customer's apartment shopping journey and enable business intelligence to help us personalize the apartment shopping experience for our residents.\n\nThe successful candidate will have a strong sense of teamwork, personal integrity, accountability, and the ability to understand business functions and requirements, translating to innovative working applications while navigating competing priority tradeoffs.\n\nJOB DESCRIPTION\n\nWhat You Will do\n\u2022 100% hands-on development - develop and unit test database code, including but not limited to T-SQL, stored procedures, functions and views.\n\u2022 Create and maintain database structures\n\u2022 As part of the Scrum team, you will work with BAs, Scrum Master, Leads and engineers to provide data support to our products and build creative solutions and features to move our product roadmap forward.\n\u2022 Participate in the design of databases, using first, second or third normalized form as needed to support business requirements.\n\u2022 Create and deploy ADF pipelines, adhering to Greystar's standards and documented best practices.\n\u2022 Perform analysis of complex data and document findings.\n\u2022 Prepare data for prescriptive and predictive modeling.\n\u2022 Combine raw data from different external sources.\n\u2022 Collaborate with data scientists and architects.\n\u2022 Play a direct role in the maintenance, technical support, documentation, and administration of databases.\n\nWho You Are\n\u2022 Strong problem solver with excellent communication skills\n\u2022 Have a growth mindset with a desire to learn and embrace challenges.\n\u2022 Innovative and passionate about your work\n\u2022 \"Self-starter\" attitude and the ability to make decisions independently.\n\nWhat You Have\n\u2022 Minimum of 3 years of relevant experience in database design and development\n\u2022 Minimum of 2 years of relevant experience in working with Azure PaaS databases\n\u2022 Minimum of 1 year of relevant experience working with Azure Data Factory.\n\u2022 Minimum of 1 year of relevant experience working with Azure Data Lakes Gen 2.\n\u2022 Working knowledge of Azure Synapse.\n\u2022 Preferred: Experience with Customer Insights and/or Dataverse.\n\u2022 Preferred: Experience with Power BI.\n\u2022 Bachelor's in Computer Science, related field, or equivalent work experience\n\nTechnical Pre-screening test will be required for all candidates\n\nWhat the Right Candidate will Enjoy!\n\u2022 100% Remote flexibility!\n\u2022 Competitive pay, benefits, and overall compensation packages.\n\u2022 The chance to be part of a technology team for a thriving organization that prioritizes accountability, respect, and operational excellence!\n\u2022 The opportunity to join a thriving, highly visible organization during its technology transformation!\n\nThe base compensation rate will vary based on education, experience, skills, and geographic location, as applicable.\n\nGreystar seeks to attract, recruit, advance and retain top talent. Greystar's compensation strategy is tailored to appropriately reward the skillset and experience that a team member will bring to the organization.\n\nDepending on the position offered, regular full-time and part-time team members may be eligible to participate in a bonus program in addition to their salary. Team members may also participate in the 401k plan, once eligible. Regular, full-time team members are offered a range of medical, financial, and other benefits from which to choose.\n\nFor Union and Prevailing Wage roles compensation and benefits may vary from the listed information above due to Collective Bargaining Agreements and/or local governing authority.\n\nGreystar will consider for employment qualified applicants with arrest and conviction records.",
            "job_is_remote": true,
            "job_posted_at_timestamp": 1689897600,
            "job_posted_at_datetime_utc": "2023-07-21T00:00:00.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "health_insurance"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=82l-Phq5MLUAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": "2023-08-20T00:00:00.000Z",
            "job_offer_expiration_timestamp": 1692489600,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 36,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": true,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "Strong problem solver with excellent communication skills",
                    "Have a growth mindset with a desire to learn and embrace challenges",
                    "Innovative and passionate about your work",
                    "\"Self-starter\" attitude and the ability to make decisions independently",
                    "Minimum of 3 years of relevant experience in database design and development",
                    "Minimum of 2 years of relevant experience in working with Azure PaaS databases",
                    "Minimum of 1 year of relevant experience working with Azure Data Lakes Gen 2",
                    "Working knowledge of Azure Synapse",
                    "Bachelor's in Computer Science, related field, or equivalent work experience",
                    "Technical Pre-screening test will be required for all candidates",
                    "The chance to be part of a technology team for a thriving organization that prioritizes accountability, respect, and operational excellence!",
                    "The opportunity to join a thriving, highly visible organization during its technology transformation!"
                ],
                "Responsibilities": [
                    "You will provide data capabilities and build out a common data model that supports 360 view of our prospect/resident powered by Azure SQL, Synapse data warehouse, and Microsoft Customer Insights",
                    "100% hands-on development - develop and unit test database code, including but not limited to T-SQL, stored procedures, functions and views",
                    "Create and maintain database structures",
                    "As part of the Scrum team, you will work with BAs, Scrum Master, Leads and engineers to provide data support to our products and build creative solutions and features to move our product roadmap forward",
                    "Participate in the design of databases, using first, second or third normalized form as needed to support business requirements",
                    "Create and deploy ADF pipelines, adhering to Greystar's standards and documented best practices",
                    "Perform analysis of complex data and document findings",
                    "Prepare data for prescriptive and predictive modeling",
                    "Combine raw data from different external sources",
                    "Collaborate with data scientists and architects",
                    "Play a direct role in the maintenance, technical support, documentation, and administration of databases"
                ],
                "Benefits": [
                    "Competitive pay, benefits, and overall compensation packages",
                    "Depending on the position offered, regular full-time and part-time team members may be eligible to participate in a bonus program in addition to their salary",
                    "Team members may also participate in the 401k plan, once eligible",
                    "Regular, full-time team members are offered a range of medical, financial, and other benefits from which to choose",
                    "For Union and Prevailing Wage roles compensation and benefits may vary from the listed information above due to Collective Bargaining Agreements and/or local governing authority"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4",
            "job_occupational_categories": [
                "15-1132.00",
                "Software Developers, Applications"
            ],
            "job_naics_code": "531110",
            "job_naics_name": "Lessors of Residential Buildings and Dwellings"
        },
        {
            "employer_name": "CyberCoders",
            "employer_logo": null,
            "employer_website": null,
            "employer_company_type": null,
            "job_publisher": "Jooble",
            "job_id": "K6j7paZxOOEAAAAAAAAAAA==",
            "job_employment_type": "FULLTIME",
            "job_title": "Data Engineer",
            "job_apply_link": "https://jooble.org/jdp/7261413729125393764/Data-Engineer-Dallas%2C-TX",
            "job_apply_is_direct": false,
            "job_apply_quality_score": 0.5112,
            "job_description": "We are looking for an experienced Data Engineer to join our Accounting - Finance team. You will be responsible for designing, implementing, and building data acquisition and data applied models. You should have experience with data analysis, data analytics, Excel, HubSpot, CRM, and Redis. If you are interested in being part of a collaborative team, working on innovative and challenging projects, then this is the job for you.\n\nTop Reasons to Work with Us\n\nWe offer a great environment that encourages collaboration, creativity, and innovation. We are always working on challenging and exciting projects that push the envelope. We also provide competitive compensation packages, flexible hours, and generous benefits.\n\nWhat You Will Be Doing\n\nAs a Data Engineer, you will be responsible for developing, implementing, and managing data acquisition and data applied models. You will also be involved in data analysis and data analytics. You will be expected to collaborate with other teams to ensure projects are completed on time and with the highest quality.\n\nWhat You Need for this Position\n\nTo qualify for this position, you should have more than 5 years of experience with data analysis, data analytics, Excel, HubSpot, CRM, and Redis. You should also have excellent problem-solving and communication skills.\n\nWhat's In It for You\n\nThis position pays between 90000 - 140000 annually. In addition to a competitive salary, we offer vacation/PTO, medical, dental, vision, bonus, and 401k.\n\nSo, if you are a Data Engineer with experience, please apply today! - View email address on jobs.institutedata.com\n\nEmail Your Resume In Word To\n\nLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:\n\nView email address on jobs.institutedata.com\n\u2022 Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : CG12-1754728 -- in the email subject line for your application to be considered.***\n\nCasey Glad - Executive Recruiter - CyberCoders\n\nApplicants must be authorized to work in the U.S.\n\nCyberCoders is proud to be an Equal Opportunity Employer\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.\n\nYour Right to Work \u2013 In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.\n\nCyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.",
            "job_is_remote": false,
            "job_posted_at_timestamp": 1689999466,
            "job_posted_at_datetime_utc": "2023-07-22T04:17:46.000Z",
            "job_city": "Dallas",
            "job_state": "TX",
            "job_country": "US",
            "job_latitude": 32.776665,
            "job_longitude": -96.79699,
            "job_benefits": [
                "retirement_savings",
                "health_insurance",
                "dental_coverage",
                "paid_time_off"
            ],
            "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=K6j7paZxOOEAAAAAAAAAAA%3D%3D",
            "job_offer_expiration_datetime_utc": null,
            "job_offer_expiration_timestamp": null,
            "job_required_experience": {
                "no_experience_required": false,
                "required_experience_in_months": 60,
                "experience_mentioned": true,
                "experience_preferred": false
            },
            "job_required_skills": null,
            "job_required_education": {
                "postgraduate_degree": false,
                "professional_certification": false,
                "high_school": false,
                "associates_degree": false,
                "bachelors_degree": false,
                "degree_mentioned": false,
                "degree_preferred": false,
                "professional_certification_mentioned": false
            },
            "job_experience_in_place_of_education": false,
            "job_min_salary": null,
            "job_max_salary": null,
            "job_salary_currency": null,
            "job_salary_period": null,
            "job_highlights": {
                "Qualifications": [
                    "If you are interested in being part of a collaborative team, working on innovative and challenging projects, then this is the job for you",
                    "To qualify for this position, you should have more than 5 years of experience with data analysis, data analytics, Excel, HubSpot, CRM, and Redis",
                    "You should also have excellent problem-solving and communication skills",
                    "Applicants must be authorized to work in the U.S"
                ],
                "Responsibilities": [
                    "As a Data Engineer, you will be responsible for developing, implementing, and managing data acquisition and data applied models",
                    "You will also be involved in data analysis and data analytics",
                    "You will be expected to collaborate with other teams to ensure projects are completed on time and with the highest quality"
                ],
                "Benefits": [
                    "We also provide competitive compensation packages, flexible hours, and generous benefits",
                    "This position pays between 90000 - 140000 annually",
                    "In addition to a competitive salary, we offer vacation/PTO, medical, dental, vision, bonus, and 401k"
                ]
            },
            "job_job_title": "Data engineer",
            "job_posting_language": "en",
            "job_onet_soc": "15113200",
            "job_onet_job_zone": "4"
        }
    ]
}