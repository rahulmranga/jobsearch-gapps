{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.txt', 'r') as file:\n",
    "    json_text = file.read()\n",
    "# Parse the JSON text to a Python dictionary\n",
    "data = json.loads(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(raw).set_index(['job_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"example_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['job_id','employer_name','job_publisher','job_employment_type','job_title','job_apply_link','job_apply_quality_score','job_posted_at_timestamp','job_posted_at_datetime_utc',\n",
    "      'job_city','job_state','job_country','job_google_link','job_highlights','job_naics_name']\n",
    "df=df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7096/1278623861.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['job_highlights']=df['job_highlights'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "df['job_highlights']=df['job_highlights'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rahulmr96/personal_web/jobsearch/query_db.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rahulmr96/personal_web/jobsearch/query_db.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m engine \u001b[39m=\u001b[39m create_engine(database_url)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rahulmr96/personal_web/jobsearch/query_db.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m table_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlistings\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/rahulmr96/personal_web/jobsearch/query_db.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m a\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mto_sql(table_name, engine, if_exists\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "# Replace these with your actual database credentials\n",
    "database_url = \"postgresql://postgres:postgres@localhost:5432/jobpostings\"\n",
    "\n",
    "engine = create_engine(database_url)\n",
    "table_name='listings'\n",
    "\n",
    "a=df.to_sql(table_name, engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      job_id               employer_name       job_publisher  \\\n",
      "0   HmRFsCRF1UsAAAAAAAAAAA==                  TEKsystems  TEKsystems Careers   \n",
      "1   Qsa1ierlUMkAAAAAAAAAAA==                       DISYS            LinkedIn   \n",
      "2   oAmKqPWYe9QAAAAAAAAAAA==              Trigger IT LLC                Dice   \n",
      "3   P7IfQYjEDhsAAAAAAAAAAA==                    Skiltrek        ZipRecruiter   \n",
      "4   QwDdBZfCBAUAAAAAAAAAAA==  Softworld, a Kelly Company           WAVY Jobs   \n",
      "..                       ...                         ...                 ...   \n",
      "84  Z3RRrMfrr9IAAAAAAAAAAA==                 ClifyX, INC       Jobrapido.com   \n",
      "85  0OPIQUWCXxMAAAAAAAAAAA==            Advaana Staffing           OPTnation   \n",
      "86  VxiGjPFk0QAAAAAAAAAAAA==                         NFI               BeBee   \n",
      "87  82l-Phq5MLUAAAAAAAAAAA==     Greystar Worldwide, LLC       CareerBuilder   \n",
      "88  K6j7paZxOOEAAAAAAAAAAA==                 CyberCoders              Jooble   \n",
      "\n",
      "   job_employment_type         job_title  \\\n",
      "0           CONTRACTOR     Data Engineer   \n",
      "1           CONTRACTOR     Data Engineer   \n",
      "2           CONTRACTOR     Data Engineer   \n",
      "3             FULLTIME     data engineer   \n",
      "4             FULLTIME     Data Engineer   \n",
      "..                 ...               ...   \n",
      "84            FULLTIME     Data Engineer   \n",
      "85          CONTRACTOR     Data Engineer   \n",
      "86            FULLTIME  Data Engineer II   \n",
      "87            FULLTIME     Data Engineer   \n",
      "88            FULLTIME     Data Engineer   \n",
      "\n",
      "                                       job_apply_link  \\\n",
      "0   https://careers.teksystems.com/ca/fr/job/JP-00...   \n",
      "1   https://www.linkedin.com/jobs/view/data-engine...   \n",
      "2   https://www.dice.com/job-detail/fbf4ceaa-f4b9-...   \n",
      "3   https://www.ziprecruiter.com/c/Skiltrek/Job/Da...   \n",
      "4   https://jobs.wavy.com/jobs/data-engineer-dalla...   \n",
      "..                                                ...   \n",
      "84     https://us.jobrapido.com/jobpreview/2926831096   \n",
      "85  https://www.optnation.com/data-engineer-job-in...   \n",
      "86  https://us.bebee.com/job/20230723-1100a4a283b7...   \n",
      "87  https://www.careerbuilder.com/job/J3Q6GK6SQ68M...   \n",
      "88  https://jooble.org/jdp/7261413729125393764/Dat...   \n",
      "\n",
      "    job_apply_quality_score  job_posted_at_timestamp  \\\n",
      "0                    0.8267               1690392102   \n",
      "1                    0.5856               1690461639   \n",
      "2                    0.5684               1690311787   \n",
      "3                    0.6290               1690277800   \n",
      "4                    0.5741               1690431118   \n",
      "..                      ...                      ...   \n",
      "84                   0.4464               1686787200   \n",
      "85                   0.6919               1677196800   \n",
      "86                   0.5000               1690073488   \n",
      "87                   0.5483               1689897600   \n",
      "88                   0.5112               1689999466   \n",
      "\n",
      "   job_posted_at_datetime_utc job_city job_state job_country  \\\n",
      "0    2023-07-26T17:21:42.000Z   Dallas        TX          US   \n",
      "1    2023-07-27T12:40:39.000Z   Dallas        TX          US   \n",
      "2    2023-07-25T19:03:07.000Z   Dallas        TX          US   \n",
      "3    2023-07-25T09:36:40.000Z   Dallas        TX          US   \n",
      "4    2023-07-27T04:11:58.000Z   Dallas        TX          US   \n",
      "..                        ...      ...       ...         ...   \n",
      "84   2023-06-15T00:00:00.000Z   Dallas        TX          US   \n",
      "85   2023-02-24T00:00:00.000Z   Dallas        TX          US   \n",
      "86   2023-07-23T00:51:28.000Z   Dallas        TX          US   \n",
      "87   2023-07-21T00:00:00.000Z   Dallas        TX          US   \n",
      "88   2023-07-22T04:17:46.000Z   Dallas        TX          US   \n",
      "\n",
      "                                      job_google_link  \\\n",
      "0   https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "1   https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "2   https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "3   https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "4   https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "..                                                ...   \n",
      "84  https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "85  https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "86  https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "87  https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "88  https://www.google.com/search?gl=us&hl=en&rciv...   \n",
      "\n",
      "                                       job_highlights  \\\n",
      "0   {'Qualifications': ['Experience writing stored...   \n",
      "1   {'Qualifications': [\"An associate degree, a ba...   \n",
      "2   {'Qualifications': [\"Option 1: Bachelorâ€™s degr...   \n",
      "3   {'Qualifications': [\"Option 1: Bachelor's degr...   \n",
      "4   {'Qualifications': ['Knowledge in AWS and mana...   \n",
      "..                                                ...   \n",
      "84  {'Qualifications': ['Expertise in batch and fi...   \n",
      "85  {'Qualifications': ['Must Have Firmwide x 2 OR...   \n",
      "86  {'Qualifications': [\"Bachelor's Degree in Comp...   \n",
      "87  {'Qualifications': ['Strong problem solver wit...   \n",
      "88  {'Qualifications': ['If you are interested in ...   \n",
      "\n",
      "                                    job_naics_name  \n",
      "0                    Employment Placement Agencies  \n",
      "1                                             None  \n",
      "2                                             None  \n",
      "3                                             None  \n",
      "4                                             None  \n",
      "..                                             ...  \n",
      "84                                            None  \n",
      "85                                            None  \n",
      "86                                            None  \n",
      "87  Lessors of Residential Buildings and Dwellings  \n",
      "88                                            None  \n",
      "\n",
      "[89 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "table_name='listings'\n",
    "engine = create_engine(database_url)\n",
    "\n",
    "\n",
    "sql_query = f\"\"\"SELECT * FROM {table_name}\n",
    "                where UPPER(job_title) like '%DATA ENGINEER%' and \n",
    "                UPPER(job_city) like '%AUSTIN%';\"\"\"\n",
    "sql_query=str(sql_query)\n",
    "df = pd.read_sql_query(\"select * from listings where job_city='Dallas';\", engine)\n",
    "\n",
    "print(df)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"job_id\":{\"0\":\"HmRFsCRF1UsAAAAAAAAAAA==\",\"1\":\"Qsa1ierlUMkAAAAAAAAAAA==\",\"2\":\"oAmKqPWYe9QAAAAAAAAAAA==\",\"3\":\"P7IfQYjEDhsAAAAAAAAAAA==\",\"4\":\"QwDdBZfCBAUAAAAAAAAAAA==\",\"5\":\"v6KgOCdJjGgAAAAAAAAAAA==\",\"6\":\"gbf_tKAAwsoAAAAAAAAAAA==\",\"7\":\"NgXphZWo5UwAAAAAAAAAAA==\",\"8\":\"ICjkXTgkKKgAAAAAAAAAAA==\",\"9\":\"HCShKDrrNT8AAAAAAAAAAA==\",\"10\":\"aV5T0382CTgAAAAAAAAAAA==\",\"11\":\"uTkg0mApiRgAAAAAAAAAAA==\",\"12\":\"eMdLLHNlwWsAAAAAAAAAAA==\",\"13\":\"g9Rv4Q-2xh0AAAAAAAAAAA==\",\"14\":\"jAYPkmjSPLIAAAAAAAAAAA==\",\"15\":\"fgEVfs0XL3QAAAAAAAAAAA==\",\"16\":\"uGwOKTzWK8YAAAAAAAAAAA==\",\"17\":\"VoQSDjkz-2QAAAAAAAAAAA==\",\"18\":\"K392nkfmd28AAAAAAAAAAA==\",\"19\":\"xWoztRVZBtcAAAAAAAAAAA==\",\"20\":\"nKAVtQsHlHgAAAAAAAAAAA==\",\"21\":\"ailo5hpdYyEAAAAAAAAAAA==\",\"22\":\"LuuL5FOvfNkAAAAAAAAAAA==\",\"23\":\"epgFodzuXh8AAAAAAAAAAA==\",\"24\":\"A2IULA4j8_8AAAAAAAAAAA==\",\"25\":\"iGjbaz-JoBcAAAAAAAAAAA==\",\"26\":\"_HaktzqqbQEAAAAAAAAAAA==\",\"27\":\"4YnqmBaYvqMAAAAAAAAAAA==\",\"28\":\"Awvrb4W-sPUAAAAAAAAAAA==\",\"29\":\"eAOHe-qYmRwAAAAAAAAAAA==\",\"30\":\"xlranAfS5CkAAAAAAAAAAA==\",\"31\":\"H4fbvCM4qssAAAAAAAAAAA==\",\"32\":\"vw-9JOTKx18AAAAAAAAAAA==\",\"33\":\"Fw73q18LK2YAAAAAAAAAAA==\",\"34\":\"Xxd017dEv5YAAAAAAAAAAA==\",\"35\":\"AXlP4nh2A4AAAAAAAAAAAA==\",\"36\":\"XBgFtaGDEqoAAAAAAAAAAA==\",\"37\":\"9vfOrrNZWXwAAAAAAAAAAA==\",\"38\":\"tHzZ5aQ6XEYAAAAAAAAAAA==\",\"39\":\"_M1aY3afaAIAAAAAAAAAAA==\",\"40\":\"t0zy0CTqYuoAAAAAAAAAAA==\",\"41\":\"UQvc_eGI98wAAAAAAAAAAA==\",\"42\":\"Zw8-q8mSkhsAAAAAAAAAAA==\",\"43\":\"zpHp8hO8r5gAAAAAAAAAAA==\",\"44\":\"n2SRAN_BDGAAAAAAAAAAAA==\",\"45\":\"hH0fMST3sloAAAAAAAAAAA==\",\"46\":\"LNndS8ll-f4AAAAAAAAAAA==\",\"47\":\"XJyrP7Ypp5EAAAAAAAAAAA==\",\"48\":\"iIPKNvIu2lsAAAAAAAAAAA==\",\"49\":\"GVXNPo1qmE4AAAAAAAAAAA==\",\"50\":\"6F_oejzQGBcAAAAAAAAAAA==\",\"51\":\"42NJql7l_7EAAAAAAAAAAA==\",\"52\":\"sli4eini4GgAAAAAAAAAAA==\",\"53\":\"8SyHUOO43CsAAAAAAAAAAA==\",\"54\":\"qIeZ_iz27YwAAAAAAAAAAA==\",\"55\":\"6VJKKZ1oBBkAAAAAAAAAAA==\",\"56\":\"Hxm5v2wJMHUAAAAAAAAAAA==\",\"57\":\"0uC1tfLfQNoAAAAAAAAAAA==\",\"58\":\"hmPiuV0MhssAAAAAAAAAAA==\",\"59\":\"F0BzDvq8V2EAAAAAAAAAAA==\",\"60\":\"GZboUoqtmI4AAAAAAAAAAA==\",\"61\":\"ziqGUCDHBPwAAAAAAAAAAA==\",\"62\":\"4cTj1j-ZIEwAAAAAAAAAAA==\",\"63\":\"pB9FEDB1j4sAAAAAAAAAAA==\",\"64\":\"7-6zJm6_MhYAAAAAAAAAAA==\",\"65\":\"fciHxK29OeAAAAAAAAAAAA==\",\"66\":\"Zl5bZRbb03QAAAAAAAAAAA==\",\"67\":\"9Z1mzqAMpa8AAAAAAAAAAA==\",\"68\":\"JH3OAdcicPwAAAAAAAAAAA==\",\"69\":\"GjPMRB4MIFMAAAAAAAAAAA==\",\"70\":\"3KBM7t96SksAAAAAAAAAAA==\",\"71\":\"t_bwv82knGAAAAAAAAAAAA==\",\"72\":\"BTtXgAW3LgEAAAAAAAAAAA==\",\"73\":\"YWCs__E_bzUAAAAAAAAAAA==\",\"74\":\"mOyDU6ACctkAAAAAAAAAAA==\",\"75\":\"B9mwS3vp2lgAAAAAAAAAAA==\",\"76\":\"vQeezN_YDuwAAAAAAAAAAA==\",\"77\":\"rLA-OhanVgEAAAAAAAAAAA==\",\"78\":\"U3j2JDYImI0AAAAAAAAAAA==\",\"79\":\"ggGlVmkBy6AAAAAAAAAAAA==\",\"80\":\"gEFo3LqOIkEAAAAAAAAAAA==\",\"81\":\"VEeZH6OmNmcAAAAAAAAAAA==\",\"82\":\"248UdFpME3EAAAAAAAAAAA==\",\"83\":\"LWxudsMKTTMAAAAAAAAAAA==\",\"84\":\"Z3RRrMfrr9IAAAAAAAAAAA==\",\"85\":\"0OPIQUWCXxMAAAAAAAAAAA==\",\"86\":\"VxiGjPFk0QAAAAAAAAAAAA==\",\"87\":\"82l-Phq5MLUAAAAAAAAAAA==\",\"88\":\"K6j7paZxOOEAAAAAAAAAAA==\"},\"employer_name\":{\"0\":\"TEKsystems\",\"1\":\"DISYS\",\"2\":\"Trigger IT LLC\",\"3\":\"Skiltrek\",\"4\":\"Softworld, a Kelly Company\",\"5\":\"Compunnel Inc.\",\"6\":\"Ascendion Inc.\",\"7\":\"Cloud BC Labs\",\"8\":\"Stefanini\",\"9\":\"Disney Media & Entertainment Distribution\",\"10\":\"Stark Dev, LLC\",\"11\":\"Alto Pharmacy\",\"12\":\"Jobot\",\"13\":\"V-Soft Consulting Group, Inc.\",\"14\":\"LTIMindtree\",\"15\":\"RIT Solutions, Inc.\",\"16\":\"Evergreen Residential Holdings, LLC\",\"17\":\"LTIMindtree\",\"18\":\"Alcority\",\"19\":\"Disney Media & Entertainment Distribution\",\"20\":\"Infojini\",\"21\":\"Fidelity Investments\",\"22\":\"AT&T\",\"23\":\"PRIMUS Global Services, Inc\",\"24\":\"Rangam Consultants Inc.\",\"25\":\"Randstad US\",\"26\":\"Incedo Inc.\",\"27\":\"Walmart\",\"28\":\"Randstad USA\",\"29\":\"Costco Wholesale\",\"30\":\"Randstad North America, Inc.\",\"31\":\"Apexon\",\"32\":\"AT&T Services, Inc.\",\"33\":\"Southwest Airlines\",\"34\":\"Galaxy i Technologies, Inc.\",\"35\":\"H-E-B\",\"36\":\"Docyt\",\"37\":\"MSRCosmos\",\"38\":\"Saxon Global\",\"39\":\"Logic20\\\\/20 Inc.\",\"40\":\"MSIT\",\"41\":\"InfoVision Inc.\",\"42\":\"UNAVAILABLE\",\"43\":\"Hinge\",\"44\":\"Georgia IT Inc.\",\"45\":\"ryan\",\"46\":\"Costco Wholesale\",\"47\":\"Deloitte\",\"48\":\"Anblicks\",\"49\":\"Amazon.com Services LLC\",\"50\":\"Galderma S.A.\",\"51\":\"Sage IT Inc\",\"52\":\"Koch Industries\",\"53\":\"Ryan, LLC\",\"54\":\"Wipro Limited\",\"55\":\"Randstad USA\",\"56\":\"Cognizant Technology Solutions\",\"57\":\"Infinity Consulting Solutions\",\"58\":\"Salesforce\",\"59\":\"H-E-B\",\"60\":\"Dice\",\"61\":\"CoreLogic\",\"62\":\"McKinsey & Company\",\"63\":\"IT Engagements,Inc.\",\"64\":\"Costco Wholesale\",\"65\":\"Walmart\",\"66\":\"BuzzClan Private Limited\",\"67\":\"Employer Direct Healthcare, LLC\",\"68\":\"Mindlance\",\"69\":\"Sam\\'s Club\",\"70\":\"Capco\",\"71\":\"CTG\",\"72\":\"Anblicks\",\"73\":\"Amazon.com Services, Inc.\",\"74\":\"JPMorgan Chase\",\"75\":\"Ad Hoc Team\",\"76\":\"The Beneficient Company Group USA LLC\",\"77\":\"H-E-B\",\"78\":\"Logic20\\\\/20, Inc.\",\"79\":\"Seamless.AI\",\"80\":\"Parkland Health and Hospital System\",\"81\":\"InnoCore Solutions, Inc.\",\"82\":\"RIT solutions Inc\",\"83\":\"Docyt\",\"84\":\"ClifyX, INC\",\"85\":\"Advaana Staffing\",\"86\":\"NFI\",\"87\":\"Greystar Worldwide, LLC\",\"88\":\"CyberCoders\"},\"job_publisher\":{\"0\":\"TEKsystems Careers\",\"1\":\"LinkedIn\",\"2\":\"Dice\",\"3\":\"ZipRecruiter\",\"4\":\"WAVY Jobs\",\"5\":\"LinkedIn\",\"6\":\"Professional Diversity Network\",\"7\":\"LinkedIn\",\"8\":\"Salary.com\",\"9\":\"JobServe\",\"10\":\"Trabajo.org\",\"11\":\"ZipRecruiter\",\"12\":\"Dice.com\",\"13\":\"LinkedIn\",\"14\":\"ZipRecruiter\",\"15\":\"Lensa\",\"16\":\"Glassdoor\",\"17\":\"LTIMindtree\",\"18\":\"ZipRecruiter\",\"19\":\"MyArklaMiss Jobs\",\"20\":\"Jobrapido.com\",\"21\":\"Fidelity Investments\",\"22\":\"Career Cast Diversity - CareerCast.com\",\"23\":\"Indeed\",\"24\":\"Rangam\",\"25\":\"Nexxt\",\"26\":\"LinkedIn\",\"27\":\"Walmart Careers\",\"28\":\"Monster\",\"29\":\"Glassdoor\",\"30\":\"Jobrapido.com\",\"31\":\"Salary.com\",\"32\":\"Professional Diversity Network\",\"33\":\"Southwest Careers - Southwest Airlines\",\"34\":\"Dice\",\"35\":\"Built In\",\"36\":\"Salary.com\",\"37\":\"Dice\",\"38\":\"Lensa\",\"39\":\"Recruit.net\",\"40\":\"OPTnation\",\"41\":\"LinkedIn\",\"42\":\"H-E-B, L.P. - ICIMS\",\"43\":\"The Muse\",\"44\":\"Talent.com\",\"45\":\"Salary.com\",\"46\":\"JobzMall\",\"47\":\"Deloitte Jobs\",\"48\":\"Built In\",\"49\":\"Dallas, TX - Geebo\",\"50\":\"ZipRecruiter\",\"51\":\"Salary.com\",\"52\":\"LinkedIn\",\"53\":\"Glassdoor\",\"54\":\"OPTnation\",\"55\":\"Monster\",\"56\":\"Ladders\",\"57\":\"Lensa\",\"58\":\"Dallas, TX - Geebo\",\"59\":\"ZipRecruiter\",\"60\":\"LinkedIn\",\"61\":\"Talent.com\",\"62\":\"JobzMall\",\"63\":\"Glassdoor\",\"64\":\"JobzMall\",\"65\":\"Dallas, TX - Geebo\",\"66\":\"Indeed\",\"67\":\"Monster\",\"68\":\"Lensa\",\"69\":\"Walmart Careers\",\"70\":\"Salary.com\",\"71\":\"Nexxt\",\"72\":\"ZipRecruiter\",\"73\":\"Dallas, TX - Geebo\",\"74\":\"Built In\",\"75\":\"Glassdoor\",\"76\":\"Lensa\",\"77\":\"Talent.com\",\"78\":\"LinkedIn\",\"79\":\"WAVY Jobs\",\"80\":\"ZipRecruiter\",\"81\":\"Dice.com\",\"82\":\"Techfetch\",\"83\":\"Glassdoor\",\"84\":\"Jobrapido.com\",\"85\":\"OPTnation\",\"86\":\"BeBee\",\"87\":\"CareerBuilder\",\"88\":\"Jooble\"},\"job_employment_type\":{\"0\":\"CONTRACTOR\",\"1\":\"CONTRACTOR\",\"2\":\"CONTRACTOR\",\"3\":\"FULLTIME\",\"4\":\"FULLTIME\",\"5\":\"FULLTIME\",\"6\":\"FULLTIME\",\"7\":\"FULLTIME\",\"8\":\"CONTRACTOR\",\"9\":\"FULLTIME\",\"10\":\"FULLTIME\",\"11\":\"FULLTIME\",\"12\":\"FULLTIME\",\"13\":\"FULLTIME\",\"14\":\"FULLTIME\",\"15\":\"FULLTIME\",\"16\":\"FULLTIME\",\"17\":\"FULLTIME\",\"18\":\"FULLTIME\",\"19\":\"FULLTIME\",\"20\":\"FULLTIME\",\"21\":\"FULLTIME\",\"22\":\"FULLTIME\",\"23\":\"CONTRACTOR\",\"24\":\"CONTRACTOR\",\"25\":\"CONTRACTOR\",\"26\":\"FULLTIME\",\"27\":\"FULLTIME\",\"28\":\"FULLTIME\",\"29\":\"FULLTIME\",\"30\":\"FULLTIME\",\"31\":\"FULLTIME\",\"32\":\"FULLTIME\",\"33\":\"FULLTIME\",\"34\":\"FULLTIME\",\"35\":\"FULLTIME\",\"36\":\"FULLTIME\",\"37\":\"FULLTIME\",\"38\":\"FULLTIME\",\"39\":\"FULLTIME\",\"40\":\"FULLTIME\",\"41\":\"CONTRACTOR\",\"42\":\"FULLTIME\",\"43\":\"FULLTIME\",\"44\":\"FULLTIME\",\"45\":\"FULLTIME\",\"46\":\"FULLTIME\",\"47\":\"FULLTIME\",\"48\":\"FULLTIME\",\"49\":\"FULLTIME\",\"50\":\"FULLTIME\",\"51\":\"FULLTIME\",\"52\":\"FULLTIME\",\"53\":\"FULLTIME\",\"54\":\"FULLTIME\",\"55\":\"CONTRACTOR\",\"56\":\"FULLTIME\",\"57\":\"FULLTIME\",\"58\":\"FULLTIME\",\"59\":\"FULLTIME\",\"60\":\"FULLTIME\",\"61\":\"FULLTIME\",\"62\":\"FULLTIME\",\"63\":\"FULLTIME\",\"64\":\"FULLTIME\",\"65\":\"FULLTIME\",\"66\":\"FULLTIME\",\"67\":\"FULLTIME\",\"68\":\"FULLTIME\",\"69\":\"FULLTIME\",\"70\":\"FULLTIME\",\"71\":\"FULLTIME\",\"72\":\"FULLTIME\",\"73\":\"FULLTIME\",\"74\":\"FULLTIME\",\"75\":\"FULLTIME\",\"76\":\"FULLTIME\",\"77\":\"FULLTIME\",\"78\":\"FULLTIME\",\"79\":\"FULLTIME\",\"80\":\"FULLTIME\",\"81\":\"CONTRACTOR\",\"82\":\"CONTRACTOR\",\"83\":\"FULLTIME\",\"84\":\"FULLTIME\",\"85\":\"CONTRACTOR\",\"86\":\"FULLTIME\",\"87\":\"FULLTIME\",\"88\":\"FULLTIME\"},\"job_title\":{\"0\":\"Data Engineer\",\"1\":\"Data Engineer\",\"2\":\"Data Engineer\",\"3\":\"data engineer\",\"4\":\"Data Engineer\",\"5\":\"Data Engineer\",\"6\":\"Data Engineer\",\"7\":\"Data engineer\",\"8\":\"Data Engineer\",\"9\":\"Lead Data Engineer\",\"10\":\"Sr. Azure Data Engineer\\\\/Dallas,TX\\\\/ W2\",\"11\":\"Senior Data Engineer\",\"12\":\"Azure Data Engineer\",\"13\":\"AWS Data Engineer\",\"14\":\"Associate Principal -Data Engineering\",\"15\":\"Data Engineer\",\"16\":\"Sr. Data Engineer\",\"17\":\"Senior Specialist - Data Engineering\",\"18\":\"Data Engineering Manager\",\"19\":\"Lead Data Engineer\",\"20\":\"Jr. Data Engineer with AWS\",\"21\":\"Data Engineer\",\"22\":\"Principal-Big Data Engineer\",\"23\":\"Software Developer\\\\/Data Engineer \\\\u2013 Apache Spark, SQL, Kafka Connect \\\\u2013 Dallas, TX (Hybrid) 44700\",\"24\":\"Data Engineer\",\"25\":\"data engineer\",\"26\":\"Data Engineer\",\"27\":\"(USA) Senior Data Engineer - Data Ventures\",\"28\":\"Data Engineer\",\"29\":\"Data Engineer - Data Science & Analytics\",\"30\":\"Data Engineer\",\"31\":\"AWS Data Engineer\",\"32\":\"Principal- Big Data Engineer\",\"33\":\"Sr Data Engineer\",\"34\":\"DATA ENGINEER\",\"35\":\"Senior Data Engineer, DevX and Platform-Dallas, Austin, or San Antonio, TX (Dallas, TX)\",\"36\":\"Big Data Engineer\",\"37\":\"Senior Data Engineer - Multiple locations\",\"38\":\"Mid- Data Engineer\",\"39\":\"Cloud Data Engineer\",\"40\":\"Data Engineer\",\"41\":\"Big Data Engineer\",\"42\":\"Staff Data Engineer, Health & Wellness - Dallas, TX\",\"43\":\"Sr. Data Engineer\",\"44\":\"Data engineer\",\"45\":\"Senior Data Engineer, Data Engineering\",\"46\":\"Data Engineer - IT Sustainability\",\"47\":\"Cloud Data Engineer - Healthcare\",\"48\":\"Senior Data Engineer (Dallas, TX)\",\"49\":\"Data Engineer\",\"50\":\"Data Engineer\",\"51\":\"Azure Data Engineer\",\"52\":\"Security Data Engineer\",\"53\":\"Senior Data Engineer, Data Engineering\",\"54\":\"Data Engineer\",\"55\":\"Sr. Data Engineer\",\"56\":\"Senior Data Engineer- Databricks\",\"57\":\"Data Engineer\",\"58\":\"Associate Data Engineer, Enterprise Data Warehouse at Salesforce in Dallas, TX\",\"59\":\"Senior Data Engineer, Dallas, Austin, or San Antonio, TX\",\"60\":\"Big Data Engineer\\\\/Data Engineer(Only W2 or Self Corp.)\",\"61\":\"Senior data engineer\",\"62\":\"Data Engineer\",\"63\":\"Sr Cloud Data Engineer\",\"64\":\"Data Engineer\",\"65\":\"Data Engineer III\",\"66\":\"Data Engineer\",\"67\":\"Data Engineer\",\"68\":\"Lead Data Engineer\",\"69\":\"Senior Manager I, Data Engineering\",\"70\":\"Data Engineer\",\"71\":\"Data Engineer - Legacy Data Conversion\",\"72\":\"Data Engineer\",\"73\":\"Senior Data Engineer\",\"74\":\"Senior Data Engineer- AWS (Dallas, TX)\",\"75\":\"Data Engineer (Remote)\",\"76\":\"Data Engineer\\\\/Analyst\",\"77\":\"Senior data engineer\",\"78\":\"Cloud Data Engineer\",\"79\":\"Vice President of Data Engineering\",\"80\":\"Data Engineer - PCHP\",\"81\":\"Sr. ETL Migration Data Engineer\",\"82\":\"Sr Data Engineer\",\"83\":\"Big Data Engineer\",\"84\":\"Data Engineer\",\"85\":\"Data Engineer\",\"86\":\"Data Engineer II\",\"87\":\"Data Engineer\",\"88\":\"Data Engineer\"},\"job_apply_link\":{\"0\":\"https:\\\\/\\\\/careers.teksystems.com\\\\/ca\\\\/fr\\\\/job\\\\/JP-003930154\\\\/Data-Engineer\",\"1\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/data-engineer-at-disys-3673557460\",\"2\":\"https:\\\\/\\\\/www.dice.com\\\\/job-detail\\\\/fbf4ceaa-f4b9-4873-bb34-2c1ed7b97228\",\"3\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/Skiltrek\\\\/Job\\\\/Data-Engineer\\\\/-in-Dallas,TX?jid=80b3c66c3cb0a5ed\",\"4\":\"https:\\\\/\\\\/jobs.wavy.com\\\\/jobs\\\\/data-engineer-dallas-texas\\\\/1074871948-2\\\\/\",\"5\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/data-engineer-at-compunnel-inc-3677796668\",\"6\":\"https:\\\\/\\\\/www.prodivnet.com\\\\/job\\\\/data-engineer-dallas-texas-13317454\",\"7\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/data-engineer-at-cloud-bc-labs-3676042142\",\"8\":\"https:\\\\/\\\\/www.salary.com\\\\/job\\\\/stefanini\\\\/data-engineer\\\\/j202307260256561284218\",\"9\":\"https:\\\\/\\\\/www.jobserve.com\\\\/us\\\\/en\\\\/extjob\\\\/LEAD-DATA-ENGINEER-in-Dallas-Texas-USA-9B3F79DD177BEEF34C\\\\/\",\"10\":\"https:\\\\/\\\\/us.trabajo.org\\\\/job-1275-20230724-aa3f02cd58ce867110e1b6bcb7663628\",\"11\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/Alto-Pharmacy\\\\/Job\\\\/Senior-Data-Engineer\\\\/-in-Dallas,TX?jid=a78db4a3205d1126\",\"12\":\"https:\\\\/\\\\/www.dice.com\\\\/job-detail\\\\/1fd6150a-43b5-46d8-8217-ee965a24970a\",\"13\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/aws-data-engineer-at-v-soft-consulting-group-inc-3673779665\",\"14\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/LTIMindtree\\\\/Job\\\\/Associate-Principal-Data-Engineering\\\\/-in-Dallas,TX?jid=244b1ab0515b9704\",\"15\":\"https:\\\\/\\\\/lensa.com\\\\/data-engineer-jobs\\\\/dallas\\\\/jd\\\\/d8a402729fcfee1b3630e62a8405127f\",\"16\":\"https:\\\\/\\\\/www.glassdoor.com\\\\/job-listing\\\\/sr-data-engineer-evergreen-residential-JV_IC1139977_KO0,16_KE17,38.htm?jl=1008718815422\",\"17\":\"https:\\\\/\\\\/careers.ltimindtree.com\\\\/job\\\\/Dallas-Senior-Specialist-Data-Engineering-TX-75201\\\\/956028601\\\\/\",\"18\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/Alcority\\\\/Job\\\\/Data-Engineering-Manager\\\\/-in-Dallas,TX?jid=36ea57b473f139e6\",\"19\":\"https:\\\\/\\\\/jobs.myarklamiss.com\\\\/jobs\\\\/lead-data-engineer-dallas-texas\\\\/1074864542-2\\\\/\",\"20\":\"https:\\\\/\\\\/us.jobrapido.com\\\\/jobpreview\\\\/2962504484\",\"21\":\"https:\\\\/\\\\/jobiak.fidelity.com\\\\/jobdetails\\\\/dallas-tx-data-engineer-64be49cf4d21b70a8cf28959\",\"22\":\"https:\\\\/\\\\/diversity.careercast.com\\\\/jobs\\\\/principal-big-data-engineer-dallas-tx-75219-135026790-d\",\"23\":\"https:\\\\/\\\\/www.indeed.com\\\\/viewjob?jk=f14fb7a2b85d5796\",\"24\":\"https:\\\\/\\\\/rangam.com\\\\/jobs\\\\/jobdetails\\\\/110586\\\\/data-engineer-dallas-tx-us\",\"25\":\"https:\\\\/\\\\/www.nexxt.com\\\\/jobs\\\\/data-engineer-dallas-tx-2557826868-job.html?aff=2ED44C72-8FD2-4B5D-BC54-2F623E88BE26\",\"26\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/data-engineer-at-incedo-inc-3675626919\",\"27\":\"https:\\\\/\\\\/careers.walmart.com\\\\/us\\\\/jobs\\\\/WD1562366-usa-senior-data-engineer-data-ventures\",\"28\":\"https:\\\\/\\\\/www.monster.com\\\\/job-openings\\\\/data-engineer-dallas-tx--53f94751-4d3a-4f39-9305-392e3510b4b8?mstr_dist=true\",\"29\":\"https:\\\\/\\\\/www.glassdoor.com\\\\/job-listing\\\\/data-engineer-data-science-and-analytics-costco-wholesale-JV_IC1139977_KO0,40_KE41,57.htm?jl=1008427328488\",\"30\":\"https:\\\\/\\\\/us.jobrapido.com\\\\/jobpreview\\\\/2962679032\",\"31\":\"https:\\\\/\\\\/www.salary.com\\\\/job\\\\/apexon\\\\/aws-data-engineer\\\\/j202307210211154619759\",\"32\":\"https:\\\\/\\\\/www.prodivnet.com\\\\/job\\\\/principal-big-data-engineer-dallas-texas-13312115\",\"33\":\"https:\\\\/\\\\/careers.southwestair.com\\\\/job\\\\/R-2023-32207\\\\/Sr-Data-Engineer\",\"34\":\"https:\\\\/\\\\/www.dice.com\\\\/job-detail\\\\/3c29ce84-8ce6-41cb-9d94-f9425b9662e2\",\"35\":\"https:\\\\/\\\\/builtin.com\\\\/job\\\\/data\\\\/senior-data-engineer-devx-and-platform-dallas-austin-or-san-antonio-tx\\\\/1665565\",\"36\":\"https:\\\\/\\\\/www.salary.com\\\\/job\\\\/docyt\\\\/big-data-engineer\\\\/j202307221606596443368\",\"37\":\"https:\\\\/\\\\/www.dice.com\\\\/job-detail\\\\/ee40beb9-1cd0-48fb-b2ce-66f5ccfc1747\",\"38\":\"https:\\\\/\\\\/lensa.com\\\\/mid-data-engineer-jobs\\\\/dallas\\\\/jd\\\\/b6d4b47da82eebd9829510cce511dcca\",\"39\":\"https:\\\\/\\\\/www.recruit.net\\\\/job\\\\/data-engineer-jobs\\\\/06F50F3CC7184312\",\"40\":\"https:\\\\/\\\\/www.optnation.com\\\\/data-engineer-job-in-dallas-tx-view-jobid-33660\",\"41\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/big-data-engineer-at-infovision-inc-3672479393\",\"42\":\"https:\\\\/\\\\/careers-heb.icims.com\\\\/jobs\\\\/10643\\\\/staff-data-engineer%2C-health-%26-wellness---dallas%2C-tx\\\\/job\",\"43\":\"https:\\\\/\\\\/www.themuse.com\\\\/jobs\\\\/hinge\\\\/sr-data-engineer\",\"44\":\"https:\\\\/\\\\/www.talent.com\\\\/view?id=376ab70841cb\",\"45\":\"https:\\\\/\\\\/www.salary.com\\\\/job\\\\/ryan\\\\/senior-data-engineer-data-engineering\\\\/j202305230636442430293\",\"46\":\"https:\\\\/\\\\/www.jobzmall.com\\\\/costco-wholesale\\\\/job\\\\/data-engineer-it-sustainability-1\",\"47\":\"https:\\\\/\\\\/jobsus.deloitte.com\\\\/dallas-tx\\\\/cloud-data-engineer-healthcare\\\\/5347059EE1A64B8EB63C4D1D17227594\\\\/job\\\\/\",\"48\":\"https:\\\\/\\\\/builtin.com\\\\/job\\\\/data\\\\/senior-data-engineer\\\\/1600833\",\"49\":\"https:\\\\/\\\\/dallas-tx.geebo.com\\\\/jobs-online\\\\/view\\\\/id\\\\/1082323003-data-engineer-\\\\/\",\"50\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/Galderma-S.A.\\\\/Job\\\\/Data-Engineer\\\\/-in-Dallas,TX?jid=a2ca00aba7b601c2\",\"51\":\"https:\\\\/\\\\/www.salary.com\\\\/job\\\\/sage-it-inc\\\\/azure-data-engineer\\\\/j202301310152169988722\",\"52\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/security-data-engineer-at-koch-industries-3675614617\",\"53\":\"https:\\\\/\\\\/www.glassdoor.com\\\\/job-listing\\\\/senior-data-engineer-data-engineering-ryan-llc-JV_IC1139977_KO0,37_KE38,46.htm?jl=1008660428279\",\"54\":\"https:\\\\/\\\\/www.optnation.com\\\\/data-engineer-job-in-dallas-tx-view-jobid-29161\",\"55\":\"https:\\\\/\\\\/www.monster.com\\\\/job-openings\\\\/sr-data-engineer-dallas-tx--37e67443-4bdb-4d63-a605-9f6778fa8531\",\"56\":\"https:\\\\/\\\\/www.theladders.com\\\\/job\\\\/senior-data-engineer-databricks-cognizant-dallas-tx_64139874\",\"57\":\"https:\\\\/\\\\/lensa.com\\\\/data-engineer-jobs\\\\/dallas\\\\/jd\\\\/b8d9c2125bf7388601b5a0f327748a49\",\"58\":\"https:\\\\/\\\\/dallas-tx.geebo.com\\\\/jobs-online\\\\/view\\\\/id\\\\/1051796261-associate-data-engineer-enterprise-\\\\/\",\"59\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/H-E-B\\\\/Job\\\\/Senior-Data-Engineer,-Dallas,-Austin,-or-San-Antonio,-TX\\\\/-in-Dallas,TX?jid=1e6c47e6fd30e3b3\",\"60\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/big-data-engineer-data-engineer-only-w2-or-self-corp-at-dice-3674337930\",\"61\":\"https:\\\\/\\\\/www.talent.com\\\\/view?id=8e0ac49a262d\",\"62\":\"https:\\\\/\\\\/www.jobzmall.com\\\\/mckinsey-company\\\\/job\\\\/data-engineer-113\",\"63\":\"https:\\\\/\\\\/www.glassdoor.com\\\\/job-listing\\\\/sr-cloud-data-engineer-it-engagements-inc-JV_IC1139977_KO0,22_KE23,41.htm?jl=1008579705781\",\"64\":\"https:\\\\/\\\\/www.jobzmall.com\\\\/costco-wholesale\\\\/job\\\\/data-engineer-251\",\"65\":\"https:\\\\/\\\\/dallas-tx.geebo.com\\\\/jobs-online\\\\/view\\\\/id\\\\/1139475167-data-engineer-iii-\\\\/\",\"66\":\"https:\\\\/\\\\/www.indeed.com\\\\/viewjob?jk=e185e49f149ae1b0\",\"67\":\"https:\\\\/\\\\/www.monster.com\\\\/job-openings\\\\/data-engineer-dallas-tx--85381867-01d8-4e40-85d5-6de644c3e7c7\",\"68\":\"https:\\\\/\\\\/lensa.com\\\\/lead-data-engineer-jobs\\\\/dallas\\\\/jd\\\\/e4b6a0390857a679def8b8fc111a2057\",\"69\":\"https:\\\\/\\\\/careers.walmart.com\\\\/us\\\\/jobs\\\\/WD1527986-senior-manager-i-data-engineering\",\"70\":\"https:\\\\/\\\\/www.salary.com\\\\/job\\\\/capco\\\\/data-engineer\\\\/j202301250117332463849\",\"71\":\"https:\\\\/\\\\/www.nexxt.com\\\\/jobs\\\\/data-engineer-legacy-data-conversion-dallas-tx-2545869795-job.html?aff=2ED44C72-8FD2-4B5D-BC54-2F623E88BE26\",\"72\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/Anblicks\\\\/Job\\\\/Data-Engineer\\\\/-in-Dallas,TX?jid=6f97aba4bed93265\",\"73\":\"https:\\\\/\\\\/dallas-tx.geebo.com\\\\/jobs-online\\\\/view\\\\/id\\\\/761054315-senior-data-engineer-\\\\/\",\"74\":\"https:\\\\/\\\\/builtin.com\\\\/job\\\\/data\\\\/senior-data-engineer-aws\\\\/1898846\",\"75\":\"https:\\\\/\\\\/www.glassdoor.com\\\\/job-listing\\\\/data-engineer-remote-ad-hoc-dc-JV_IC1139977_KO0,20_KE21,30.htm?jl=1008732513836\",\"76\":\"https:\\\\/\\\\/lensa.com\\\\/data-engineeranalyst-jobs\\\\/dallas\\\\/jd\\\\/d9df6f49861354a487aa4b420188497a\",\"77\":\"https:\\\\/\\\\/www.talent.com\\\\/view?id=d7d2d7817df4\",\"78\":\"https:\\\\/\\\\/www.linkedin.com\\\\/jobs\\\\/view\\\\/cloud-data-engineer-at-logic20-20-inc-3676014190\",\"79\":\"https:\\\\/\\\\/jobs.wavy.com\\\\/jobs\\\\/vice-president-of-data-engineering-dallas-texas\\\\/1073551209-2\\\\/\",\"80\":\"https:\\\\/\\\\/www.ziprecruiter.com\\\\/c\\\\/Parkland-Health-and-Hospital-System\\\\/Job\\\\/Data-Engineer-PCHP\\\\/-in-Dallas,TX?jid=8029868922761f0f\",\"81\":\"https:\\\\/\\\\/www.dice.com\\\\/job-detail\\\\/e4fea931-6de0-41bf-8fd6-df33a15a3e9a\",\"82\":\"https:\\\\/\\\\/www.techfetch.com\\\\/job-description\\\\/sr-data-engineer-dallas-tx-j3576490\",\"83\":\"https:\\\\/\\\\/www.glassdoor.com\\\\/job-listing\\\\/big-data-engineer-docyt-JV_IC1139977_KO0,17_KE18,23.htm?jl=1008768999974\",\"84\":\"https:\\\\/\\\\/us.jobrapido.com\\\\/jobpreview\\\\/2926831096\",\"85\":\"https:\\\\/\\\\/www.optnation.com\\\\/data-engineer-job-in-dallas-tx-view-jobid-28150\",\"86\":\"https:\\\\/\\\\/us.bebee.com\\\\/job\\\\/20230723-1100a4a283b734c77bfe515ca2401cf7\",\"87\":\"https:\\\\/\\\\/www.careerbuilder.com\\\\/job\\\\/J3Q6GK6SQ68MDBSDJ23\",\"88\":\"https:\\\\/\\\\/jooble.org\\\\/jdp\\\\/7261413729125393764\\\\/Data-Engineer-Dallas%2C-TX\"},\"job_apply_quality_score\":{\"0\":0.8267,\"1\":0.5856,\"2\":0.5684,\"3\":0.629,\"4\":0.5741,\"5\":0.5598,\"6\":0.5781,\"7\":0.5773,\"8\":0.5621,\"9\":0.5575,\"10\":0.3783,\"11\":0.6175,\"12\":0.6698,\"13\":0.5829,\"14\":0.6232,\"15\":0.4732,\"16\":0.5623,\"17\":0.7696,\"18\":0.7286,\"19\":0.5492,\"20\":0.4464,\"21\":0.5711,\"22\":0.5555,\"23\":0.5816,\"24\":0.6779,\"25\":0.4909,\"26\":0.7006,\"27\":0.7968,\"28\":0.5581,\"29\":0.5467,\"30\":0.4492,\"31\":0.5713,\"32\":0.5681,\"33\":0.8069,\"34\":0.5622,\"35\":0.688,\"36\":0.5621,\"37\":0.5757,\"38\":0.462,\"39\":0.4154,\"40\":0.6987,\"41\":0.5886,\"42\":0.6721,\"43\":0.5766,\"44\":0.4858,\"45\":0.6703,\"46\":0.5613,\"47\":0.7262,\"48\":0.7071,\"49\":0.424,\"50\":0.7258,\"51\":0.6615,\"52\":0.5519,\"53\":0.566,\"54\":0.6901,\"55\":0.6571,\"56\":0.5671,\"57\":0.4713,\"58\":0.4366,\"59\":0.6602,\"60\":0.5631,\"61\":0.4889,\"62\":0.5696,\"63\":0.5566,\"64\":0.5613,\"65\":0.404,\"66\":0.6156,\"67\":0.6695,\"68\":0.475,\"69\":0.6831,\"70\":0.6827,\"71\":0.4993,\"72\":0.7224,\"73\":0.4224,\"74\":0.6987,\"75\":0.573,\"76\":0.4711,\"77\":0.5006,\"78\":0.5875,\"79\":0.5612,\"80\":0.7163,\"81\":0.6635,\"82\":0.4578,\"83\":0.573,\"84\":0.4464,\"85\":0.6919,\"86\":0.5,\"87\":0.5483,\"88\":0.5112},\"job_posted_at_timestamp\":{\"0\":1690392102,\"1\":1690461639,\"2\":1690311787,\"3\":1690277800,\"4\":1690431118,\"5\":1690474929,\"6\":1690352408,\"7\":1690416391,\"8\":1690416000,\"9\":1690416120,\"10\":1690175599,\"11\":1689934451,\"12\":1689423709,\"13\":1690213278,\"14\":1690467480,\"15\":1689276160,\"16\":1687392000,\"17\":1688947200,\"18\":1690271308,\"19\":1690429495,\"20\":1690329600,\"21\":1690329600,\"22\":1689897600,\"23\":1690497100,\"24\":1690296823,\"25\":1690304753,\"26\":1690319689,\"27\":1690156800,\"28\":1690329600,\"29\":1690156800,\"30\":1690329600,\"31\":1689984000,\"32\":1690205878,\"33\":1690502400,\"34\":1690473702,\"35\":1678251629,\"36\":1690156800,\"37\":1690403463,\"38\":1690215552,\"39\":1690502400,\"40\":1685923200,\"41\":1690214955,\"42\":1627261705,\"43\":1619044350,\"44\":1690329600,\"45\":1688515200,\"46\":1676932976,\"47\":1689047561,\"48\":1675497656,\"49\":1690416000,\"50\":1689552000,\"51\":1690243200,\"52\":1690317872,\"53\":1689984000,\"54\":1677196800,\"55\":1690416000,\"56\":1688352222,\"57\":1689437824,\"58\":1690502400,\"59\":1627369200,\"60\":1690213856,\"61\":1690416000,\"62\":1610146599,\"63\":1688947200,\"64\":1676932949,\"65\":1690416000,\"66\":1680704545,\"67\":1688688000,\"68\":1689265152,\"69\":1686268800,\"70\":1684454400,\"71\":1689356309,\"72\":1690441200,\"73\":1690416000,\"74\":1688270440,\"75\":1688083200,\"76\":1688343552,\"77\":1690156800,\"78\":1690329600,\"79\":1690295159,\"80\":1689724800,\"81\":1688390886,\"82\":1676937600,\"83\":1689811200,\"84\":1686787200,\"85\":1677196800,\"86\":1690073488,\"87\":1689897600,\"88\":1689999466},\"job_posted_at_datetime_utc\":{\"0\":\"2023-07-26T17:21:42.000Z\",\"1\":\"2023-07-27T12:40:39.000Z\",\"2\":\"2023-07-25T19:03:07.000Z\",\"3\":\"2023-07-25T09:36:40.000Z\",\"4\":\"2023-07-27T04:11:58.000Z\",\"5\":\"2023-07-27T16:22:09.000Z\",\"6\":\"2023-07-26T06:20:08.000Z\",\"7\":\"2023-07-27T00:06:31.000Z\",\"8\":\"2023-07-27T00:00:00.000Z\",\"9\":\"2023-07-27T00:02:00.000Z\",\"10\":\"2023-07-24T05:13:19.000Z\",\"11\":\"2023-07-21T10:14:11.000Z\",\"12\":\"2023-07-15T12:21:49.000Z\",\"13\":\"2023-07-24T15:41:18.000Z\",\"14\":\"2023-07-27T14:18:00.000Z\",\"15\":\"2023-07-13T19:22:40.000Z\",\"16\":\"2023-06-22T00:00:00.000Z\",\"17\":\"2023-07-10T00:00:00.000Z\",\"18\":\"2023-07-25T07:48:28.000Z\",\"19\":\"2023-07-27T03:44:55.000Z\",\"20\":\"2023-07-26T00:00:00.000Z\",\"21\":\"2023-07-26T00:00:00.000Z\",\"22\":\"2023-07-21T00:00:00.000Z\",\"23\":\"2023-07-27T22:31:40.000Z\",\"24\":\"2023-07-25T14:53:43.000Z\",\"25\":\"2023-07-25T17:05:53.000Z\",\"26\":\"2023-07-25T21:14:49.000Z\",\"27\":\"2023-07-24T00:00:00.000Z\",\"28\":\"2023-07-26T00:00:00.000Z\",\"29\":\"2023-07-24T00:00:00.000Z\",\"30\":\"2023-07-26T00:00:00.000Z\",\"31\":\"2023-07-22T00:00:00.000Z\",\"32\":\"2023-07-24T13:37:58.000Z\",\"33\":\"2023-07-28T00:00:00.000Z\",\"34\":\"2023-07-27T16:01:42.000Z\",\"35\":\"2023-03-08T05:00:29.000Z\",\"36\":\"2023-07-24T00:00:00.000Z\",\"37\":\"2023-07-26T20:31:03.000Z\",\"38\":\"2023-07-24T16:19:12.000Z\",\"39\":\"2023-07-28T00:00:00.000Z\",\"40\":\"2023-06-05T00:00:00.000Z\",\"41\":\"2023-07-24T16:09:15.000Z\",\"42\":\"2021-07-26T01:08:25.000Z\",\"43\":\"2021-04-21T22:32:30.000Z\",\"44\":\"2023-07-26T00:00:00.000Z\",\"45\":\"2023-07-05T00:00:00.000Z\",\"46\":\"2023-02-20T22:42:56.000Z\",\"47\":\"2023-07-11T03:52:41.000Z\",\"48\":\"2023-02-04T08:00:56.000Z\",\"49\":\"2023-07-27T00:00:00.000Z\",\"50\":\"2023-07-17T00:00:00.000Z\",\"51\":\"2023-07-25T00:00:00.000Z\",\"52\":\"2023-07-25T20:44:32.000Z\",\"53\":\"2023-07-22T00:00:00.000Z\",\"54\":\"2023-02-24T00:00:00.000Z\",\"55\":\"2023-07-27T00:00:00.000Z\",\"56\":\"2023-07-03T02:43:42.000Z\",\"57\":\"2023-07-15T16:17:04.000Z\",\"58\":\"2023-07-28T00:00:00.000Z\",\"59\":\"2021-07-27T07:00:00.000Z\",\"60\":\"2023-07-24T15:50:56.000Z\",\"61\":\"2023-07-27T00:00:00.000Z\",\"62\":\"2021-01-08T22:56:39.000Z\",\"63\":\"2023-07-10T00:00:00.000Z\",\"64\":\"2023-02-20T22:42:29.000Z\",\"65\":\"2023-07-27T00:00:00.000Z\",\"66\":\"2023-04-05T14:22:25.000Z\",\"67\":\"2023-07-07T00:00:00.000Z\",\"68\":\"2023-07-13T16:19:12.000Z\",\"69\":\"2023-06-09T00:00:00.000Z\",\"70\":\"2023-05-19T00:00:00.000Z\",\"71\":\"2023-07-14T17:38:29.000Z\",\"72\":\"2023-07-27T07:00:00.000Z\",\"73\":\"2023-07-27T00:00:00.000Z\",\"74\":\"2023-07-02T04:00:40.000Z\",\"75\":\"2023-06-30T00:00:00.000Z\",\"76\":\"2023-07-03T00:19:12.000Z\",\"77\":\"2023-07-24T00:00:00.000Z\",\"78\":\"2023-07-26T00:00:00.000Z\",\"79\":\"2023-07-25T14:25:59.000Z\",\"80\":\"2023-07-19T00:00:00.000Z\",\"81\":\"2023-07-03T13:28:06.000Z\",\"82\":\"2023-02-21T00:00:00.000Z\",\"83\":\"2023-07-20T00:00:00.000Z\",\"84\":\"2023-06-15T00:00:00.000Z\",\"85\":\"2023-02-24T00:00:00.000Z\",\"86\":\"2023-07-23T00:51:28.000Z\",\"87\":\"2023-07-21T00:00:00.000Z\",\"88\":\"2023-07-22T04:17:46.000Z\"},\"job_city\":{\"0\":\"Dallas\",\"1\":\"Dallas\",\"2\":\"Dallas\",\"3\":\"Dallas\",\"4\":\"Dallas\",\"5\":\"Dallas\",\"6\":\"Dallas\",\"7\":\"Dallas\",\"8\":\"Dallas\",\"9\":\"Dallas\",\"10\":\"Dallas\",\"11\":\"Dallas\",\"12\":\"Dallas\",\"13\":\"Dallas\",\"14\":\"Dallas\",\"15\":\"Dallas\",\"16\":\"Dallas\",\"17\":\"Dallas\",\"18\":\"Dallas\",\"19\":\"Dallas\",\"20\":\"Dallas\",\"21\":\"Dallas\",\"22\":\"Dallas\",\"23\":\"Dallas\",\"24\":\"Dallas\",\"25\":\"Dallas\",\"26\":\"Dallas\",\"27\":\"Dallas\",\"28\":\"Dallas\",\"29\":\"Dallas\",\"30\":\"Dallas\",\"31\":\"Dallas\",\"32\":\"Dallas\",\"33\":\"Dallas\",\"34\":\"Dallas\",\"35\":\"Dallas\",\"36\":\"Dallas\",\"37\":\"Dallas\",\"38\":\"Dallas\",\"39\":\"Dallas\",\"40\":\"Dallas\",\"41\":\"Dallas\",\"42\":\"Dallas\",\"43\":\"Dallas\",\"44\":\"Dallas\",\"45\":\"Dallas\",\"46\":\"Dallas\",\"47\":\"Dallas\",\"48\":\"Dallas\",\"49\":\"Dallas\",\"50\":\"Dallas\",\"51\":\"Dallas\",\"52\":\"Dallas\",\"53\":\"Dallas\",\"54\":\"Dallas\",\"55\":\"Dallas\",\"56\":\"Dallas\",\"57\":\"Dallas\",\"58\":\"Dallas\",\"59\":\"Dallas\",\"60\":\"Dallas\",\"61\":\"Dallas\",\"62\":\"Dallas\",\"63\":\"Dallas\",\"64\":\"Dallas\",\"65\":\"Dallas\",\"66\":\"Dallas\",\"67\":\"Dallas\",\"68\":\"Dallas\",\"69\":\"Dallas\",\"70\":\"Dallas\",\"71\":\"Dallas\",\"72\":\"Dallas\",\"73\":\"Dallas\",\"74\":\"Dallas\",\"75\":\"Dallas\",\"76\":\"Dallas\",\"77\":\"Dallas\",\"78\":\"Dallas\",\"79\":\"Dallas\",\"80\":\"Dallas\",\"81\":\"Dallas\",\"82\":\"Dallas\",\"83\":\"Dallas\",\"84\":\"Dallas\",\"85\":\"Dallas\",\"86\":\"Dallas\",\"87\":\"Dallas\",\"88\":\"Dallas\"},\"job_state\":{\"0\":\"TX\",\"1\":\"TX\",\"2\":\"TX\",\"3\":\"TX\",\"4\":\"TX\",\"5\":\"TX\",\"6\":\"TX\",\"7\":\"TX\",\"8\":\"TX\",\"9\":\"TX\",\"10\":\"TX\",\"11\":\"TX\",\"12\":\"TX\",\"13\":\"TX\",\"14\":\"TX\",\"15\":\"TX\",\"16\":\"TX\",\"17\":\"TX\",\"18\":\"TX\",\"19\":\"TX\",\"20\":\"TX\",\"21\":\"TX\",\"22\":\"TX\",\"23\":\"TX\",\"24\":\"TX\",\"25\":\"TX\",\"26\":\"TX\",\"27\":\"TX\",\"28\":\"TX\",\"29\":\"TX\",\"30\":\"TX\",\"31\":\"TX\",\"32\":\"TX\",\"33\":\"TX\",\"34\":\"TX\",\"35\":\"TX\",\"36\":\"TX\",\"37\":\"TX\",\"38\":\"TX\",\"39\":\"TX\",\"40\":\"TX\",\"41\":\"TX\",\"42\":\"TX\",\"43\":\"TX\",\"44\":\"TX\",\"45\":\"TX\",\"46\":\"TX\",\"47\":\"TX\",\"48\":\"TX\",\"49\":\"TX\",\"50\":\"TX\",\"51\":\"TX\",\"52\":\"TX\",\"53\":\"TX\",\"54\":\"TX\",\"55\":\"TX\",\"56\":\"TX\",\"57\":\"TX\",\"58\":\"TX\",\"59\":\"TX\",\"60\":\"TX\",\"61\":\"TX\",\"62\":\"TX\",\"63\":\"TX\",\"64\":\"TX\",\"65\":\"TX\",\"66\":\"TX\",\"67\":\"TX\",\"68\":\"TX\",\"69\":\"TX\",\"70\":\"TX\",\"71\":\"TX\",\"72\":\"TX\",\"73\":\"TX\",\"74\":\"TX\",\"75\":\"TX\",\"76\":\"TX\",\"77\":\"TX\",\"78\":\"TX\",\"79\":\"TX\",\"80\":\"TX\",\"81\":\"TX\",\"82\":\"TX\",\"83\":\"TX\",\"84\":\"TX\",\"85\":\"TX\",\"86\":\"TX\",\"87\":\"TX\",\"88\":\"TX\"},\"job_country\":{\"0\":\"US\",\"1\":\"US\",\"2\":\"US\",\"3\":\"US\",\"4\":\"US\",\"5\":\"US\",\"6\":\"US\",\"7\":\"US\",\"8\":\"US\",\"9\":\"US\",\"10\":\"US\",\"11\":\"US\",\"12\":\"US\",\"13\":\"US\",\"14\":\"US\",\"15\":\"US\",\"16\":\"US\",\"17\":\"US\",\"18\":\"US\",\"19\":\"US\",\"20\":\"US\",\"21\":\"US\",\"22\":\"US\",\"23\":\"US\",\"24\":\"US\",\"25\":\"US\",\"26\":\"US\",\"27\":\"US\",\"28\":\"US\",\"29\":\"US\",\"30\":\"US\",\"31\":\"US\",\"32\":\"US\",\"33\":\"US\",\"34\":\"US\",\"35\":\"US\",\"36\":\"US\",\"37\":\"US\",\"38\":\"US\",\"39\":\"US\",\"40\":\"US\",\"41\":\"US\",\"42\":\"US\",\"43\":\"US\",\"44\":\"US\",\"45\":\"US\",\"46\":\"US\",\"47\":\"US\",\"48\":\"US\",\"49\":\"US\",\"50\":\"US\",\"51\":\"US\",\"52\":\"US\",\"53\":\"US\",\"54\":\"US\",\"55\":\"US\",\"56\":\"US\",\"57\":\"US\",\"58\":\"US\",\"59\":\"US\",\"60\":\"US\",\"61\":\"US\",\"62\":\"US\",\"63\":\"US\",\"64\":\"US\",\"65\":\"US\",\"66\":\"US\",\"67\":\"US\",\"68\":\"US\",\"69\":\"US\",\"70\":\"US\",\"71\":\"US\",\"72\":\"US\",\"73\":\"US\",\"74\":\"US\",\"75\":\"US\",\"76\":\"US\",\"77\":\"US\",\"78\":\"US\",\"79\":\"US\",\"80\":\"US\",\"81\":\"US\",\"82\":\"US\",\"83\":\"US\",\"84\":\"US\",\"85\":\"US\",\"86\":\"US\",\"87\":\"US\",\"88\":\"US\"},\"job_google_link\":{\"0\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=HmRFsCRF1UsAAAAAAAAAAA%3D%3D\",\"1\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Qsa1ierlUMkAAAAAAAAAAA%3D%3D\",\"2\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=oAmKqPWYe9QAAAAAAAAAAA%3D%3D\",\"3\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=P7IfQYjEDhsAAAAAAAAAAA%3D%3D\",\"4\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=QwDdBZfCBAUAAAAAAAAAAA%3D%3D\",\"5\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=v6KgOCdJjGgAAAAAAAAAAA%3D%3D\",\"6\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=gbf_tKAAwsoAAAAAAAAAAA%3D%3D\",\"7\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=NgXphZWo5UwAAAAAAAAAAA%3D%3D\",\"8\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ICjkXTgkKKgAAAAAAAAAAA%3D%3D\",\"9\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=0&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=HCShKDrrNT8AAAAAAAAAAA%3D%3D\",\"10\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=aV5T0382CTgAAAAAAAAAAA%3D%3D\",\"11\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=uTkg0mApiRgAAAAAAAAAAA%3D%3D\",\"12\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=eMdLLHNlwWsAAAAAAAAAAA%3D%3D\",\"13\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=g9Rv4Q-2xh0AAAAAAAAAAA%3D%3D\",\"14\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=jAYPkmjSPLIAAAAAAAAAAA%3D%3D\",\"15\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=fgEVfs0XL3QAAAAAAAAAAA%3D%3D\",\"16\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=uGwOKTzWK8YAAAAAAAAAAA%3D%3D\",\"17\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=VoQSDjkz-2QAAAAAAAAAAA%3D%3D\",\"18\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=90&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=K392nkfmd28AAAAAAAAAAA%3D%3D\",\"19\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=xWoztRVZBtcAAAAAAAAAAA%3D%3D\",\"20\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=nKAVtQsHlHgAAAAAAAAAAA%3D%3D\",\"21\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ailo5hpdYyEAAAAAAAAAAA%3D%3D\",\"22\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=LuuL5FOvfNkAAAAAAAAAAA%3D%3D\",\"23\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=epgFodzuXh8AAAAAAAAAAA%3D%3D\",\"24\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=A2IULA4j8_8AAAAAAAAAAA%3D%3D\",\"25\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=iGjbaz-JoBcAAAAAAAAAAA%3D%3D\",\"26\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=_HaktzqqbQEAAAAAAAAAAA%3D%3D\",\"27\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=4YnqmBaYvqMAAAAAAAAAAA%3D%3D\",\"28\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=10&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Awvrb4W-sPUAAAAAAAAAAA%3D%3D\",\"29\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=eAOHe-qYmRwAAAAAAAAAAA%3D%3D\",\"30\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=xlranAfS5CkAAAAAAAAAAA%3D%3D\",\"31\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=H4fbvCM4qssAAAAAAAAAAA%3D%3D\",\"32\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=vw-9JOTKx18AAAAAAAAAAA%3D%3D\",\"33\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Fw73q18LK2YAAAAAAAAAAA%3D%3D\",\"34\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Xxd017dEv5YAAAAAAAAAAA%3D%3D\",\"35\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=AXlP4nh2A4AAAAAAAAAAAA%3D%3D\",\"36\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=XBgFtaGDEqoAAAAAAAAAAA%3D%3D\",\"37\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=9vfOrrNZWXwAAAAAAAAAAA%3D%3D\",\"38\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=20&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=tHzZ5aQ6XEYAAAAAAAAAAA%3D%3D\",\"39\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=_M1aY3afaAIAAAAAAAAAAA%3D%3D\",\"40\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=t0zy0CTqYuoAAAAAAAAAAA%3D%3D\",\"41\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=UQvc_eGI98wAAAAAAAAAAA%3D%3D\",\"42\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Zw8-q8mSkhsAAAAAAAAAAA%3D%3D\",\"43\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=zpHp8hO8r5gAAAAAAAAAAA%3D%3D\",\"44\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=n2SRAN_BDGAAAAAAAAAAAA%3D%3D\",\"45\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=30&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=hH0fMST3sloAAAAAAAAAAA%3D%3D\",\"46\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=LNndS8ll-f4AAAAAAAAAAA%3D%3D\",\"47\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=XJyrP7Ypp5EAAAAAAAAAAA%3D%3D\",\"48\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=iIPKNvIu2lsAAAAAAAAAAA%3D%3D\",\"49\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=GVXNPo1qmE4AAAAAAAAAAA%3D%3D\",\"50\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=6F_oejzQGBcAAAAAAAAAAA%3D%3D\",\"51\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=42NJql7l_7EAAAAAAAAAAA%3D%3D\",\"52\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=40&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=sli4eini4GgAAAAAAAAAAA%3D%3D\",\"53\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=8SyHUOO43CsAAAAAAAAAAA%3D%3D\",\"54\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=qIeZ_iz27YwAAAAAAAAAAA%3D%3D\",\"55\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=6VJKKZ1oBBkAAAAAAAAAAA%3D%3D\",\"56\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Hxm5v2wJMHUAAAAAAAAAAA%3D%3D\",\"57\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=0uC1tfLfQNoAAAAAAAAAAA%3D%3D\",\"58\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=hmPiuV0MhssAAAAAAAAAAA%3D%3D\",\"59\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=F0BzDvq8V2EAAAAAAAAAAA%3D%3D\",\"60\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=GZboUoqtmI4AAAAAAAAAAA%3D%3D\",\"61\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ziqGUCDHBPwAAAAAAAAAAA%3D%3D\",\"62\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=4cTj1j-ZIEwAAAAAAAAAAA%3D%3D\",\"63\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=50&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=pB9FEDB1j4sAAAAAAAAAAA%3D%3D\",\"64\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=7-6zJm6_MhYAAAAAAAAAAA%3D%3D\",\"65\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=fciHxK29OeAAAAAAAAAAAA%3D%3D\",\"66\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Zl5bZRbb03QAAAAAAAAAAA%3D%3D\",\"67\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=9Z1mzqAMpa8AAAAAAAAAAA%3D%3D\",\"68\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=JH3OAdcicPwAAAAAAAAAAA%3D%3D\",\"69\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=GjPMRB4MIFMAAAAAAAAAAA%3D%3D\",\"70\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=3KBM7t96SksAAAAAAAAAAA%3D%3D\",\"71\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=60&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=t_bwv82knGAAAAAAAAAAAA%3D%3D\",\"72\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=BTtXgAW3LgEAAAAAAAAAAA%3D%3D\",\"73\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=YWCs__E_bzUAAAAAAAAAAA%3D%3D\",\"74\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=mOyDU6ACctkAAAAAAAAAAA%3D%3D\",\"75\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=B9mwS3vp2lgAAAAAAAAAAA%3D%3D\",\"76\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=vQeezN_YDuwAAAAAAAAAAA%3D%3D\",\"77\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=rLA-OhanVgEAAAAAAAAAAA%3D%3D\",\"78\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=U3j2JDYImI0AAAAAAAAAAA%3D%3D\",\"79\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=70&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=ggGlVmkBy6AAAAAAAAAAAA%3D%3D\",\"80\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=gEFo3LqOIkEAAAAAAAAAAA%3D%3D\",\"81\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=VEeZH6OmNmcAAAAAAAAAAA%3D%3D\",\"82\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=248UdFpME3EAAAAAAAAAAA%3D%3D\",\"83\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=LWxudsMKTTMAAAAAAAAAAA%3D%3D\",\"84\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=Z3RRrMfrr9IAAAAAAAAAAA%3D%3D\",\"85\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=0OPIQUWCXxMAAAAAAAAAAA%3D%3D\",\"86\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=VxiGjPFk0QAAAAAAAAAAAA%3D%3D\",\"87\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=82l-Phq5MLUAAAAAAAAAAA%3D%3D\",\"88\":\"https:\\\\/\\\\/www.google.com\\\\/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+dallas,tx&start=80&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+dallas,tx&htidocid=K6j7paZxOOEAAAAAAAAAAA%3D%3D\"},\"job_highlights\":{\"0\":\"{\\'Qualifications\\': [\\'Experience writing stored procedures to ingest data from a spreadsheet (Excel) or SQL or other data sources\\', \\'Experience summarizing data and building those reporting tables and reviews, Tableaus and PowerBI\\', \\'Experience looking over the multiple databases they work with, main one being SQL but looking into the databases and the data that is in each database and understanding that well\\', \\'Working with APIs, ingesting data from APIs, writing scripts within this, Shell or Python scripts\\'], \\'Responsibilities\\': [\\'Experience building database views to summarize the data and pull the data\\', \\'Writing queries or packages to query the data and summarize it\\', \\'Data modeling designs \\\\/ custom development of data models so the data flows into Tableau and PowerBI correctly\\', \\'Making enhancements to the dashboards within Tableau and PowerBI\\', \\'They will receive requests for another view or metrics to add to the table or the column so need to be able to take those requests and implement them\\', \\'Will be sourcing from Hadoop platform as well and that work will continue to grow so ideally have experience within Hadoop\\']}\",\"1\":\"{\\'Qualifications\\': [\\\\\"An associate degree, a bachelor\\'s degree in computer science or equivalent courses\\\\\", \\'At least 4 years of experience in Data Engineering with SQL, Python\\', \\'Experience with relational SQL and NoSQL databases\\', \\'Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\\', \\'At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\\', \\'At least 2 years of experience with AWS cloud (with focus on Data services)\\'], \\'Responsibilities\\': [\\'The team is looking for a versatile Data Engineer who will provide data and report development services or technical support\\', \\'You will develop, test, and maintain data or report solutions (data warehouse\\\\/mart\\\\/stores\\\\/data lake\\\\/reporting\\\\/analytics) using tools and programming languages\\', \\'You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality\\', \\'As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members\\']}\",\"2\":\"{\\'Qualifications\\': [\\\\\"Option 1: Bachelor\\\\u2019s degree in Computer Science and 4 years\\' experience in software engineering or related field\\\\\", \\\\\"Option 3: Master\\'s degree in Computer Science and 2 years\\' experience in software engineering or related field\\\\\", \\\\\"3 years\\' experience in data engineering, database engineering, business intelligence, or business analytics\\\\\", \\'Nice to have soft skills \\\\u2013\\', \\'7+ years of experience with 3+ years of Big data development experience\\', \\'Experience in HDFS, Hive, Hive UDF\\\\u2019s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix\\', \\'Demonstrates expertise in writing complex, highly optimized queries across large data sets\\', \\'Retail experience and knowledge of commercial data is a huge plus\\'], \\'Responsibilities\\': [\\'Designs, develops, and implements Hadoop eco-system based applications to support business requirements\\', \\'Follows approved life cycle methodologies, creates design documents, and performs program coding and testing\\', \\'Resolves technical issues through debugging, research, and investigation\\']}\",\"3\":\"{\\'Qualifications\\': [\\\\\"Option 1: Bachelor\\'s degree in Computer Science and 4 years\\' experience in software engineering or related field\\\\\", \\\\\"Option 3: Master\\'s degree in Computer Science and 2 years\\' experience in software engineering or related field\\\\\", \\\\\"3 years\\' experience in data engineering, database engineering, business intelligence, or business analytics\\\\\", \\'Nice to have soft skills -\\', \\'7+ years of experience with 3+ years of Big data development experience\\', \\\\\"Experience in HDFS, Hive, Hive UDF\\'s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix\\\\\", \\'Demonstrates expertise in writing complex, highly optimized queries across large data sets\\', \\'Retail experience and knowledge of commercial data is a huge plus\\', \\'Experience level: Experienced\\', \\'Minimum 4 years of experience\\', \\'Education: Bachelors\\', \\'SQL\\'], \\'Responsibilities\\': [\\'work hours: 8am to 5pm\\', \\'Designs, develops, and implements Hadoop eco-system based applications to support business requirements\\', \\'Follows approved life cycle methodologies, creates design documents, and performs program coding and testing\\', \\'Resolves technical issues through debugging, research, and investigation\\'], \\'Benefits\\': [\\'salary: $62 - 72 per hour\\']}\",\"4\":\"{\\'Qualifications\\': [\\'Knowledge in AWS and management technologies such as S3\\', \\'Strong written communication skills\\', \\'5-10 years of experience within data engineering\\', \\'Python experience\\', \\'Snowflake experience (developing ETL pipelines)\\', \\'Kafka experience (loading batch and streaming data)\\', \\'Hybrid position - 2 days a week on-site at client office in downtown Dallas, TX\\', \\'Data Engineer, Python, IDMC, Snowflake, AWS, Dallas, ETL, ETL Pipeline, Kafka, RBAC controls, S3, ETL, AWS, Snowflake, IDMC, intelligent data management cloud, data engineer, hybrid\\'], \\'Responsibilities\\': [\\'This role will be part of a team focused on cloud transformation, modernizing analytics platforms and improving agility\\', \\'The role requires hands-on experience in building and managing analytics solutions in SnowFlake, Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of Data Engineering architecture and Development\\', \\'Establish Data Engineering architecture strategy, best practices, standards, and roadmap\\', \\'Experience developing ETL Pipeline using Python, Snowflake and IDMC\\', \\'Experience with loading batch data and streaming data via Kafka\\', \\'Build Data Flows mapping Source systems and Process flows\\', \\'Assemble large, complex data sets that meet non-functional and functional business requirements\\', \\'Mentor team members on best practices, efficient implementations and delivering high quality data products\\', \\'Lead onshore and offshore teams\\', \\'Perform code reviews and assist developers in optimization and troubleshooting\\', \\'Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features\\']}\",\"5\":\"{\\'Qualifications\\': [\\'Bachelor\\\\u2019s degree required and 5+ years of related experience in Data Engineering\\', \\'Solid experience in writing complex SQL queries on Snowflake (AWS) or Oracle and performance optimization for large data volumes\\', \\'Proficiency working with Python specifically as it relates to data processing\\', \\'Proven technical leadership in definition\\\\/support of standards and best practices\\', \\'Superior Data Modeling skills and experience performing deep data analysis on multiple database platforms\\', \\'Deep understanding of data warehousing techniques and methodologies\\', \\'Exposure and\\\\/or understanding Client\\\\u2019s enterprise data lake strategy\\', \\'Experience in Linux commands and basic shell scripting\\', \\'Experience in working with devops tools for code migrations\\', \\'Ability to develop ELT\\\\/ETL pipelines to move data to and from Snowflake data store using combination of Python and Snowflake SnowSQL\\', \\'Proven track record of working in collaborative teams to deliver high quality data solutions in a multi-developer agile environment following design & coding best practices\\', \\'Excellent communicator to both technical and non-technical data players to ensure common understanding of design to streamline and optimize data enablement\\', \\'Prior experience working in Agile software development environments, with proven ability to convert user stories into delivery work that provides incremental, iterative delivery of business value\\']}\",\"6\":\"{\\'Qualifications\\': [\\'About the Role: We are looking for an experienced Data Engineer with 6+ years of experience with Snowflake and Informatica cloud\\', \\'The data engineer should be somewhere between an engineer and an architect\\', \\\\\"Bachelor\\'s degree\\\\/University degree or equivalent experience\\\\\", \\'Experience in Informatica cloud, SQL Server\\', \\'Snowflake, creating pipes\\', \\'SQL & stored procedure development in MS SQL Server or Oracle\\', \\'Implementation experience in scripting\\\\/coding for automation (Windows Batch, Unix Shell, PowerShell Python or similar scripting languages)\\', \\'Bring your knowledge, unique viewpoint, and creativity to the table\\'], \\'Responsibilities\\': [\\\\\"Build the coolest tech for world\\'s leading brands\\\\\", \\'Solve complex problems - and learn new skills\\', \\'Experience the power of transforming digital engineering for Fortune 500 clients\\', \\'Master your craft with leading training programs and hands-on experience\\', \\'Manage & deliver solutions by liaising with business users, architects and understanding the business requirements\\', \\'Help design and develop technical and business solutions\\', \\'Should be able to coordinate & work with offshore developers\\', \\'Design, develop, test and support from SQL stored procedures, views, Informatica workflows, Control-M flows or script objects\\', \\'Executes unit and system test scripts, analyzes, captures and publishes test results\\', \\'Mentors junior staff members in all areas of database engineering\\', \\'Guide team members in setting expectations and monitor their progress\\', \\'Identifies opportunities to create efficiencies in technical work streams as well as in operational procedures of team\\'], \\'Benefits\\': [\\'Salary Range: The salary for this position is between $1,28,900- $1,41,400 annually\\', \\'Factors which may affect pay within this range may include geography\\\\/market, skills, education, experience and other qualifications of the successful candidate\\', \\\\\"This position is eligible for commissions in accordance with the terms of the Company\\'s plan\\\\\", \\'Additionally, this role is also eligible for bonus based on achievement of mutually agreed KRAs\\', \\'Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [personal days accrued each calendar year\\', \\'The Paid time off benefits meet the paid sick and safe time laws that pertains to the City\\\\/ State] [12-15 days (about 2 weeks) of paid vacation time] [6-8 weeks (about 2 months) of paid parental leave after a year of service] [9 paid holidays and 2 floating holidays per calendar year] [Ascendion Learning Management System] [Tuition Reimbursement Program]\\']}\",\"7\":\"{}\",\"8\":\"{\\'Qualifications\\': [\\\\\"An associate degree, a bachelor\\'s degree in computer science or equivalent courses\\\\\", \\'At least 4 years of experience in Data Engineering with SQL, Python\\', \\'Experience with relational SQL and NoSQL databases\\', \\'Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\\', \\'At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\\', \\'At least 2 years of experience with AWS cloud (with focus on Data services)\\', \\'Equivalent education and\\\\/or experience may be substituted for any of the above requirements\\'], \\'Responsibilities\\': [\\'The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support\\', \\'You will develop, test, and maintain data or report solutions (data warehouse\\\\/mart\\\\/stores\\\\/data lake\\\\/reporting\\\\/analytics) using tools and programming languages\\', \\'You will also develop data set processes and assist with design and identify ways to improve data reliability, efficiency, and quality\\', \\'As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members\\', \\'Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns\\', \\'Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity\\', \\'Work under general guidance and clear framework of accountability with substantial autonomy\\', \\'Use best practices and knowledge of internal or external business issues to improve products or services\\', \\'Solve complex problems; takes a new perspective using existing solutions\\', \\'Able to interpret user requirements and identify additional information needed in user requirements\\', \\'Able to see effects of current design with future requirements and see possible coding solutions to meet the requirements\\', \\'Detailed understanding of logical and physical data structures\\', \\'and discuss with the team\\', \\'Highly skilled at designing tests for unfamiliar designs; Evaluates tests for weaknesses and continuously improves them\\'], \\'Benefits\\': [\\'Also, some positions may include bonuses or other incentives\\']}\",\"9\":\"{\\'Qualifications\\': [\\'At least 7 years of data engineering experience developing large data pipelines\\', \\'Strong SQL skills and ability to create queries to extract data and build performant datasets\\', \\'Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data\\', \\'Strong programming skills in Python\\', \\'Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)\\', \\'Solid experience with data integration toolsets (ie Airflow) and writing and maintaining Data Pipelines\\', \\'Strong in Data Modeling techniques and Data Warehousing standard methodologies and practices\\', \\'Familiar with Scrum and Agile methodologies\\', \\'You are a problem solver with strong attention to detail and excellent analytical and communication skills\\', \\'Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2) Required Education\\', \\\\\"Bachelor\\'s or Master\\'s Degree in Computer Science, Information Systems or related field Additional Information\\\\\"], \\'Responsibilities\\': [\\'Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science\\', \\'Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)\\', \\'Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala\\', \\'Help architect data solutions\\\\/frameworks and define data models for the underlying data warehouse and data marts\\', \\'Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions\\', \\'Maintain detailed documentation of your work and changes to support data quality and data governance\\', \\'Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)\\', \\'Be an active participant and advocate of agile\\\\/scrum practice to ensure health and process improvements for your team Key Qualifications\\'], \\'Benefits\\': [\\\\\"The base pay actually offered will take into account internal equity and also may vary depending on the candidate\\'s geographic region, job-related knowledge, skills, and experience among other factors\\\\\", \\'A bonus and\\\\/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and\\\\/or other benefits, dependent on the level and position offered\\']}\",\"10\":\"{\\'Qualifications\\': [\\'5 years\\', \\'Dallas, TX: Reliably commute or planning to relocate before starting work (Required)\\', \\'CI\\\\/CD: 3 years (Required)\\', \\'Azure DevOps: 4 years (Required)\\', \\'Azure Synapse: 3 years (Required)\\'], \\'Benefits\\': [\\'CONTRACT with the candidate directly with an extension option after initial 6 months of employment.*\\', \\'Pay: From $65.00 per hour\\', \\'Compensation package:\\', \\'Monday to Friday\\']}\",\"11\":\"{\\'Qualifications\\': [\\'7+ years of production data engineering experience\\', \\'Strong technical skills in python, SQL, and data modeling\\', \\'Experience with data warehouses, E(LT\\\\/TL) tools, and cloud services\\', \\'Strong sense of ownership over your work\\', \\'Comfortable working at startup pace and focus\\', \\'Read English, comprehend, and follow simple oral and written instructions\\', \\'The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading\\', \\'Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and\\\\/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body\\', \\'Walking & standing are required occasionally\\'], \\'Responsibilities\\': [\\'ETL, data visualization, and metadata platforms (fivetran, snowflake, dbt, looker, datahub) that power business analytics and decision making\\', \\'ML platform (kubeflow) powering the models that automate and optimize pharmacy operations\\', \\'Data integrations with healthcare networks, drug manufacturers, and other partnerships\\', \\'Assessing the accuracy, neatness and thoroughness of the work assigned\\', \\'Communicating with others to exchange information\\', \\'Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound\\', \\'Frequent repeating motions required to operate a computer that may include the wrists, hands and\\\\/or fingers\\'], \\'Benefits\\': [\\'Salary and Benefits\\', \\'Salary Range: $159,000 - $199,000\\', \\'Commission Eligible: No\\', \\'Benefits: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD\\\\/LTD, generous PTO and parental leave\\']}\",\"12\":\"{\\'Qualifications\\': [\\'We are seeking an experienced Senior Data Engineer who will work with clients and members of the consulting team on the architecture, design, and development of highly scalable data integration and data engineering processes\\', \\'The Senior Consultant must have a strong understanding and experience with data & analytics solution architecture, including data warehousing, data lakes, ETL\\\\/ELT workload patterns, and related BI & analytics systems\\', \\'3+ years hands-on experience with one or more of these data integration\\\\/ETL tools:\\', \\'Experience building on-prem data warehousing solutions\\', \\\\\"Experience with designing and developing ETL\\'s, Data Marts, Star Schema\\'s\\\\\", \\'Moving data from on-prem to cloud\\', \\'Designing a data warehouse solution using Synapse or Azure SQL DB\\', \\'Experience building pipelines using Synapse or Azure Data Factory to ingest data from various sources\\', \\'Understanding of integration run times available in Azure\\', \\'Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases\\', \\'Knowledge of scripting languages like Python, Scala\\', \\'Microsoft Azure Cloud platform certifications (nice to have)\\', \\'Must be able to travel to client locations based on project needs\\'], \\'Responsibilities\\': [\\'Deliver consulting projects\\\\/work on-time, on-budget, and in a way that accomplishes client goals\\', \\'Develop and implement technical best practices for data ingestion, data quality, data cleansing, and other data integration\\\\/ETL\\\\/Engineering-related activities\\', \\'Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and modern cloud technologies\\', \\'Conduct or participate in meetings with owners of key system components to fully understand current data and systems environments\\', \\'Resolve source data issues and refine transformation rules\\', \\'Analyze source system data to assess transformation logic and data quality through data profiling\\', \\'Leverage data quality processes to assist with data cleansing requirements\\', \\'Work with technical and business representatives to determine strategies for handling data anomalies that are identified\\', \\'Design ETL processes and develop source-to-target data mappings, integration workflows, and load processes\\', \\'Develop, test, integrate, and deploy data pipelines using a variety of tools and external programming\\\\/scripting languages as necessary\\', \\'Provide technical documentation and other artifacts for data pipelines, ingestion, integration or other data solutions\\', \\'Identify problems, develop ideas and propose solutions within differing situations requiring analytical, evaluative or constructive thinking in daily work\\', \\'Apply creative thinking to identify possible reporting solution alternatives\\', \\'Other duties assigned as needed\\'], \\'Benefits\\': [\\'Salary: $125,000 - $150,000 per year\\', \\'You will be eligible for full-time benefits during the contract period\\']}\",\"13\":\"{\\'Qualifications\\': [\\'Java, Python or PySpark hands on development experience\\'], \\'Responsibilities\\': [\\'AWS cloud\\\\u2014KMS, S3, Glue, Lambda etc\\', \\'Deployed data pipelines\\', \\'Exposure to even driven streaming\\']}\",\"14\":\"{\\'Qualifications\\': [\\'Experience in Data Vault modeling approach\\', \\'Extensive knowledge of Snowflake\\', \\'Good knowledge of metadata management, data modeling, and related tools (Erwin or ER Studio or others) required\\'], \\'Responsibilities\\': [\\'Responsible for the development of the conceptual, logical, and physical data models\\', \\'Implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms\\', \\'Define and govern data modeling and design standards, tools, best practices, and related development for enterprise data models\\', \\'Hands-on modeling, design, configuration, installation, performance tuning,\\', \\'Interact with SMEs (business) and Architect to validate whether the data model is aligned with the technology suggested for the project\\', \\'Relay the information gained from onshore to the team and offshore for seamless collaboration and desired outcome\\']}\",\"15\":\"{\\'Qualifications\\': [\\'Visa: Only GC\\\\/USC -will need GC copy \\\\/citizenship proof\\', \\'Must have a valid LinkedIn profile\\', \\'Must be local and go onsite 2-3 days\\\\/week - Charlotte NC, Dallas TX, Chandler AZ, Newark, NJ\\', \\'Terraform -5+ years\\', \\'Azure -4+ years\\', \\'Data Lake- 4+ years\\', \\'Data Architecture -2+ years\\', \\'We are seeking Cloud DB Platform Engineers with Cloud and Data Engineering experience\\', \\'Terraform or Kubernetes, Azure\\\\/GCP, DevOps Engineering, ETL , Client\\\\/AI, Data lake, Data architecture\\']}\",\"16\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s Degree in a relevant field required\\\\\", \\'Min 5 years of experience in data engineering or a related field\\', \\'Proficiency in one or more programming languages such as Python, Java, or Scala\\', \\'Experience with data processing and storage technologies such as Hadoop, Spark, Kafka, Snowflake, and NoSQL databases\\', \\'Prior experience managing a team of direct reports within the Data Science, Data Engineering, Analytics space in the SFR or Multifamily industry\\', \\'Significant Experience building, motivating, and retaining a high- performing, flexible and collaborative data and analytics function\\', \\'Proven hands-on technical background in data science, business intelligence or data engineering with demonstrated strategic impact at an executive level\\', \\'A strong problem solver with experience building technical strategy and understanding technical tradeoffs and risk\\', \\'Collaborative team player, you are truly a \\\\\"do-er\\\\\", happy to be a hands-on problem-solver to move the data program forward\\', \\'Excellent communication skills \\\\u2013 verbal and written\\'], \\'Responsibilities\\': [\\'You will design and support data warehouse systems, perform data extraction and ensure data accuracy, enabling real-time insights from both internal and market data that will drive revenue growth and capital efficiency\\', \\'Initial focus is to work with external data purchased, and to harvest internal data and work within Snowflake warehouse for use in our 3rd party property mgt system and BI reporting tool\\', \\'Overall ensure there is one source of truth\\', \\'Designing and implementing data pipelines to extract, transform, and load data from various sources into a centralized data repository\\', \\'Developing and maintaining data processing and storage infrastructure\\', \\'Establish productive relationships and effective communications with Company leadership to understand business drivers and align on required outcomes\\', \\'Collaborating with data analysts to ensure that data is readily available for analysis and modeling\\', \\'Optimizing database performance and troubleshooting issues as they arise\\', \\'Implementing data security and access controls to protect sensitive data\\', \\'Highlight key trends derived from data analysis and be a resource for improving data proficiency throughout the organization\\', \\'Staying up-to-date with emerging trends and technologies in data engineering\\', \\'Leverage historical data and predictive models to identify key historical factors that impact critical KPIs, and recommend actions to drive future performance\\', \\'Ensure scientific method and research are key drivers of the product roadmap\\']}\",\"17\":\"{\\'Qualifications\\': [\\'Required Experience : 06 - 12 Years\\', \\'The ideal candidate is highly skilled in MSBI-SSIS and has experience in dealing with large-scale data extraction and manipulation\\', \\'Bachelor\\\\u2019s degree in Computer Science or related field\\', \\'6+ years of experience in MSBI-SSIS development\\', \\'Expertise in data integration and data manipulation\\', \\'Experience with SQL databases and query optimization\\', \\'Knowledge of data warehousing concepts and ETL design patterns\\', \\'Knowledge of data profiling and data cleansing\\', \\'Experience with SSRS reporting\\', \\'Excellent problem-solving, communication, and analytical skills\\', \\'Ability to work independently and in a team environment\\'], \\'Responsibilities\\': [\\'The Data Engineer will be responsible for designing and developing SSIS packages to extract and manipulate data from multiple sources\\', \\'The Engineer will also be responsible for managing and troubleshooting data flows, ensuring the data is correct and within the required formats\\', \\'Design, develop, and maintain ETL packages using MSBI-SSIS\\', \\'Develop data dictionary for the data sources and target systems\\', \\'Analyze data requirements and develop solutions to meet them\\', \\'Troubleshoot and debug data integration issues\\', \\'Proficient in creating jobs, packages, and stored procedures in MSBI-SSIS\\', \\'Design and develop data models to support the data analysis process\\', \\'Ensure data consistency, integrity, and security\\', \\'Develop and maintain data related documentation\\', \\'Develop and maintain SSIS packages for data integration and migration\\', \\'Develop and maintain SSRS reports\\', \\'Perform data profiling and data cleansing\\', \\'Monitor performance and optimize ETL processes\\', \\'Prepare data for reporting and analysis\\', \\'Work with BI teams to ensure data accuracy\\', \\'Assist with other data related tasks as needed\\'], \\'Benefits\\': [\\'Role-based Training programs\\', \\'Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional\\', \\'We encourage you to acquire various beneficial international certifications, with costs s reimbursed\\', \\'Excellent benefits plan: medical, dental, vision, life, FSA, & PTO\\', \\'Roll over vacation days\\', \\'Commuter benefits\\', \\'Excellent growth and advancement opportunities\\', \\'Rewards and recognition programs\\', \\'Innovative and collaborative company culture\\']}\",\"18\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s degree in computer science, Information Technology, or a related field\\\\\", \\'At least 10 years of experience in data engineering, with at least 5 years in a leadership role managing teams that have spanned across multiple international Time zones\\', \\'Strong technical skills in data engineering, including experience with data architecture, data pipelines, data platforms, and data integration tools, with a focus on cloud-based technologies such as Informatica Cloud, Azure Data Factory with a Strong mindset on Building the softwares as Scalable micro services or micro components\\', \\'Experience with Informatica SaaS and other relevant cloud-based technologies such as Snowflake, Databricks, and Apache Kafka\\', \\'Strong leadership skills, with experience in managing large data sets and engineering teams\\', \\'Should be a master in delegating, guiding, and completing\\', \\'Excellent communication and interpersonal skills, with the ability to work effectively with cross-functional teams\\', \\'Strong problem-solving and analytical skills\\', \\'Ability to clearly understand priority under pressure, strategize, guide, and develop scalable solutions\\', \\'Experience leveraging AI for development, using machine learning and other AI technologies to improve data quality, automate processes, and enhance the data engineering pipeline\\', \\'Ability to work under pressure and navigate multiple priorities in a fast-paced environment\\', \\'(Eg: Devops, Jira or GitHub)\\', \\'Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future\\'], \\'Responsibilities\\': [\\'The Data Engineering Manager will be responsible for overseeing the development and maintenance of our data engineering architecture, data pipelines, and data platforms, and must have experience in managing data engineering teams and working with the latest cloud based and Informatica SaaS technologies\\', \\'He\\\\/she will have a passion for data with a keen interest to not only understand the technical components, but also understand the business context and impact of technical solutions that are provided\\', \\'The candidate can prioritize competing requirements, strategize development, guide in developing scalable solutions, and leverage AI for development\\', \\'Lead and manage the internal Data Engineering Team, including hiring, training, coaching\\\\/mentoring, and performance management\\', \\'Develop resource plans to determine the right mix of internal and external resources; engage and manage 3rd party consultants\\\\/contractors as needed\\', \\\\\"Develop and maintain the data engineering architecture and design to support the company\\'s business objectives and strategy\\\\\", \\'Develop and maintain data pipelines and data platforms using cloud-based technologies such as Informatica Data Management Cloud and Azure Data Factory ensuring data quality and reliability\\', \\'Work collaboratively with cross-functional teams and businesses including product management, data science, and analytics teams, to deliver data solutions that meet business needs\\', \\'Develop and maintain standards, policies, and procedures for data engineering best practices\\', \\'Develop and maintain documentation for data engineering processes and procedures\\', \\'Implement and manage best SDLC practices including agile CI\\\\/CD, automation\\', \\'Manage budgets, timelines, and resources for data engineering projects\\', \\'Clearly understand priority under pressure, strategize, guide, and develop scalable solutions\\', \\'Have knowledge to leveraging AI for development, using machine learning and other AI technologies to improve data quality, automate processes, and enhance the data engineering pipeline\\'], \\'Benefits\\': [\\'Time Off: 20 days of PTO for full-time employees and 12 company holidays\\', \\'Summer Fridays: July 4th through Labor Day, the office is completely closed\\\\/offline every other Friday\\', \\'Company Paid Benefits: Life insurance, Short-term disability, Long-term disability, Paid parental leave, Employee Assistance Program, and medical insurance in our high deductible health plan\\', \\'Optional Employee Paid Benefits:Medical insurance in our EPO plan, Dental benefits, and Vision benefits\\', \\'We also offer Health Savings Accounts, Flexible Spending Accounts, Supplemental Life insurance, and more\\', \\'401(k): Eligible after 60 days\\', \\'Discretionary company match of 50% up to the first 6% of contributions\\']}\",\"19\":\"{\\'Qualifications\\': [\\'At least 7 years of data engineering experience developing large data pipelines\\', \\'Strong SQL skills and ability to create queries to extract data and build performant datasets\\', \\'Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data\\', \\'Strong programming skills in Python\\', \\'Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)\\', \\'Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines\\', \\'Strong in Data Modeling techniques and Data Warehousing standard methodologies and practices\\', \\'Familiar with Scrum and Agile methodologies\\', \\'You are a problem solver with strong attention to detail and excellent analytical and communication skills\\', \\'Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)\\', \\\\\"Bachelor\\'s or Master\\'s Degree in Computer Science, Information Systems or related field\\\\\"], \\'Responsibilities\\': [\\'Contribute to the design and growth of our Data Products and Data Warehouses around Engagement and Retention Analytics and Data Science\\', \\'Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)\\', \\'Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow and languages include Python, Scala\\', \\'Help architect data solutions\\\\/frameworks and define data models for the underlying data warehouse and data marts\\', \\'Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions\\', \\'Maintain detailed documentation of your work and changes to support data quality and data governance\\', \\'Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)\\', \\'Be an active participant and advocate of agile\\\\/scrum practice to ensure health and process improvements for your team\\'], \\'Benefits\\': [\\\\\"The base pay actually offered will take into account internal equity and also may vary depending on the candidate\\'s geographic region, job-related knowledge, skills, and experience among other factors\\\\\", \\'A bonus and\\\\/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and\\\\/or other benefits, dependent on the level and position offered\\']}\",\"20\":\"{\\'Qualifications\\': [\\'Looking for Junior level resource\\', \\'scripting (python or powershell), general knowledge of AWS\\', \\'will be touching multiple technologies - someone that is ok to pick up and learn\\', \\'Need to have a great attitude - they can coach, teach mentor in areas that they may be lacking\\']}\",\"21\":\"{\\'Qualifications\\': [\\'Bachelor\\\\u2019s Degree or equivalent in a technology related field (e.g. Computer Science, Engineering, etc.) required\\', \\'10+ years of hands-on experience in Data engineering, data warehousing and analytics technologies\\', \\'Proven ability with modern Object-Oriented Programming Languages (Python, Scala, Java)\\', \\'Advanced skills in data intensive application development, data integration, and data pipeline design patterns on a distributed platform\\', \\'Strong knowledge of designing data engineering solutions and platforms\\', \\'Working experience with Relational Databases like Oracle\\', \\'Experience with business intelligence and analytics tools such as OBIEE, PowerBI or Tableau\\', \\'Strong working knowledge of cloud native data warehousing and data lake solutions using Snowflake, Redshift etc\\', \\'Experience building scalable patterns for data consumption from cloud based data-lakes\\', \\'Advanced experience with PL\\\\/SQL and complex queries\\', \\'Solid understanding of Cloud Computing and DevOps concepts including CI\\\\/CD pipelines\\', \\'Significant experience with ELT data integration and data movement design patterns\\', \\'Working experience of NoSQL and BigData technologies such as e.g. Hadoop, HBase, MongoDB, Cassandra, etc\\', \\'Ability and passion for leading tech teams and mentor junior engineers\\', \\'Your ability to collaborate with other technical and business minds in the organization\\', \\'Ability to learn and experiment with new technologies and patterns\\', \\'Your penchant for modern test driven and automation driven software development methodologies\\', \\'Expertise in converting technology goals into achievable initiatives and Epics and stories\\', \\'Experience in executing projects in an Agile environment\\'], \\'Responsibilities\\': [\\'This person will be playing a key role in designing and crafting a modern Data and Information Delivery and Analytics platform in the cloud to support Equity Compensation products for the Global markets\\', \\'This person will be working closely with architects and engineers and business SMEs to build and release solutions that help customers get the information they need fast and intuitively\\'], \\'Benefits\\': [\\'You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home\\']}\",\"22\":\"{\\'Qualifications\\': [\\'REQUIREMENTS: Requires a Masters Degree, or foreign equivalent degree, in Electrical and Electronic Engineering, Computer Science, or Computer Engineering and three (3) years of experience in the job offered or three (3) years of experience in a related occupation creating tasks for Data Replication and Data Synchronization; utilizing Oracle, Teradata, Vertica, Azure DataLake, Databricks, Snowflake and Palantir Foundry; utilizing Hbase and Hbase Shell; developing UNIX shell scripts; developing database load scripts: VSQL for Vertica; utilizing BTEQ, Mload, Fastload and fast export scripts for Teradata; utilizing SnowSQL for Snowflake and PySpark for Databricks; and developing schedules using workload scheduling tools: TWS\\'], \\'Responsibilities\\': [\\\\\"DUTIES: Interpret the requirements of various big data analytic use cases and scenarios, and drive the design and implementation of specific data models to ultimately help drive better business decisions through insights from a combination of external and AT&T\\'s data assets\\\\\", \\'Develop necessary enablers and data platform in the big data lake environment and has the responsibility of maintaining its integrity during the life cycle phases\\', \\'Define data requirements, gather and mine large scale of structured and unstructured data, and validate data by running various data tools in the big data environment\\', \\'Support the standardization, customization and ad-hoc data analysis, and develop the mechanisms in ingest, analyze, validate, normalize and clean data\\', \\'Implement statistical data quality procedures on new data sources, and apply rigorous iterative data analytics\\', \\'Support data scientists in data sourcing and preparation to visualize data and synthesize insights of commercial value\\', \\'Work with big data policy and security teams and legal to create data policy and develop interfaces and retention models which requires synthesizing or anonymizing data\\', \\'Develop and maintain data engineering best practices and contribute to insights on data analytics and visualization concepts, methods and techniques\\', \\'Technical design to ingest data into Palantir Foundry platform on Azure\\', \\'Participate in creating ingestion strategy and technology patterns\\', \\'Provide technical direction (architecture and design) for projects ingesting data into Palantir Foundry\\', \\'Conduct design, architecture and code reviews\\', \\'Engage with the vendor to meet AT&T requirements and deliverables\\', \\'Create tasks for Data Replication and Data Synchronization\\', \\'Develop database load scripts: VSQL for Vertica\\', \\'Utilize BTEQ, Mload, Fastload and fast export scripts for Teradata\\', \\'Utilize SnowSQL for Snowflake and PySpark for Databricks\\', \\'Develop schedules using workload scheduling tools: TWS\\'], \\'Benefits\\': [\\'Our Principal-Big Data Engineers earn between $158,200 - $254,300 yearly\\', \\'Medical\\\\/Dental\\\\/Vision coverage\\', \\'401(k) plan\\', \\'Tuition reimbursement program\\', \\'Paid Time Off and Holidays (based on date of hire, at least 23 days of vacation each year and 9 company-designated holidays)\\', \\'Paid Parental Leave\\', \\'Paid Caregiver Leave\\', \\'Additional sick leave beyond what state and local law require may be available but is unprotected\\', \\'Adoption Reimbursement\\', \\'Disability Benefits (short term and long term)\\', \\'Life and Accidental Death Insurance\\', \\'Supplemental benefit programs: critical illness\\\\/accident hospital indemnity\\\\/group legal\\', \\'Extensive employee wellness programs\\', \\'Employee discounts up to 50% off on eligible AT&T mobility plans and accessories, AT&T internet (and fiber where available) and AT&T phone\\']}\",\"23\":\"{\\'Qualifications\\': [\\'Proven experience as a Software Developer\\\\/Data Engineer with expertise in Apache Spark, GCP, Azure, Databricks, SQL, Kafka Connect (Java), and Java data structures\\', \\'Strong knowledge of data engineering concepts, data integration, and data processing\\', \\'Experience with ETL development and data pipelines\\', \\'Familiarity with cloud platforms like GCP and Azure for data storage and processing\\', \\'*ALL successful candidates for this position are required to work directly for PRIMUS\\'], \\'Responsibilities\\': [\\'As a Software Developer\\\\/Data Engineer, you will be responsible for developing and implementing data engineering solutions, with a focus on utilizing Apache Spark, GCP (Google Cloud Platform), Azure, Databricks (batch processing), SQL, Kafka Connect (Java), and data structures in Java\\']}\",\"24\":\"{\\'Qualifications\\': [\\'Experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC etc.)\\', \\'At least 3 years of experience working with large data sets, streaming, experience working with distributed computing (Map Reduce, Hadoop, Hive, Apache Spark, Apache Kafka etc.)\\', \\'At least 2 years of experience with AWS cloud (with focus on Data services)\\', \\'U.S. citizenship required\\', \\'The team is is looking for a versatile Data Engineer who will provide data and report development services or technical support\\', \\'An associate degree, a bachelor\\\\u2019s degree in computer science or equivalent courses\\', \\'At least 4 years of experience in Data Engineering with SQL, Python\\', \\'Experience with relational SQL and NoSQL databases\\', \\'Experience with RESTful API development, Familiarity with HTTP and invoking web-APIs\\', \\'Working knowledge of data structures, SQL, XML, JSON, Data visualization tools, Version Control Systems, Programming, and Unix\\\\/Linux shell scripting\\', \\'Highly skilled in tools, evaluates the need for various tools for continuous integration, testing, automation, deployment etc\\', \\'Equivalent education and\\\\/or experience may be substituted for any of the above requirements\\'], \\'Responsibilities\\': [\\'As the Data Engineer you will work independently, receive minimal guidance, and have accountability for their work and work of junior members\\', \\'Able to interpret user requirements and identify additional information needed in user requirements\\', \\'Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns\\', \\'Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity\\', \\'Work under general guidance and clear framework of accountability with substantial autonomy\\', \\'Use best practices and knowledge of internal or external business issues to improve products or services\\', \\'Solve complex problems; takes a new perspective using existing solutions\\']}\",\"25\":\"{\\'Qualifications\\': [\\\\\"Option 1: Bachelor\\'s degree in Computer Science and 4 years\\' experience in software engineering or related field\\\\\", \\\\\"Option 3: Master\\'s degree in Computer Science and 2 years\\' experience in software engineering or related field\\\\\", \\\\\"3 years\\' experience in data engineering, database engineering, business intelligence, or business analytics\\\\\", \\'Nice to have soft skills -\\', \\'7+ years of experience with 3+ years of Big data development experience\\', \\\\\"Experience in HDFS, Hive, Hive UDF\\'s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix\\\\\", \\'Demonstrates expertise in writing complex, highly optimized queries across large data sets\\', \\'Retail experience and knowledge of commercial data is a huge plus\\', \\'Experience level: Experienced\\', \\'Minimum 4 years of experience\\', \\'Education: Bachelors\\', \\'SQL\\', \\'TableauEqual Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group ~~~ Randstad, we welcome people of all abilities and want to ensure that our hiring and interview process meets the needs of all applicants\\'], \\'Responsibilities\\': [\\'work hours: 8am to 5pm\\', \\'Designs, develops, and implements Hadoop eco-system based applications to support business requirements\\', \\'Follows approved life cycle methodologies, creates design documents, and performs program coding and testing\\', \\'Resolves technical issues through debugging, research, and investigation\\'], \\'Benefits\\': [\\'salary: $62 - 72 per hour\\', \\'In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility)\\']}\",\"26\":\"{\\'Qualifications\\': [\\'An ideal candidate will have experience in coding large-scale, responsive web sites with an aim towards performance and progressive enhancement\\', \\'We are looking to fill out a Software Engineer position requiring solid knowledge of software engineering and architectural best practices\\', \\'Qualified candidates should have solid coding experience and be eager and willing to learn\\', \\'You will be asked to prove your skills during the technical interview\\', \\'Technical lead with data science experience who can also support the team from an architectural standpoint\\', \\'- GCP\\', \\'- Solution Architect Experience\\', \\'B.S. in Computer Science or related field, with 10 years\\\\u2019 work experience\\', \\'Hands-on experience working in Unix\\\\/Linux ecosystem\\', \\'Experience with developing or maintaining ETL pipelines in a Big Data Hadoop environment\\', \\'Experience in setting up and maintaining CI\\\\/CD infrastructure\\', \\'Experience with test automation\\'], \\'Responsibilities\\': [\\'Incedo Software Solution Architect\\\\/Solution Lead play a critical role in developing solutions for client engagements\\', \\'You will work on the Big Data platform at the one of the telecom networks scale (hundreds of terabytes of data per day)\\', \\'You will design and develop solutions for Big Data challenges, working with applications deployed on-premise and in the Google Cloud\\']}\",\"27\":\"{\\'Qualifications\\': [\\'You have consistently high standards, your passion for quality is inherent in everything\\', \\'Well versed with Hadoop, Spark, Cloud, Python\\\\/Scala and Java, Streaming, Kafka, Backend, J2EE\\', \\'You evangelize an extremely high standard of code quality, system reliability, and performance\\', \\'You have a proven track record coding with at least one programming language (e.g., Scala, Python)\\', \\'You\\\\u2019re experienced in one of cloud computing platforms (e.g., GCP, Azure)\\', \\'You\\\\u2019re skilled in data modeling & data migration protocols\\', \\'Experience with the integration tools like Automic, Airflow\\', \\\\\"Option 1: Bachelor\\\\u2019s degree in Computer Science and 3 years\\' experience in software engineering or related field\\\\\", \\'Option 2: 5 years\\\\u2019 experience in\\', \\\\\"2 years\\' experience in data engineering, database engineering, business intelligence, or business analytics\\\\\", \\\\\"Master\\\\u2019s degree in Computer Science, Computer Engineering, Computer Information Systems, Software Engineering, or related area and 1 year\\'s experience in software engineering or related area\\\\\"], \\'Responsibilities\\': [\\'You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way.\\\\\\\\u202fYou will partner with Data Scientists, Analysts, other engineers, and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers\\\\u2019 lives\\', \\'Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function\\', \\'Data Transformation and Integration: Extracts data from identified databases\\', \\'Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques\\', \\'Develops knowledge of current analytics trends\\', \\'Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements\\', \\'Helps identify the most suitable source for data that is fit for purpose\\', \\'Performs initial data quality checks on extracted data\\', \\'Data Modeling: Analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models\\', \\'Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs\\', \\'Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure\\', \\'Evaluates existing data models and physical databases for variances and discrepancies\\', \\'Develops efficient data flows\\', \\'Analyses data-related system integration challenges and proposes appropriate solutions\\', \\'Code Development and Testing: Writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical and data requirements\\', \\'Creates test cases to review and validate the proposed solution design\\', \\'Creates proofs of concept\\', \\'Tests the code using the appropriate testing approach\\', \\'Deploys software to production servers\\', \\'Contributes code documentation, maintains playbook, and provides timely progress updates\\', \\\\\"Problem Formulation: Translates business problems within one\\'s discipline to data related or mathematical solutions\\\\\", \\'Identifies what methods (for example, analytics, big data analytics, automation) would provide a solution for the problem\\', \\'Shares use cases and gives examples to demonstrate how the method would solve the business problem\\', \\'Applied Business Acumen: Provides recommendations to business stakeholders to solve complex business issues\\', \\'Develops business cases for projects with a projected return on investment or cost savings\\', \\'Translates business requirements into projects, activities, and tasks and aligns to overall business strategy\\', \\'Serves as an interpreter and conduit to connect business needs with tangible solutions and results\\', \\'Recommends new processes and ways of working\\', \\'Data Governance: Establishes, modifies, and documents data governance projects and recommendations\\', \\'Implements data governance practices in partnership with business stakeholders and peers\\', \\'Interprets company and regulatory policies on data\\', \\'Educates others on data governance processes, practices, policies, and guidelines\\', \\'Provides recommendations on needed updates or inputs into data governance policies, practices, or guidelines\\', \\'Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others\\', \\'Supporting and aligning efforts to meet customer and business needs and building commitment for perspectives and rationales\\', \\'Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders\\', \\'Identifying business needs, determining, and carrying out necessary processes and practices\\', \\'Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application, ensuring compliance with them\\', \\'Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives\\', \\'Applying suggestions for improving efficiency and cost effectiveness; and participating in and supporting community outreach events\\', \\'Creates training documentation and trains end-users on data modeling\\', \\'Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports\\', \\'Drives the execution of multiple business plans and projects by identifying customer and operational needs\\', \\'Developing and communicating business plans and priorities, removing barriers and obstacles that impact performance\\', \\'Providing resources, identifying performance standards, measuring progress, and adjusting performance accordingly\\', \\'Developing contingency plans and demonstrating adaptability and supporting continuous learning\\'], \\'Benefits\\': [\\'Benefits: Beyond our great compensation package, you can receive incentive awards for your performance\\', \\'Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more\\']}\",\"28\":\"{\\'Qualifications\\': [\\'Experience level: Experienced\\', \\'Minimum 4 years of experience\\', \\'Education: Associates\\', \\'data engineer\\', \\'SQL (4 years of experience is required)\\', \\'Big Data\\', \\'AWS\\', \\'UNIX\\', \\'Linux\\', \\'Data Warehouse\\'], \\'Responsibilities\\': [\\'Design, develop and implement data mining tools and analyses to sift through large amounts of data stored in a data warehouse or data mart to find relationships and patterns\\', \\'Be responsible for implementing the systems, processes and logic required to extract, transform, clean, and distribute data across one or more data stores from a wide variety of sources for systems with moderate complexity\\', \\'Work under general guidance and clear framework of accountability with substantial autonomy\\', \\'Use best practices and knowledge of internal or external business issues to improve products or services\\', \\'Solve complex problems; takes a new perspective using existing solutions\\'], \\'Benefits\\': [\\'salary: $50 - 55 per hour\\', \\\\\"Pay offered to a successful candidate will be based on several factors including the candidate\\'s education, work experience, work location, specific job duties, certifications, etc\\\\\", \\'In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility)\\']}\",\"29\":\"{\\'Qualifications\\': [\\'5+ years\\\\u2019 experience engineering and operationalizing data pipelines with large and complex datasets\\', \\'5+ years\\\\u2019 of hands on experience with Informatica PowerCenter\\', \\'2+ years\\\\u2019 of hands on experience with Informatica IICS\\', \\'3+ years\\\\u2019 experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies\\', \\'5+ years\\\\u2019 experience with Data Modeling, ETL, and Data Warehousing\\', \\'2+ years\\\\u2019 hands on experience implementing data integration techniques such as event \\\\/ message based integration (Kafka, Azure Event Hub), ETL\\', \\'3+ years\\\\u2019 hands on experience with Git \\\\/ Azure DevOps\\', \\'Extensive experience working with various data sources; SQL,Oracle database, flat files (csv, delimited), Web API, XML\\', \\'Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources\\', \\'Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing\\', \\'Able to work in a fast-paced agile development environment\\', \\'Microsoft Azure\\\\/similar certifications\\', \\'Experience delivering data solutions through agile software development methodologies\\', \\'Exposure to the retail industry\\', \\'Excellent verbal and written communication skills\\', \\'Experience working with SAP integration tools including BODS\\', \\'Experience with UC4 Job Scheduler\\', \\'BA\\\\/BS in Computer Science, Engineering, or equivalent software\\\\/services experience\\', \\'Last two performance reviews\\', \\'Attendance records for current year (Do not include absences covered by paid sick\\\\/personal time, FMLA or other protected absences.)\\', \\'If hired, you will be required to provide proof of authorization to work in the United States\\'], \\'Responsibilities\\': [\\'This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources\\', \\'The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth\\', \\'Develops and operationalizes data pipelines to make data available for consumption (BI, Advanced analytics, Services)\\', \\'Works with data architects and data\\\\/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration\\', \\'Designs, develops, and implements ETL\\\\/ELT processes using IICS (informatica cloud)\\', \\'Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services\\', \\'Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization\\', \\'Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery\\', \\'Identifies ways to improve data reliability, efficiency, and quality of data management\\', \\'Communicates technical concepts to non-technical audiences both written and verbal\\', \\'Performs peer reviews for other data engineer\\\\u2019s work\\'], \\'Benefits\\': [\\'Level 2 - $100,000 - $135,000,\\', \\'Level 3 - $125,000 - $165,000\\', \\'Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible\\', \\'We offer a comprehensive package of benefits including paid time off, health benefits \\\\u2014 medical\\\\/dental\\\\/vision\\\\/hearing aid\\\\/pharmacy\\\\/behavioral health\\\\/employee assistance, health care reimbursement account, dependent care assistance plan, commuter benefits, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan, SmartDollar financial wellness program, to eligible employees\\']}\",\"30\":\"{\\'Qualifications\\': [\\\\\"Required Qualifications - Option 1: Bachelor\\'s degree in Computer Science and 4 years\\' experience in software engineering or related field\\\\\", \\\\\"Option 2: 6 years\\' experience in software engineering or related field\\\\\", \\\\\"Option 3: Master\\'s degree in Computer Data Engineer, Software Engineer, Computer Science, Database Engineer, Technology, Staffing, Engineer\\\\\"]}\",\"31\":\"{\\'Qualifications\\': [\\'Experience with big data tools like Hadoop, Spark, Kafka, fink, Hive, Sqoop etc\\', \\'Experience with relational SQL and NoSQL databases like Mysql, Postgres, Mongodb and Cassandra\\', \\'Experience with data pipeline tools like Airflow, etc\\', \\'Experience with AWS cloud services like: EC2, S3, EMR ETC\\', \\'Experience with stream-processing systems like: Storm, Spark-Streaming, Flink etc\\', \\'Experience with object-oriented\\\\/object function scripting languages: Python, Java, C , Scala, etc\\']}\",\"32\":\"{\\'Responsibilities\\': [\\'responsible for interpreting the requirements of various Big Data analytics use cases and scenarios\\']}\",\"33\":\"{\\'Qualifications\\': [\\'U.S. citizenship or current authorization to work in the U.S. is required, and no current or future work authorization sponsorship available\\', \\'Knowledge of the practical application of engineering science and technology, including applying principles, techniques, procedures, and equipment to the design and production of various goods and services\\', \\'Knowledge of design techniques, tools, and principles involved in production of precision technical plans, blueprints, drawings, and models\\', \\'Ability to use logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems\\', \\'Ability to understand the implications of new information for both current and future problem-solving and decision-making\\', \\'Skilled in identifying complex problems and reviewing related information to develop and evaluate options and implement solutions\\', \\'Ability to tell when something is wrong or is likely to go wrong\\', \\'It does not involve solving the problem, only recognizing there is a problem\\', \\'Ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material\\', \\'Required: High School Diploma or GED\\', \\\\\"Required: Bachelor\\'s Degree in Business, Engineering, Computer Science, Information Systems, Cybersecurity, or related field; or equivalent formal training\\\\\", \\'Required: Advanced level experience, seasoned and specialized knowledge in:\\', \\'Must maintain a well-groomed appearance per Company appearance standards as described in established guidelines\\', \\'Must be a U.S. citizen or have authorization to work in the United States as defined by the Immigration Reform Act of 1986\\', \\'Must be at least 18 years of age\\', \\'Must be able to comply with Company attendance standards as described in established guidelines\\'], \\'Responsibilities\\': [\\'Work on complex problems, where analysis of situations or data requires an in-depth evaluation of multiple factors\\', \\'Lead and\\\\/or provide expertise to functional project teams and participate in cross-functional initiatives\\', \\'Provide direction and guidance to process improvements, including helping to establish\\\\/advise on policies\\', \\'Work with a number of external vendors, helping to provide them with effective solutions and insights\\', \\'Use independent judgment within broadly defined policies and practices, including determining the best method for accomplishing work\\', \\'This role is offered as a remote workplace position, which may require travel for trainings, meetings, conferences, etc\\', \\'Assemble large, complex sets of data that meet non-functional and functional business requirements\\', \\'Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes\\', \\'Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies\\', \\'Build analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition\\', \\'Work with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues\\', \\'Generate or adapt equipment and technology to serve user needs\\', \\\\\"May perform other job duties as directed by Employee\\'s Leaders\\\\\", \\'Ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events)\\', \\'ETL experience ensuring source to target data integrity\\', \\'Various filetypes (Delimited Text, Fixed Width, XML, JSON, Parque)\\'], \\'Benefits\\': [\\'Competitive market salary from $137,250 per year to $152,500 per year* depending on qualifications and experience\\', \\'For eligible Leadership and individual contributor roles, additional bonus opportunities are available and awarded at the discretion of the company\\', \\'Fly for free, as a privilege, on any open seat on all Southwest flights\\\\u2014your eligible dependents too\\', \\'Up to a 9.3% 401(k) Company match, dollar for dollar, per paycheck.*\\', \\'Potential for annual ProfitSharing contribution toward retirement - when Southwest profits, you profit\\', \\'Explore more Benefits you\\\\u2019ll love: swa.is\\\\/benefits\\', \\'Pay amount doesn\\\\u2019t guarantee employment for any particular period of time\\', \\'*401(k) match contributions are subject to the plan\\\\u2019s vesting schedule and applicable IRS limits\\', \\'**ProfitSharing contributions are subject to plan\\\\u2019s vesting schedule and are made at the discretion of the Company\\']}\",\"34\":\"{\\'Qualifications\\': [\\'5+ years of relevant experience in data analytics or process improvement\\', \\'Proven ability in data analytics, data warehousing, and business intelligence\\', \\'Ability to synthesize sophisticated data from multiple, disparate sources to present analysis and relevant insights\\', \\'Sophisticated knowledge of database concepts and expertise with SQL (prefer Oracle and Snowflake)\\', \\'Data integration development experience procedure and tools (e.g\\', \\'Informatica, Talend, DataStage)\\', \\'Extensive experience deploying data on public cloud (prefer AWS)\\', \\'High proficiency in dimensional reporting structures and database design\\', \\'Acquainted with the creation and use of semantic layers\\', \\'Data prep experience using agile tools such as Alteryx, Qlik, R, or Python\\', \\'Tableau, OBIEE, Cognos, Domo, Power BI, etc.)\\'], \\'Responsibilities\\': [\\'Analyzing data to identify areas of improvement or insights which drive actions to advance organization objectives\\', \\'Work closely with the squad leaders, engineers & the architects to form strategic partnerships and influence strategic decisions and solution development\\', \\'Bring depth of experience and influence chosen technologies and design implementation Database, Data Warehousing and Analytics\\']}\",\"35\":\"{\\'Qualifications\\': [\\'Hands-on experience in Cloud and data pipelines\\', \\'Expert in SQL, and experienced programmer in one or more than one of the languages such as Python, Java, or Scala\\', \\'Understanding of Big Data and Hybrid Cloud infrastructure\\', \\'Experienced in more than one of the technologies such as Kafka, Kubernetes, Spark, Databricks, AWS EMR, S3, Data warehouses (Snowflake, Teradata), AWS and GCP Cloud services\\', \\'Experienced in DevOps tools such as GitLab CI\\\\/CD, and Jenkins\\', \\'Up to date on the latest technology developments\\', \\'Should be able to evaluate and propose new tooling\\\\/solutions for data platforms\\', \\'Excellent written, oral communication and presentation skills\\', \\'DevOps Certifications\\'], \\'Responsibilities\\': [\\'you consistently demonstrate and uphold the standards of coding, infrastructure, and process\\', \\'Develop solutions to build and continuously improve monitoring and observability for data pipelines and data platform\\', \\'Build data platform components using hybrid cloud services (AWS, GCP, and Azure)\\', \\'Implement features to improve data platform performance and security continuously\\', \\'Build Real-time data streaming tools and associated experience\\', \\'Create self-service tools and experience for all enterprise data engineering teams\\', \\'Build a data platform that can handle petabytes of data and help running advanced analytics workloads\\', \\'Improve the data quality and consumer experience for 100K+ enterprise data consumer\\'], \\'Benefits\\': [\\'Databricks or Spark Certifications\\']}\",\"36\":\"{\\'Qualifications\\': [\\'At least 3 years of experience in big data engineering or related field\\', \\'Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling\\', \\'Proficient in at least one programming language, such as Python or Java\\', \\'Experience with SQL and NoSQL databases\\', \\'Familiarity with distributed computing frameworks, such as Hadoop or Spark\\', \\'Understanding of data security and access control best practices\\', \\'Strong problem-solving skills and ability to work independently and in a team environment\\', \\'Excellent verbal and written communication skills\\', \\'Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field\\'], \\'Responsibilities\\': [\\'Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse\\', \\'Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines\\', \\'Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques\\', \\'Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities\\', \\'Build and maintain data models and ensure data accuracy and consistency\\', \\'Implement and manage data security measures, including backups and access controls\\', \\'Participate in code reviews, providing constructive feedback to ensure code quality\\', \\\\\"Bachelor\\'s degree in Computer Science, Engineering, or related field\\\\\", \\'Company-provided laptop and necessary hardware to ensure your setup for success\\', \\'Comprehensive health, dental and vision coverage\\', \\'Inclusive and motivating work culture that values team collaboration\\']}\",\"37\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s degree in Computer Science, Information Technology, Management Information Systems (MIS), Data Science or related field\\\\\", \\'Applicable years of experience may be substituted for degree requirement\\', \\'Minimum 8 years of experience in software engineering\\', \\'Proven ability to build, manage and foster a team-oriented environment\\', \\'Excellent communication (written and oral) and interpersonal skills\\', \\'Excellent organizational, multi-tasking, and time-management skills\\'], \\'Responsibilities\\': [\\'The Data Engineer will help build and maintain the cloud Delta Lake platform leveraging Databricks\\', \\'Candidates will be expected to contribute to all stages of the data lifecycle including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation\\', \\'Design, implement (deploy) and support on-premise and cloud-based data infrastructure (systems, flow) that are resilient to disruptions and failures\\', \\'Enhance and support corporate SQL\\\\/NoSQL database, DWH assets and streaming data solutions\\', \\'Ensure high uptime for all data services and consider enhanced solutions through scheduled or event-driven design\\', \\\\\"Bring multi cloud\\\\/cross-platform agnostic technologies and practices into the system to enhance reliability and support rapid scaling of the business\\'s data needs\\\\\", \\'Scale up our data infrastructure to meet cross-functional, multi industry business needs\\', \\'Develop, leverage and maintain end-to-end data pipelines in production\\', \\'Provide subject matter expertise and hands on delivery of data acquisition, curation and consumption pipelines on Azure, Databricks, AWS, Confluent\\', \\'Responsible for maintaining current and emerging state of the art compute and cloud based solutions and technologies\\', \\'Build effective relationships with internal stakeholders\\', \\'Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc\\', \\'All other duties as assigned\\']}\",\"38\":\"{}\",\"39\":\"{\\'Responsibilities\\': [\\'You will improve risk modeling that will allow the client to better understand the integrity of their pipelines for improved customer safety\\']}\",\"40\":\"{\\'Qualifications\\': [\\'Key Skillls Strong programming skills\\', \\'Particularly in languages\\', \\'Python Java Scala and SQL\\'], \\'Responsibilities\\': [\\'Build new data acquisition and transformation pipelines using big data and cloud technologies\\', \\'Work with the broader technology team including information technology information systems and 3rd parties to align pipelines\\', \\'Contribute to proof-of-concept efforts in Advanced Data Analytics\\', \\'Perform data analysis required to troubleshoot data-related issues and assist in the resolution of data issues\\', \\'Participate in establishing system documentation standards and QA methodologies\\']}\",\"41\":\"{\\'Qualifications\\': [\\'Hadoop\\', \\'Apache Spark\\']}\",\"42\":\"{\\'Qualifications\\': [\\'7+ years of data engineering experience\\', \\'Proficient with data technologies (e.g\\', \\'Spark, Kinesis, Kafka, Airflow, Oracle, PostgreSQL, Redshift, Presto, etc.)\\', \\'Experienced with designing and developing ETL data pipelines using tools such as Airflow, Nifi, or Kafka\\', \\'Strong understanding of SQL and data modeling\\', \\'Understanding of Linux, Amazon Web Services (or other cloud platforms), Python, Docker, and Kubernetes\\', \\'Experienced with common software engineering tools (e.g., Git, JIRA, Confluence, or similar)\\', \\\\\"Bachelor\\'s degree in computer science or comparable field or equivalent experience\\\\\", \\'A proven understanding and application of computer science fundamentals: data structures, algorithms, design patterns, and data modeling\\'], \\'Responsibilities\\': [\\'HEART FOR PEOPLE\\\\u2026 you can organize multiple engineers, negotiate solutions, and provide upward communication\\', \\'HEAD FOR BUSINESS\\\\u2026 you consistently demonstrate and uphold the standards of coding, infrastructure, and process\\', \\'Work with HEB Digital teams to provide data solutions for health and wellness\\', \\'Contribute to existing data platforms and implement new technologies\\', \\'Develop a deep understanding of HEB\\\\u2019s data and become a domain expert\\', \\'Ensure data is distributed in a timely and accurate manner\\', \\'Make data discoverable and accessible to business users\\', \\'Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed\\', \\'Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications\\', \\'Identify, scope, and architect solutions for new features while applying sound technical judgment that considers technology alternatives, impact on affected \\\\/ adjacent systems, and tradeoffs\\', \\'Get the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team\\\\u2019s architecture\\', \\'Implement data pipelines on AWS using Argo, Kubernetes, Spark and Python\\', \\'Integrate health, pharmacy and nutritional data systems\\', \\'Evaluate new technologies to improve quality, performance and cost of data pipelines\\'], \\'Benefits\\': [\\'A robust Benefits plan with coverage starting Day OneDental, vision, life, and other insurance plans; flexible spending accounts; short term \\\\/ long term disability coverage\\', \\'Partner Care Team, for any time you have healthcare or coverage questionsTelehealth offers 24\\\\/7 access to board-certified doctors by phone\\', \\'Partner Guidance allows free counselor visits\\', \\'Funeral leave, jury duty, and military pay (subject to applicable law)Maternal \\\\/ paternal leave for new parents, including adoptions10% off H-E-B brand products in-store and online\\']}\",\"43\":\"{\\'Qualifications\\': [\\'Expertise in SQL, Data Modeling, and Python\\', \\'Used Redshift, Airflow, Spectrum and relational database like SQL Server\\', \\'Capability to drive initiatives and articulate their value to Engineering and other stakeholders\\', \\'Experience delivering data products from conception to delivery\\', \\'Good communication skills (written\\\\/verbal)\\', \\'Passionate about designing elegant ETL pipelines\\', \\'5+ years of professional\\\\/industry experience\\', \\'Empathy: Be empathetic, communitarian and trustworthy\\'], \\'Responsibilities\\': [\\'As a Senior Data Engineer, you will be implementing critical ETL pipelines and advancing best practices for the data engineering team, and the rest of the organization\\', \\'You will work on delivering an actual big data architecture while concentrating on real-world problems such as privacy concerns\\', \\'Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers\\', \\'Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large\\', \\'Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and building scalable\\\\/reliable solutions\\', \\'Work with stakeholders and translate their needs and expectations into action items and deliverables\\', \\'Lead infrastructure initiatives, from design to implementation to delivery\\', \\'Support existing on-prem infrastructure and help expand our processes into the cloud (AWS)\\'], \\'Benefits\\': [\\'Annual training budget for each employee\\', \\'100% employer match on 401k contributions\\', \\'Specific COVID-19 allowance for home office set-up\\', \\'Matched giving to qualified organizations\\', \\'100% paid Parental Leave\\']}\",\"44\":\"{\\'Qualifications\\': [\\'Strong Python, bash, Linux shell or similar\\', \\'Experience integrating with native AWS services (CodePipeline, CodeCommit, CodeBuild, CodeDeploy, EC2, EKS, ECR, S3)\\', \\'Knowledge of networking, IAM, API and security assessment tools \\\\/ methodologies\\', \\'Familiar with IAM protocols such as SAML, SPML, XACML, SCIM, OpenID and OAuth\\', \\'Understanding of the cyber threat landscape and methodologies to protect technology assets\\', \\'Excellent verbal and written communication skills\\', \\'Proficient in automation and deploying CI and CD tools and services (Jenkins Pipeline as Code, Git, Maven)\\', \\'Hands on experience building solutions with tools and services like AWS CloudFormation, Terraform, or custom build orchestration tools leveraging SDKs or directly interacting with APIs\\', \\'Strong understanding of fundamental Application and Infrastructure Security concepts, including common types of attacks and exploitation techniques\\', \\'Solid Experience with various application security tools (Example ZAP, BURP, Tenable,Check Mark, Semmel, fortify, Sonatype, Kali, WebInspect \\\\/ AppScan, dependency check)\\', \\'Solid understanding of common web and systems application vulnerabilities\\', \\'Experience integrating security tools into the DevOps environment (such as Zap or Burp)\\', \\'Last updated : 2023-07-26\\']}\",\"45\":\"{}\",\"46\":\"{\\'Qualifications\\': [\\'The successful candidate should have a strong background in data engineering and experience working with sustainability initiatives in the IT space\\', \\'You will need to be a highly-motivated, self-driven individual with strong interpersonal and problem-solving skills\\', \\'We are looking for someone who is detail-oriented and can think outside the box to develop innovative solutions to complex problems\\', \\'The successful candidate will possess a degree in a related field such as computer science, engineering, or information technology\\', \\'Additionally, a minimum of two years of professional experience in data engineering and\\\\/or working with sustainability initiatives is required\\'], \\'Responsibilities\\': [\\'In this role, you will be responsible for developing and managing data solutions that support our IT sustainability objectives\\']}\",\"47\":\"{\\'Qualifications\\': [\\\\\"3 + years\\' experience as a Cloud Data Engineer\\\\\", \\\\\"3 + years\\' hands on experience in Snowpark, SnowSQL, Hadoop and PySpark\\\\\", \\\\\"3 + years\\' experience in AWS services such as S3, Glue, Athena, Redshift, EMR, Lambda and Cloud Formation\\\\\", \\\\\"3 + years\\' experience in Python with a focus on data processing and analytics\\\\\", \\'3+ years in healthcare domain\\', \\'3 + years in consulting\\', \\'Strong knowledge and hands-on experience in designing, developing, and deploying scalable solutions on the cloud platforms\\', \\'Expertise in SQL and database technologies for data manipulation and querying\\', \\\\\"Bachelor\\'s degree or equivalent experience\\\\\", \\'Limited immigration sponsorship may be available\\'], \\'Responsibilities\\': [\\'You will determine processes and automation tools to reduce IT spend and increase efficiencies on multiple projects within the Healthcare domain\\', \\'This position includes collaborating with DevOps teams to implement CI\\\\/CD pipelines, automated deployments, and infrastructure as code (IaC) practices for AWS-based solutions\\', \\'Document design, development, and deployment processes, as well as create technical specifications and user guides for developed solutions\\', \\'Your role will be to design, develop, and deploy cloud-based solutions for data processing, analytics, and integration using cloud services and big data technologies\\', \\'Collaborate with architects, data engineers, and business stakeholders to understand requirements and translate them into technical solutions\\', \\\\\"You will implement data ingestion, transformation, and storage processes using cloud services like AWS\\'s S3, Glue, Athena, Redshift, and EMR\\\\\", \\'Implement security, data governance, and compliance measures to ensure data integrity and protection in AWS-based solutions\\', \\'Develop and optimize data pipelines using Snowpark, SnowSQL, Hadoop and PySpark to extract, transform, and load data efficiently\\', \\'You will conduct performance tuning and optimization of data processing and analytics workflows to maximize efficiency and scalability\\', \\'Work with cross-functional teams to troubleshoot and resolve issues related to data processing, data integration, and analytics solutions\\', \\'Communicate regularly with Engagement Managers (Directors), project team members, and representatives from various functional and \\\\/ or technical teams, including escalating any matters that require additional attention and consideration from engagement management\\']}\",\"48\":\"{\\'Qualifications\\': [\\'Create CI&CD using azure dev ops pipeline to deploy Azure Services (Storage, Data factory, Key vault & Logic App) using ARM Templates\\', \\\\\"In lieu of the above, we will also accept a Bachelor\\'s degree in Computer Science, Computer Information Systems, or Engineering related or Technical related fields plus 5 years of progressively responsible post-baccalaureate experience\\\\\", \\'Foreign degree equivalent is acceptable\\', \\'We will also accept any suitable combination of education, training and\\\\/or experience\\', \\'Experience to include working on Azure Data Factory (ADF), Oracle, SQL server, Azure Databricks, Azure Synapse Analytics, Data Lake Storage, Azure PaaS services, SSIS packages, Azure Cosmos DB (SQL API), Python, Spark applications, Azure SQL, SQL queries, Azure Monitor and Alert services, GIT Support, ARM Templates\\'], \\'Responsibilities\\': [\\'Participate in daily agile and scrum processes to understand changing business requirements, including examining system configurations and operating procedures, as well as functional requirements gathering\\', \\'Ingest and prepare business-ready data in the cloud using Azure Data Factory (ADF) to build ELT(Extract Load and Transform)\\\\/ETL (Extract Transform and Load) data pipelines, then move the data into a data warehouse (Dedicated SQL Pool) and create data lake zones for data analytics and visualization\\', \\'Work with a combination of Azure Data Factory and Azure Databricks, extract, load, and transform data from cloud sources and on-premises databases such as Oracle, SAP, and SQL Server to Data Lake Storage and Azure Synapse Analytics\\', \\'Analyze, design, and build modern data solutions using Azure PaaS services to support data visualization\\', \\'Understand the current production state of the application and the impact of new implementation on existing business processes\\', \\'Enable private end point, firewall setting and Azure Key Vault for robust data security\\', \\'Analyze the existing SSIS packages and integrate it with Azure Data Factory, using SSIS transformations like Lookup, Derived column, Data conversion, Aggregate, Conditional split, SQL task, Script task, and Send Mail task\\', \\'Create a JSON structure for data storage in Azure Cosmos DB (SQL API), write stored procedures and functions and work with the API team to create Cosmos DB queries that use fewer request units\\', \\'Data Model in Snowflake and ELT using Snowflake SQL, implementing complex stored procedures and best practices with data warehouse and ETL concepts\\', \\'Design and customize dimension data models for data warehouse supporting data using Azure Synapse Analytics, and select the appropriate distribution method in dimension and fact tables to load the data in an optimized manner, as well as implement complex stored procedures and best practices with data warehouse\\', \\'Build a distributed in-memory application using spark applications and perform analytics efficiently on large datasets using python and Spark SQL, also use Spark SQL to implement transformation logic in Databricks and mount\\\\/unmount Azure Blob Storage\\', \\'Read the data from different file format parquet, avro, csv and json using pySpark (Python API) in Azure Databricks and perform data extraction, transformation to uncover insights into customer usage patterns and insert curated data into a data warehouse\\', \\'Create data visualization reports and dashboards in Power BI using data from the data warehouse, flat files, and Azure SQL\\', \\'Responsible for fixing problems and conducting investigations into SQL queries, Stored Procedures related to long running jobs and Azure service performance\\', \\'Utilize Azure Monitor and Alert services, create monitors, alerts, and notifications for Data Factory, Synapse Analytics, and Data Lake Storage\\', \\'Perform the required daily GIT support for various projects\\', \\'Be in charge of maintaining GIT repositories and access control procedures\\']}\",\"49\":\"{\\'Responsibilities\\': [\\'Translate business and functional requirements into robust, scalable, operable solutions that work well within the overall data architecture\\', \\'Design, develop, implement, test, document, and operate large-scale, high-volume and low latency applications\\', \\'Build integrations between HR, Time & Attendance and Payroll systems to automate employee hire, schedule, punch and pay\\', \\'Designs data integrations and data quality framework\\', \\'Develops and maintains scalable data pipelines and builds out new API integrations\\', \\'Implement data structures using best practices in data modeling, ETL\\\\/ELT processes, SQL, and Oracle\\', \\'Manage stakeholder communication, prioritization of tasks and on time solution delivery\\', \\'Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance\\', \\'Produce comprehensive, usable dataset documentation and metadata\\', \\'Design and develop operational and analytical reports as per the customer needs by using the tools\\', \\'Evaluate and make decisions around the use of new or existing software products and tools\\'], \\'Benefits\\': [\\'Estimated Salary: $20 to $28 per hour based on qualifications\\']}\",\"50\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s degree in Information Technology or a related field, required\\\\\", \\'5 years of experience in implementing DWH \\\\/ BI solutions\\', \\'3 years of experience leading projects in analysis, architecture, design, and development of traditional data warehouse, data pipeline and business intelligence solutions\\', \\'3 years of experience as a Data Engineer using Data Brick and Snowflake\\', \\'Good understanding of various Data Models such as Dimensional Data Modeling and DataVault Modeling\\', \\'Experience with Azure cloud services such as azure cloud security, monitoring, logging, scaling, caching, etc\\', \\'Experience dealing with cloud workloads, application, and infrastructure architecture, data ingestion architecture, data storage, and transformations, data analytics, serverless function\\\\/application design, micro-services, DevOps\\', \\'Experience with Linux and cloud environment including commands and scripting\\', \\'Solid experience in writing complex SQL queries as well as PL\\\\/SQL programs on Oracle database\\', \\'Familiarity with DevOps best practices and automation of documentation, testing, build, deployment, configuration, and monitoring\\', \\'Experience in visualization tools\\\\/solution such as Tableau and Power BI\\', \\'Solid communication skills, particularly in writing technical solution proposals\\'], \\'Responsibilities\\': [\\'This individual serves as the primary liaison between IT Analytics, Shared Services, and the Business Units\\', \\\\\"Understand customers\\' overall data estate business, related success measures, and IT priorities in order to design data solutions that drive business value\\\\\", \\'Educate the business and promote best practices on how to leverage BI and analytics solutions, as well as help facilitate discussions to anticipate future needs and opportunities\\', \\'Assist in the identification and integration of data sources\\', \\'Clearly communicate findings, recommendations, and opportunities to improve data systems and solutions\\', \\'Seek out information to learn about emerging methodologies and technologies\\', \\'Work closely with DevOps team to automate the deployment of resources to our various Azure subscription\\', \\'Performs technical design reviews and code reviews\\'], \\'Benefits\\': [\\'You will also have access to a range of company benefits, including a competitive wage with shift differential, annual bonus opportunities and career advancement and cross-training\\']}\",\"51\":\"{\\'Qualifications\\': [\\'The proper candidate will be expected to identify risks and potential blockers and elevate appropriately or mitigate as needed\\', \\'This candidate should have strong communication skills\\', \\'Technical Skills: SAP Development Very Strong\\'], \\'Responsibilities\\': [\\'The Data Engineer Lead will be responsible for guiding development of multiple (2-4) projects at a single time\\', \\'His\\\\/her tasks will include directing Data Engineer team members in best practices and design\\\\/implementation decisions to support Data and Analytics Business Customers\\', \\'He\\\\/she will also be responsible to actively develop at least one of the projects under their leadership\\']}\",\"52\":\"{\\'Qualifications\\': [\\'Experience supporting a SIEM tool\\', \\'Experience deploying Infrastructure in cloud environments\\', \\'Experience building and troubleshooting data pipelines\\', \\'Experience with Global Information\\\\\\\\\\\\\\\\Cyber Security Teams\\', \\'Experience with Machine Learning\\', \\'Experience with Multiple Operating Systems\\', \\'Knowledge with PowerShell, JavaScript, or Python Scripting\\', \\'Windows Or Linux Administration Experience\\', \\'SOAR Platform Experience\\', \\'Familiar with CIM date compliance\\'], \\'Responsibilities\\': [\\'Oversee the ingestion and normalization of new data sources\\', \\'Maintain data availability\\', \\'Detect and remediate any drop in data ingestion\\', \\'Support, maintain and improve infrastructure for the data collection tools overall health\\', \\'Respond to customer inquiries surrounding the collection of data\\', \\'Work closely with Cyber Security team to ensure that analyst have access to the data they need and that it\\\\u2019s presented in a clear and concise manner\\', \\'Stay up to date with the latest trends and technologies in data engineering and cloud infrastructure management and applying them to our data collection tool stack\\', \\'Actively seek ways to improve our current data collection tool stack\\'], \\'Benefits\\': [\\'Any compensation range provided for a role is an estimate determined by available market data\\', \\'Our benefits plan includes medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation\\\\/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance\\']}\",\"53\":\"{\\'Qualifications\\': [\\'Bachelor\\\\u2019s and\\\\/or Master\\\\u2019s degree in a related field\\', \\'7+ years of experience deploying ETL solutions in production environments\\', \\'7+ years of experience with cloud-based data services, preferably in AWS or Azure\\', \\'7+ years of experience developing Python, Scala, Java, .Net or similar solutions in a backend or data wrangling capacity\\', \\'7+ years of experience in mixed Windows\\\\/Linux environments\\', \\'Results-proven track record of exceeding goals and evidence of the ability to consistently make good decisions through a combination of analysis, experience and judgment\\', \\'Experience deploying, monitoring, and maintaining data pipelines in production environments\\', \\'To perform this job successfully, an individual must have intermediate knowledge of Microsoft Project, Word, Excel, Access, PowerPoint, Outlook, and Internet navigation and research\\'], \\'Responsibilities\\': [\\'This role requires an enterprise mindset to build out robust, high-performance technology\\', \\'Use a variety of programming languages and tools to develop, test, and maintain data pipelines within the Platform Reference Architecture\\', \\'Working directly with management, product teams and practice personnel to understand their platform data requirements\\', \\'Maintaining a positive work atmosphere by behaving and communicating in a manner that encourages productive interactions with customers, co-workers and supervisors\\', \\'Developing and engaging with team members by creating a motivating work environment that recognizes, holds team members accountable, and rewards strong performance\\', \\'Fostering an innovative, inclusive and diverse team environment, promoting positive team culture, encouraging collaboration and self-organization while delivering high quality solutions\\', \\'Collaborating on an Agile team to design, develop, test, implement and support highly scalable data solutions\\', \\'Collaborating with product teams and clients to deliver robust cloud-based data solutions that drive tax decisions and provide powerful experiences\\', \\'Analyzing user feedback and activity and iterate to improve the services and user experience\\', \\'Creating and implementing robust cloud-based data solutions that scale effectively, and provide powerful experiences for both internal teams and clients\\', \\'Performing unit tests and conducting reviews with other team members to make sure solutions and code are rigorously designed, elegantly coded and effectively tuned for performance\\', \\'Staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities and mentoring other members of the engineering community\\', \\'Perform other duties as assigned\\', \\'Requires supervisory responsibilities, including training employees, assigning work, and assuring quality throughout any deliverables\\', \\'Occasional long periods of sitting while working at computer\\'], \\'Benefits\\': [\\'For Oakland and San Jose, CA-based roles, the base salary hiring range for this position is $155,000-$185,000\\']}\",\"54\":\"{\\'Qualifications\\': [\\'Requirements BS or MS degree in Computer Science or a related technical field 0-2 years of experience in the Information Technology field\\'], \\'Responsibilities\\': [\\'Responsibilities Design data integrations and data quality framework using AWS Cloud technologies\\', \\'Develop and maintain scalable data pipelines and ETL processes using Python\\', \\'Write unit\\\\/integration tests contribute to engineering wiki and document work\\', \\'SQL Performance & Tuning Implement processes to monitor data quality ensuring high availability and high accuracy of production data\\', \\'Provide operational support for the data pipeline and perform data analysis required to troubleshoot data related issues and the resolution\\', \\'Collaborate with analytics and business teams to build data models that feed business intelligence tools to foster data-driven decision making across the organization\\', \\'Work closely with the business users product managers analysts and off-shore team (India) to develop strategy for long term data architecture and the development for the business solution\\']}\",\"55\":\"{\\'Qualifications\\': [\\'Proficient in RESTful Services, Java, Scala, Spring Boot\\\\/Play Framework, RDBMS, NoSql, Python\\', \\'Proficient in development of scalable cloud native microservices\\', \\'Proficient with Designing and building APIs\\', \\'Azure and or GCP exposure and experience\\', \\'Experience level: Experienced\\', \\'Minimum 7 years of experience\\', \\'Hadoop (7 years of experience is required)\\'], \\'Responsibilities\\': [\\'work hours: 8am to 4pm\\'], \\'Benefits\\': [\\'salary: $70 - 75 per hour\\', \\\\\"Pay offered to a successful candidate will be based on several factors including the candidate\\'s education, work experience, work location, specific job duties, certifications, etc\\\\\", \\'In addition, Randstad offers a comprehensive benefits package, including health, an incentive and recognition program, and 401K contribution (all benefits are based on eligibility)\\']}\",\"56\":\"{\\'Qualifications\\': [\\'Bachelors in science , engineering or equivalent\\', \\'10+ years of experience building, deploying, and designing data solutions: ETL, data warehousing, or Big Data\\', \\'3-6 Years of experience working with DataBricks\\', \\'Experience working with Azure Data stack; Data Lake, Data factory, DataBricks, Synapse, etc\\', \\'PySpark\\', \\'eCommerce\\'], \\'Responsibilities\\': [\\'Provide input to cloud engineer for the design and implementation of data management and\\\\/or architecture solutions\\', \\'Partner with cloud engineer and ML engineer to develop and evolve the concept of data ops\\', \\'Design implement and deploy data loaders to load data into the Engineering Sandbox\\', \\'Assist in pulling filtering tagging joining parsing and normalizing data sets for use\\', \\'Work with the analytics translator data quality analyst and IT to resolve data quality issues\\', \\'Experience in designing and implementing large scale data loading manipulation processing analysis and exploration solutions\\', \\'Deep technical expertise with pulling and massaging data understanding of first\\\\/third party data\\', \\'Advanced SQL skills and understanding of data management principles and processes\\'], \\'Benefits\\': [\\'This position is also eligible for Cognizant\\\\u2019s discretionary annual incentive program, based on performance and subject to the terms of Cognizant\\\\u2019s applicable plans\\', \\'Medical\\\\/Dental\\\\/Vision\\\\/Life Insurance\\', \\'Paid holidays plus Paid Time Off\\', \\'401(k) plan and contributions\\', \\'Long-term\\\\/Short-term Disability\\', \\'Paid Parental Leave\\', \\'Employee Stock Purchase Plan\\', \\'Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting\\']}\",\"57\":\"{\\'Qualifications\\': [\\'Experience with MS SQL Server and T-SQL code development\\', \\'Experience with ETL - SSIS\\', \\'Experience in building stored procedures, functions, merges and Ad Hoc scripts\\', \\'Experience with data management and understanding of data structures, database design\\', \\'Ability to collaborate and communicate effectively across functional groups\\', \\'Ability to independently perform database development, testing and implementation with little supervision\\', \\'Ability to effectively communicate technical issues and resolve problems\\', \\'Visual studio to create SSIS package, stored procedure, code review\\'], \\'Responsibilities\\': [\\'This position will sit on the Data Services Team and be responsible for sourcing data and loading it into the enterprise data warehouse across multiple projects in the organization\\', \\'The ideal candidate will be a self-starter and a team player!\\', \\'Communicating with Technology teams and business owners to develop, test, and implement reports for internal and external users\\', \\'The developer will be responsible for the design, development, and deployment of data extracts, transformations, and loads\\', \\'ETL development will utilize the Microsoft SQL platform, specifically SQL Server Integration Services (SSIS)\\', \\'Experience with various reporting methods, including SSRS\\', \\'Develop reports based on defined requirements\\', \\'Responsible for analyzing the business requirements for data file requests, designing, developing, and deploying fully automated ETL jobs to produce data files\\', \\'Write advanced SQL statements to retrieve data from tables and stored procedures\\', \\'Use known education principles and stay up-to-date on new reporting methods and techniques\\', \\'Partner with internal stakeholders and liaise with experts regarding instructional design\\'], \\'Benefits\\': [\\'Pay Rate: $50-60\\\\/hour\\']}\",\"58\":\"{\\'Qualifications\\': [\\'This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel\\', \\'1+year of experience working as a BI technical resource in a customer-focused IT EDW team ?\\', \\'2+years of experience in the data warehousing domain as a technical resource ?\\', \\'Deep understanding of data warehousing concepts, relational star-schema database designs and big data platforms and associated tools\\', \\'Basic understanding and hands-on experience of Informatica 9.x, Tableau and snowflake system components, internal processes and architecture\\', \\'Good knowledge of SQL and relational database models\\', \\'Hands-on experience creating Unix shell scripts ?\\', \\'Data visualization tool experience like Tableau ?\\', \\'Good understanding of data modeling ?\\', \\'Basic working experience in an agile environment ?\\', \\'Excellent team player able to work with virtual and global across functional teams at all levels\\', \\'Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines\\', \\'Excellent spoken and written communication as well as receptive listening skills, with ability to present complex ideas in a clear, concise fashion towards technical and nontechnical audiences\\', \\'Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for success of this role\\', \\'Salesforce, Data Warehousing, customer success and Data Analysis ?\\', \\'Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements\\', \\'Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications\\'], \\'Responsibilities\\': [\\'Contribute towards designing data models, ETL mappings and associated objects of analytical solutions\\', \\'Review solution design with Lead members and ensure that the defined EDW standards and framework are followed\\', \\'Review and validate logical and physical design to ensure alignment with the defined solution architecture\\', \\'Create\\\\/review technical documentation for all new and modified objects\\', \\'Ensure quality assurance plans and cases are comprehensive to validate the solution thoroughly\\', \\'Follow standard practises for QA in the organization ?\\', \\'Support QA, UAT and performance testing phases of the development cycle\\', \\'Understand and incorporate required security framework in the developed data model and ETL objects\\', \\'Define standards and procedures; refine methods and techniques for data extraction, transformation and loading (ETL) both in batch and near real time modes\\', \\'Evaluate, determine root cause and resolve production issues\\', \\'Work closely with other IT development groups to deliver coordinated software solutions Required Skills:\\'], \\'Benefits\\': [\\'$80K -- $100K\\']}\",\"59\":\"{\\'Qualifications\\': [\\'3-5+ years of hands-on experience in multiple modern programming languages such as Python or Java\\', \\'2+ years of experience leveraging DevOps principals such as CI\\\\/CD and using tools like Git, Jenkins, etc\\', \\'Strong working knowledge of Data Engineering and associated tools and technologies like Apache, Spark, Databricks, Python, SQL, and data lake concepts\\', \\'Experience with Argo Informatica\\', \\'Working experience with AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3, etc\\', \\'An equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above\\', \\'Experience working with other public cloud technologies AWS, GCP\\', \\'Hands on experience with data virtualization technologies: CIS (Tibco), Denodo, or SQL 2019 Virtualization\\', \\'Experience building test automation to ensure data quality and accuracy\\', \\'Proficient in data modeling (specifically RDS, PostgreSQL, Oracle)\\'], \\'Responsibilities\\': [\\\\\"As a Data Engineer you\\'ll:Design and build a modern data warehouse in the cloud\\\\\", \\'Enhance data collection procedures to build analytic systems\\', \\'Work as an Agile partner and participate in back log refinement, sprint planning, review, and retrospective\\', \\'Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed\\', \\'Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications\\', \\'Support the build and deployment pipeline and when necessary, both diagnose and solve production support issues\\', \\'Work with Product, Design, and QA to deliver world-class digital experiences\\', \\\\\"Get the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team\\'s architecture\\\\\", \\'Apply understanding to help improve the cloud infrastructure that powers our high-performance, consumer-scale site and mobile apps\\']}\",\"60\":\"{\\'Qualifications\\': [\\\\\"8 plus year\\'s experience\\\\\", \\'Nice to have Banking experience\\']}\",\"61\":\"{\\'Qualifications\\': [\\'8 years of relevant IT experience and 3 years of experience with Java coding\\', \\\\\"Bachelor\\'s degree in Engineering, Computer Science, Information Technology, related discipline or significant industry experience\\\\\", \\'Excellent coding skills with expertise in Java, REST APIs, SQL, and micro-services\\', \\'Experience with Apache Spark \\\\/ Beam, Google Cloud Dataflow, MySQL, Big Query, Airflow, Kafka, Kubernetes, and Anthos\\', \\'Broad knowledge of open-source libraries & packages\\', \\'Well versed with big data concepts and data engineering principles or techniques\\', \\'Excellent communication and organization skills\\', \\'Must have a go-getter attitude and a willingness to pick up new technologies and programming languages\\', \\\\\"Must be a team player with a creative mindset who\\'s always looking for novel solutions to challenging problems and open to learning as well as sharing knowledge with others\\\\\", \\'Working knowledge of Python, DBT, React.JS & Angular.JS\\', \\'Professional certifications\\', \\'Knowledge of Cloud architecture, networking and protocols\\', \\'Analytical mindset and good problem-solving skills\\', \\'Good organizational skills\\', \\'Someone who enjoys having fun and maintaining work life balance while working on exciting projects\\', \\\\\"Someone who\\'s interested in working at a company that believes in mandatory development of its staff and has a culture of promoting from within\\\\\"], \\'Responsibilities\\': [\\'Process complex data sets, leverage technologies used to process these disparate data sets and understand the correlation as well as patterns that exist between the data sets\\', \\'Work closely with other teams to develop solutions to meet business requirements, perform assessments, POC\\\\u2019s and establish an execution or development plan\\', \\'Forecast technical challenges and develop implementation strategies\\', \\'Identify enabling technologies and develop solutions based on identified technologies as required\\', \\'Prepare and present white papers and proposals\\', \\'Employ lateral thinking to develop innovative solutions to meet business needs\\', \\'Communicate complex technical ideas in a straightforward and compelling way\\'], \\'Benefits\\': [\\'Annual Pay Range : 97,200 - 130,000 USD\\', \\'CoreLogic benefits information can be found here : . Qualifications, locations and experience of the individual ultimately selected for the position may impact the final actual offered compensation, which may vary from any posted range\\', \\'We offer an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package\\']}\",\"62\":\"{\\'Responsibilities\\': [\\'Your main expertise and responsibility will be data identification and extraction\\', \\'Leveraging proprietary and third-party data-extraction tools (e.g. Snaplogic), you will identify spend relevant data from general ledger, purchase order, material & supplier master and other relevant tables within ERP or data\\\\/business warehouse applications\\', \\'You will conduct data assessment, perform data quality checks, transform and load raw data using SQL and ETL tools\\', \\'With your experience in data engineering and data models, you will build sustainable data pipelines and adjust them for special needs\\']}\",\"63\":\"{\\'Qualifications\\': [\\'Building data lake platform in AWS (Glue, DynamoDB S3, EMR)\\', \\'Understand data engineering, data governance, data lineage\\', \\'Linux Shell Scripting\\', \\'Data warehousing\\', \\'Kubernetes\\', \\'Expertise in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies\\', \\'Sound knowledge of distributed systems and data architecture - design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures\\', \\'5+ years of work experience with building Data Pipelines, Data Processing, Data Modeling, and Data Architecture\\', \\'Experience operating very large data warehouses or data lakes\\', \\'Knowledge of Engineering and Operational Excellence using standard methodologies\\', \\'Knowledge of IT, service-oriented architectures, software development life cycles, or information security platforms and applications\\', \\'Minimum 5+ years of experience in software development\\', \\'3+ years of related industry experience in an enterprise environment\\', \\'Scala \\\\/ Python, pySpark(Boto, Boto3, etc.)\\\\/ Spark experience\\', \\'Experience with lambda, EMR, SQS, DynamoDB, Glue, Stepfunctions, etc\\', \\'Formal design patterns and industry best-practices\\', \\'2+ years of experience with requirements, design, implementation, integration, and testing for data and analytics integration\\', \\'2+ years of experience across a variety of technologies such databases, directory services, application servers, network infrastructures, Linux operating systems, and an understanding of fundamental security and data flows within these components\\', \\'Excellent verbal and written communication skills\\', \\'Self-motivated, driven, and creative individual\\', \\'Scaling systems and microservices\\', \\'Familiarity with CI\\\\/CD processes\\', \\'Code coverage analysis \\\\/ static analysis tools\\', \\'Agile programming processes and methodologies such as Scrum\\', \\'Scheduling tools like Autosys , ControlM\\', \\'Informatica IICS , Talend\\', \\'8 years\\'], \\'Responsibilities\\': [\\'Delta lake, delta table and lakehouse architecture\\'], \\'Benefits\\': [\\'Pay: From $65.00 per hour\\']}\",\"64\":\"{\\'Qualifications\\': [\\'The successful candidate will have a strong technical background and the ability to think strategically about our data architecture\\', \\\\\"You must have a Bachelor\\'s degree in Computer Science, Software Engineering, Information Systems, Data Science or a related field and at least two years of experience in working with data architectures, ETL pipelines, and data warehousing.We are looking for an individual who is passionate about data and has the ability to analyze and interpret complex datasets\\\\\", \\'You should have excellent problem-solving skills, be comfortable working with multiple stakeholders, and have a keen eye for detail\\', \\'If you are motivated, organized, and an excellent communicator, we would love to hear from you!\\']}\",\"65\":\"{\\'Qualifications\\': [\\'years of experience with 1\\', \\\\\"years of Big data development experience Experience in HDFS, Hive, Hive UDF\\'s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix\\\\\", \\'Demonstrates expertise in writing complex, highly optimized queries across large data sets Retail experience and knowledge of commercial data is a huge plus Experience with BI Tool Tableau or Looker is a plu The above information has been designed to indicate the general nature and level of work performed in the role\\', \\'Data Science & Machine LearningEstimated Salary: $20 to $28 per hour based on qualifications\\'], \\'Responsibilities\\': [\\'You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way\\', \\\\\"You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers\\' lives\\\\\", \\'Design, develop and build database to power Big Data analytical systems\\', \\'Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies\\', \\'Build robust and scalable applications using SQL, Scala\\\\/Python and Spark\\', \\'Create real time data streaming and processing using Kafka and\\\\/or Spark streaming\\', \\'Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure Engage with architects and senior technical leads to create and enhance complex software components\\', \\'Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud\\', \\'Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion\\', \\'Develop proof-of-concept prototype with fast iteration and experimentation\\', \\\\\"Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager Perform continuous integration and deployment using Jenkins and Git You\\'ll sweep us off our feet if 3\\\\\"], \\'Benefits\\': [\\'Beyond competitive pay, you can receive incentive awards for your performance\\', \\'Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more\\', \\\\\"All the benefits you need for you and your family Multiple health plan options, including vision & dental plans for you & dependents Financial benefits including 401(k), stock purchase plans, life insurance and more Associate discounts in-store and online Education assistance for Associate and dependents Parental Leave Pay during military service Paid Time off - to include vacation, sick, parental Short-term and long-term disability for when you can\\'t work because of injury, illness, or childbirth Eligibility requirements apply to some benefits and may depend on your job classification and length of employment\\\\\", \\'Benefits are subject to change and may be subject to specific plan or program terms\\', \\'$80K -- $100K\\']}\",\"66\":\"{\\'Qualifications\\': [\\'Equivalent combination of education and experience may be substituted in lieu of degree\\', \\'2 to 4 years of Microsoft SSIS package development experience including experience with Microsoft Visual Studio\\', \\'1+ years of experience working with enterprise data warehouses\\', \\'5+ years of experience in SQL and be able to write complex logic using SQL as part of ETL, and use SQL effectively to perform complex data analysis and discovery\\', \\'1+ years of experience building reports with SSRS\\', \\'Exposure to Azure and Azure Data Factory\\', \\'Exposure to an Enterprise Data Lake\\', \\'Demonstrate strong organization skills and detail-oriented\\', \\'Experience with CMD shell and PowerShell\\', \\'Experience with large-scale, complex data environments\\', \\'Ability to self-motivate and meet deadlines\\', \\'Intense desire to learn\\', \\'Ability to express complex technical concepts effectively, both verbally and in writing\\', \\'Ability to multi-task in a fast-paced, changing environment\\', \\'Ability to maintain confidentiality\\', \\'Dallas, TX 75240: Reliably commute or planning to relocate before starting work (Required)\\'], \\'Responsibilities\\': [\\'queries and data engineering solutions that moves, cleans, and loads data to differing systems\\', \\'person will interact with our clients and differing roles within IT to gain understanding of the business\\', \\'This person will also be responsible for\\', \\'generating documentation as needed; conform to security and quality standards; and stay current on\\', \\'Be comfortable with researching data questions, identify root causes, and interact closely with business users and technical resources on various data related decisions\\', \\'Proactively identify and assist in solving recurring data quality or data availability issues\\', \\'Have experience in monitoring, troubleshooting, and resolving ETL issues\\', \\'Be able to develop high performance data queries, stored procedures and\\\\/or functional code for data related ad-hoc reporting and\\\\/or ETL batch triage reasons\\', \\'Understand how to profile code, queries, programming objects and optimize performance\\', \\'Monitors, supports, triages data pipelines and ETL tasks that ingest, move, transform, and integrate data in a secure and performant manner\\', \\'Prepares necessary SQL scripts to perform data manipulation in key systems to address data related application and reporting needs\\', \\'Explores new technologies and data processing methods to increase efficiency, performance, and flexibility to proactively address recurring data related issues\\', \\'Document requirements and translate into proper system requirements specifications using high-maturity methods, processes, and tools\\', \\'Designs, prepares, and executes unit tests\\', \\'Participates in cross-functional teams\\', \\'Represents team to clients\\', \\'Demonstrates technical leadership and exerts influence outside of immediate team\\', \\'Develops innovative team solutions to complex problems\\', \\'Contributes to strategic direction for teams\\', \\'Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g\\', \\'Integrates technical expertise and business understanding to create superior solutions for clients\\', \\'Consults with team members and other organizations, clients and vendors on complex issues\\', \\'Work on Special projects as requested\\', \\'Performs other duties as assigned\\'], \\'Benefits\\': [\\'Salary: Up to $80,000.00 per year\\', \\'401(k)\\', \\'Dental insurance\\', \\'Flexible schedule\\', \\'Health insurance\\', \\'Paid time off\\', \\'Tuition reimbursement\\', \\'Vision insurance\\']}\",\"67\":\"{\\'Qualifications\\': [\\'Minimum Requirements: B.S. in Computer Science, Electrical Engineering, Electronics Engineering, or closely related field; five years of IT development or related experience, including specific experience using Microsoft SQL server database to write complex logic and data analysis; knowledge of data warehouses, Microsoft SSIS package development, and building reports with SSR; and proficiency with large-scale, complex data environments, Microsoft Power BI, Azure and Azure Data Factory, Enterprise Data Lake, CMD shell and PowerShell\\'], \\'Responsibilities\\': [\\'Responsible for developing complex logic to analyze, prepare, and discover insights within relational databases for innovative healthcare solutions provider\\', \\'Specific duties include: (1) architecting database schema and other programmatic objects within database to store and secure data; (2) developing routines using SQL Server Integration Services, Azure Data Factory, and other ETL toolsets to ingest, transform, move, prepare, and extract data from different data stores; (3) utilizing SQL, Databricks, and other analytics tools to derive insights from data sets; (4) utilizing data visualization toolsets such as SQL Server Reporting Services and Microsoft Power BI to produce reports and data visualization for business partners; and (5) working data engineering team to conduct peer review and code analysis for data SDLC process\\']}\",\"68\":\"{\\'Qualifications\\': [\\\\\"Principal level engineering background technically but won\\'t be spending most of their time hands on and need to be comfortable managing the process without getting hands-on\\\\\", \\'Need to be able to manage off-shore resources\\', \\'Each team is going to have 3 data engineers and 2 QA engineers and each lead will be responsible for two of these off-shore teams\\', \\'Highly regulatory industry experience would be a huge advantage\\', \\'Snowflake and python - high level experience in the data engineering\\\\/science space\\', \\'For ETL -- Into snowflake using Kafka - in snowflake its python and IDMC\\', \\'Experience doing large enterprise data migrations like they are doing\\'], \\'Responsibilities\\': [\\'More delegating because they will manage 2 teams - architects are doing the modeling so these leads will be working with the business most\\']}\",\"69\":\"{\\'Qualifications\\': [\\'3 - 5 Years of Experience leading a high performing data engineering team\\', \\'8 - 10 years of Big data development experience\\', \\'2 -3 Years of Experience in GCP\\\\/Azure cloud platforms\\', \\'2 -3 years of experience building streaming pipeline using spark streaming or similar\\', \\'Demonstrates up-to-date expertise in Data Engineering, complex data pipeline development\\', \\'Exposure to Data Governance ( Data Quality, Metadata Management, Security, etc.)\\', \\'Experience with Java, Scala and\\\\/or Python to write data pipelines and data processing layers\\', \\'Demonstrates expertise in writing complex, highly-optimized queries across large data sets\\', \\'Proven working expertise with Big Data Technologies Spark Scala\\\\/PySpark, and SQL\\', \\'Knowledge and experience in Kafka, Storm, Druid and Presto\\', \\\\\"Option 1: Bachelor\\\\u2019s degree in Computer Science and 4 years\\' experience in software engineering or related field\\\\\", \\\\\"Option 3: Master\\'s degree in Computer Science and 2 years\\' experience in software engineering or related field\\\\\", \\\\\"Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master\\\\u2019s degree in Computer Science or related field and 4 years\\' experience in software engineering or related field, Supervisory\\\\\"], \\'Responsibilities\\': [\\'As a Senior Manager I, Data Engineer you are managing a large global Engineering team building scalable, highly available, enterprise-grade data flows\\', \\'You will drive the execution of multiple data engineering project that unlocks the potential of data for the business\\', \\'You will lead and inspire your teams while effectively prioritizing work and maintaining a strong customer focus to create business value while putting processes in-place to ensure successful outcomes\\', \\'You will find ways to deliver incremental value while solving tactical problems strategically\\', \\'Lead a team of Data engineers to design, build, test and deploy cutting edge solutions at scale, impacting millions of customers worldwide drive value from data\\', \\\\\"Interact with Sam\\'s Club engineering teams across geographies to leverage expertise and contribute to the tech community\\\\\", \\'Engage with Product Management and Business to drive the agenda, set your priorities and deliver awesome product features to keep platform ahead of market scenarios\\', \\'Identify right open source tools to deliver product features by performing research, POC\\\\/Pilot and\\\\/or interacting with various open source forums\\', \\'Develop and\\\\/or Contribute to add features that enable adoption of data across Sam\\\\u2019s Club\\', \\'Deploy and monitor products on Cloud platforms\\', \\'Develop and implement best-in-class monitoring processes to enable data applications meet SLAs\\', \\'Guide the team technically for end-to-end solution Lifecyle\\'], \\'Benefits\\': [\\'Benefits: Beyond our great compensation package, you can receive incentive awards for your performance\\', \\'Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more\\', \\'Health benefits include medical, vision and dental coverage\\', \\'Financial benefits include 401(k), stock purchase and company-paid life insurance\\', \\'Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting\\', \\'Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more\\', \\'You will also receive PTO and\\\\/or PPTO that can be used for vacation, sick leave, holidays, or other purposes\\', \\'It will meet or exceed the requirements of paid sick leave laws, where applicable\\', \\'Tuition, books, and fees are completely paid for by Walmart\\', \\'The annual salary range for this position is $112,000.00-$192,000.00\\', \\'Additional compensation includes annual or quarterly performance incentives\\', \\'Additional compensation for certain positions may also include:\\', \\'Regional Pay Zone (RPZ) (based on location)\\', \\'Stock equity incentives\\']}\",\"70\":\"{\\'Qualifications\\': [\\'6 years of demonstrated hands on data engineering experience-- preferably in consulting or financial services\\', \\'Success working in ANY ONE of the following Data Engineering\\\\/Management technology areas, including but not limited to:\\', \\'Experience with ETL tools with data tracking\\', \\'Experience developing ETL Data Pipelines and Data Model solutions for integrating new data sets into a data platform from different sources\\', \\'Experience implementing enterprise systems\\', \\'Experience with Big Data (Hadoop, MongoDB, Exadata)\\', \\'Experience developing real time ETL solutions (Kafka, Kinesis, etc)\\', \\'Strong data modelling skills\\', \\'Experience developing complex SQL queries and PL\\\\/SQL\\', \\'PL\\\\/SQL development experience with enterprise software systems\\', \\'Experience developing Shell scripts and Python programs to automate tasks\\', \\'Knowledge of BI Solutions (MS Power BI, Tableau, Qlik, Alteryx, etc.)\\'], \\'Responsibilities\\': [\\'As a Data Engineer, you will create analytics reporting and provide data-driven strategic insights, trends, and perspective to help drive transformation for our clients\\', \\'Work on hard problems with smart people\\', \\'Learn and share knowledge across our engineering teams, so we can continue to iterate and improve\\', \\'Write reusable, testable, and efficient code as needed\\', \\'Design and implement of low-latency, high-availability, and performant database systems\\', \\'Work on implementation of data pipelines\\\\/data warehouses\\\\/data marts\\\\/ODSs\\\\/Lakehouses\\', \\'Collaborate and work on integration of data storage solutions\\', \\'Focus on performance tuning, improvement, balancing, usability and automation\\'], \\'Benefits\\': [\\'We offer highly competitive benefits, including medical, dental and vision insurance, a 401(k) plan, tuition reimbursement, and a work culture focused on innovation and creation of lasting value for our clients and employees\\']}\",\"71\":\"{\\'Qualifications\\': [\\'REQUIRED SKILLS (primary and nice-to-have): (Required means that the client only wants to see candidates who have these skills)\\', \\'3-5 years\\\\u2019 experience in SSIS, ETL and ELT processes are required\\', \\'3-5 years\\\\u2019 experience in understanding data models and extracts\\', \\'Understanding in the construct of data cubes\\', \\'Familiarity with Data Lineage and Data Governance\\', \\'exposure to healthcare and systems, preferably Epic or Cerner\\', \\'exposure to Dimensional Insights\\', \\'would also be familiarity working in healthcare IT database systems\\', \\'working with Cerner Millennium data\\', \\'familiarity with Star Schema database design\\', \\'familiarity with python scripting\\', \\\\\"Education Level: Bachelor\\'s Degree (\\\\u00b116 years)\\\\\"], \\'Responsibilities\\': [\\'Develop and maintain data extracts and data models\\', \\'Tools used include SSMS SSAS \\\\/ SSMS SSIS, ETL and ELT processes\\', \\'On-call rotations exist for this team, and would be every 3-4 weeks\\', \\'After an initial Knowledge Transfer period of 2 to 4 weeks at the client site (tentative), work will be performed remotely, with the exception of a major outage\\', \\'troubleshooting Tableau Dashboards\\']}\",\"72\":\"{\\'Qualifications\\': [\\'May require travel\\\\/relocation to unanticipated work locations within the USA\\', \\'Require Master of Science in Computer Science\\\\/Engineering, computer\\\\/management Information Systems\\\\/Technology, or related field and 1 year of experience in the job offered, software engineer\\\\/developer, data architect\\\\/engineer, Hadoop developer\\\\/associate\\\\/consultant, or related field\\'], \\'Responsibilities\\': [\\'Design and develop data pipeline models to extract, transform and load data from heterogenous source systems onto common data repository\\\\/data lake for provisioning data to various downstream teams to help them build their reports and dashboards for various analytics purposes and derive key business insights to enhance the user\\\\/customer satisfaction\\', \\'Use Python, PySpark, Hive QL, Spark, Snowflake, BitBucket, Autosys, Jenkins CI\\\\/CD, Tableau, JuPyter, and ShellScripting to design, develop, extract, load, build, and code data models and frameworks\\', \\'Gather requirements from stakeholders and identify the source data systems that provide transactional data\\', \\'Build reusable framework and code scripts using python, spark, shell and yaml to automate data pipelines on top of distributed Hadoop cluster\\', \\'Perform unit and regression testing to make sure developed data pipeline framework meets data quality and integrity standards\\', \\'Generate key performance indicators metrics from data stored on reporting layer which is built by creating views on top of data objects in data lake and communicate with business teams in regular intervals when any deviations observed in data\\']}\",\"73\":\"{\\'Responsibilities\\': [\\'You will help drive data architecture across many large datasets, perform exploratory data analysis, implement new data pipelines that feed into or from critical data systems at Amazon, and your insights will impact millions of Customers, Vendors and Sellers who communicate with each other for mission-critical use cases every day\\', \\'As a Senior Data Engineer, you will develop new data engineering patterns that leverage new cloud architectures, and will extend or migrate existing data pipelines to the architectures as needed\\', \\'You will also be assisting with integrating the Redshift platform as our primary processing platform to create the curated Amazon\\'], \\'Benefits\\': [\\'Estimated Salary: $20 to $28 per hour based on qualifications\\']}\",\"74\":\"{\\'Qualifications\\': [\\'Scripting skills e.g. Python, R and experience with tools such as Alteryx or similar tool\\', \\'Strong understanding of data and database methodologies as well as hands on Oracle and\\\\/or AWS Cloud experience\\', \\\\\"Bachelor\\'s degree in a relevant quantitative field (e.g. Statistics, Economics, Finance, Business Analytics, Mathematics, Engineering, Computer Science, Information Technology)\\\\\", \\'5+ years of industry experience in business analytics roles (e.g., marketing analytics, product analytics, business insights)\\', \\'5+ years of work experience across broad range of analytics platforms, languages, and tools (SAS, SQL, Spark and Python, Unix, Excel Pivot etc.); hands-on experience using big data platform required\\', \\'Strong understanding of CI\\\\/CD Pipelines in a globally distributed environment using Git, Bit-Bucket, Jenkins, Spinnaker, etc\\', \\'Experience with the entire Software Development Life Cycle (SDLC) including planning, analysis, development and testing of new applications and enhancements to existing application\\'], \\'Responsibilities\\': [\\\\\"The Data Analytics and Reporting (DART) team\\'s mission is to provide high-quality dashboards & insights that executive leadership will use to monitor KPIs, steer the business, and make investment decisions\\\\\", \\'This team will constantly balance the need for speed with the importance of providing accurate results, to enable leadership to make informed decisions with agility & precision\\', \\'The position will be part of a team that will take a loosely-defined idea or problem statement, find & acquire the relevant data, design & build the dashboards, develop the automated workflows, and document the details\\', \\'You will also help to develop frameworks to ensure cross-platform data & logic consistency and optimization\\', \\'They will write code to enrich and transform data sets to produce output that will help to advance the DART teams objectives and impact to the broader organization\\', \\'Perform user acceptance testing and deliver demos to stakeholders by SQL queries or Python scripts\\', \\'Perform data analysis to define \\\\/ support model development including metadata and data dictionary documentation that will enable data analysis and analytical exploration\\', \\'Participate in strategic projects and provide ideas and inputs on ways to leverage quantitative analytics to generate actionable business insights and\\\\/or solutions to influence business strategies and identify opportunities to grow\\', \\'Partners closely with business partners to identify impactful projects, influence key decisions with data, and ensure client satisfaction\\'], \\'Benefits\\': [\\'We offer a competitive total rewards package including base salary determined based on the role, experience, skill set, and location\\', \\'For those in eligible roles, discretionary incentive compensation which may be awarded in recognition of individual achievements and contributions\\', \\'We also offer a range of benefits and programs to meet employee needs, based on eligibility\\', \\'These benefits include comprehensive health care coverage, on-site health and wellness centers, a retirement savings plan, backup childcare, tuition reimbursement, mental health support, financial coaching and more\\']}\",\"75\":\"{\\'Qualifications\\': [\\'We want to bring skills to federal agencies, help them better meet the needs of their users, and close the gap between consumer expectations and government\\', \\'A minimum of four (4) years of professional software development experience\\', \\'AWS experience\\', \\'Understanding of ETL\\\\/ELT processes and tooling\\', \\'Understanding of database technologies - setup\\\\/ maintenance \\\\/ data loads \\\\/ etc\\', \\'(not data modeling)\\', \\'Understand system security\\', \\'Some experience with older file systems \\\\/ file based processes such as MOVEit\\', \\'Some experience with Mulesoft\\', \\'Experience with agile software development practices emphasizing agility, flexibility, and iterative development\\', \\'All work must be conducted within the U.S., excluding U.S. territories\\', \\'Some federal contracts require U.S. citizenship to be eligible for employment\\', \\'You must be legally authorized to work in the U.S now and in the future without sponsorship\\', \\'As a government contractor, you may be required to obtain a public trust security clearance\\'], \\'Responsibilities\\': [\\'Data Engineers are responsible for working with the systems and infrastructure that enable data storage, processing, and analysis\\', \\'They work closely with data scientists and analysts to ensure that data is properly collected, organized, enriched, refined, and available for analysis\\', \\'They are the critical connectors between the teams that maintain existing legacy systems, and the data analysts and data scientists that will use aggregated data for analysis, reporting, and predictive analytics\\', \\'Shipping software that impacts the lives of millions of people\\', \\'Using modern programming languages and frameworks to build scalable services that gracefully integrate with legacy systems\\', \\'Building and working with APIs to support both the digital services we deliver as well as third-party usage\\', \\'Helping us continuously, iteratively improve\\'], \\'Benefits\\': [\\'Company-subsidized Health, Dental, and Vision Insurance\\', \\'Use What You Need Vacation Policy\\', \\'401K with employer match\\', \\'Paid parental leave after one year of service\\', \\'The range of starting pay for this role is $101,570 - $136,994 and information on benefits offered is here\\']}\",\"76\":\"{\\'Qualifications\\': [\\'Ben requires a technical background working with regulated environments, demonstrated Data Management skills, and a self-motivated attitude\\', \\'Experience with various ETL tools in an audited environment\\', \\'Experience working with source control (git)\\', \\'Experience with Data Warehouse technologies\\', \\'Experience with enterprise reporting packages\\', \\'Technical bachelor\\\\u2019s degree or equivalent experience required\\', \\'5+ years of database (SQL) experience required\\', \\'4+ year of ETL experience required\\', \\'3+ years of Python experience required\\', \\'SQL, ETL, Python, Snowflake, Matillion, Tableau, AWS Lambda, AWS S3, AWS SQS, AWS CloudFormation\\'], \\'Responsibilities\\': [\\'Ben\\\\u2019s Data Engineer\\\\/Analyst will be responsible for designing and implementing ETL, Storage, and Reporting solutions\\', \\'Manage data design and the creation of database architecture and data repositories\\', \\'Facilitate Data ETL for customer facing and internal processes\\', \\'Work with departments throughout Ben that are creating data or ingesting data to build and maintain a documented set of information life cycles; serve as a knowledge resource, it must have a clear understanding of our processes and the resulting data\\', \\'Ensure that data projects and milestones are met\\', \\'Follow PMO\\\\/Release\\\\/Change Management policies\\\\/procedures\\', \\'Follow and participate in all IT projects and initiatives to effectively develop and maintain an understanding of existing and new data flows\\', \\'Interface with Project Managers, Developers, and Production support to keep processes, databases, and reports kept up to date with all changes\\', \\'Engage stakeholders to understand and document business requirements, translate them into technical requirements, and then deliver the solution\\', \\'Install processes for auditing data warehouse transfers, storage, and data quality\\', \\'In-depth understanding of Python programming\\\\/objects\\', \\'Work with stakeholders across the company that are responsible for data governance and drive the business to consensus\\', \\'Support release management in deployment efforts\\', \\'Develop data flow diagrams (technical and functional)\\', \\'Single source of truth analysis\\']}\",\"77\":\"{\\'Qualifications\\': [\\'3-5+ years of hands-on experience in multiple modern programming languages such as Python or Java\\', \\'2+ years of experience leveraging DevOps principals such as CI \\\\/ CD and using tools like Git, Jenkins, etc\\', \\'Strong working knowledge of Data Engineering and associated tools and technologies like Apache, Spark, Databricks, Python, SQL, and data lake concepts\\', \\'Experience with Argo Informatica\\', \\'Working experience with AWS services such as Lambda, RDS, ECS, DynamoDB, API Gateway, S3, etc\\', \\'An equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above\\', \\'Experience working with other public cloud technologies AWS, GCP\\', \\'Hands on experience with data virtualization technologies : CIS (Tibco), Denodo, or SQL 2019 Virtualization\\', \\'Experience building test automation to ensure data quality and accuracy\\', \\'Proficient in data modeling (specifically RDS, PostgreSQL, Oracle)\\'], \\'Responsibilities\\': [\\'Design and build a modern data warehouse in the cloud\\', \\'Enhance data collection procedures to build analytic systems\\', \\'Work as an Agile partner and participate in back log refinement, sprint planning, review, and retrospective\\', \\'Coach and mentor junior engineers in engineering techniques, processes, and new technologies; enable others to succeed\\', \\'Contribute to overall system design, architecture, security, scalability, reliability, and performance of applications\\', \\'Support the build and deployment pipeline and when necessary, both diagnose and solve production support issues\\', \\'Work with Product, Design, and QA to deliver world-class digital experiences\\']}\",\"78\":\"{\\'Qualifications\\': [\\'5+ years of cloud data engineering experience\\', \\'Strong experience designing and developing cloud ELT and data pipelines with AWS, Python, SQL, Athena\\', \\'Must also have experience with the following technology: CLI, Glue, RDS, Pandas, KMS, S3\\', \\'Experience with developing and operating CI\\\\/CD pipelines and other DataOps fundamentals\\', \\'Strong understanding of data modeling, data pipelines, data lakes\\', \\'Excellent foundation of consulting skills: analytical, written and verbal communication, and presentation skills\\', \\'Demonstrated ability to identify business and technical impacts of user requirements and incorporate them into the project schedule\\\\\\\\u202f\\', \\\\\"Bachelor\\'s degree in Computer Science, Engineering, or a related field\\\\\"], \\'Responsibilities\\': [\\'You will improve risk modeling that will allow the client to better understand the integrity of their pipelines for improved customer safety\\'], \\'Benefits\\': [\\'Compensation range for senior data engineer: $125,000 - $175,000 annually\\', \\'Compensation range for lead data engineer: $145,000 - $195,000 annually\\', \\'Career Development \\\\u2013 A built-in program from day 1, providing a mentor and individually-directed training opportunities, plus access to leaders across the company\\', \\'PTO, Paid Holidays, & Voluntary Leave \\\\u2013 Worry-free time off to recharge and pursue your personal goals\\', \\'Community & Committees \\\\u2013 As part of our \\\\u201cCulture of We,\\\\u201d Logic20\\\\/20 invests in providing many social, interest, and learning opportunities\\', \\'Recognition \\\\u2013 From peer recognition, swag, and the chance to win a once-in-a-lifetime type of award, we make your Logic20\\\\/20 journey stand out\\', \\'Referral Programs & Bonuses \\\\u2013 Employee, project, and sales referral programs with paid incentives\\']}\",\"79\":\"{\\'Qualifications\\': [\\'The ideal candidate will have a proven track record in building robust data pipelines and processes, along with a strong background in AWS, Python, and data consolidation techniques\\', \\'Bachelors or masters degree in computer science, engineering, or a related field\\', \\'Proven experience in a similar role, preferably in a fast-growing SaaS company\\', \\'Strong expertise in building scalable data pipelines using AWS services such as S3, Glue, Redshift, and Lambda\\', \\'Proficiency in Python programming and related frameworks\\\\/libraries for data engineering tasks\\', \\'Solid understanding of data consolidation and validation techniques, including data cleansing, deduplication, and data integrity checks\\', \\'Excellent communication and interpersonal skills, with the ability to collaborate effectively with both technical and non-technical stakeholders\\', \\'Strong organizational skills and the ability to manage multiple priorities in a fast-paced environment\\', \\'Demonstrated leadership experience, with the ability to mentor and develop a high-performing team\\', \\'Analytical mindset with a focus on data-driven decision-making\\', \\'Experience managing budgets and reporting on financial and performance metrics\\', \\'Strong business acumen and the ability to align data engineering strategies with company goals and objectives\\'], \\'Responsibilities\\': [\\'Reporting directly to the CTO, this strategic role requires excellent communication and organization skills, as well as the ability to lead and mentor a team\\', \\'The Vice President of Data Engineering will play a crucial role in driving our companys growth and scalability by managing budgets, reporting on ROI, and analyzing team and individual metrics\\', \\'Lead the design, development, implementation, and maintenance of scalable data pipelines and processes to support our rapidly growing SaaS platform\\', \\'Oversee the consolidation, validation, and transformation of data from various sources into a unified and reliable format\\', \\'Collaborate closely with cross-functional teams to understand business requirements and provide data solutions that meet their needs\\', \\'Manage and mentor a team of data engineers, fostering a collaborative and high-performance work environment\\', \\'Develop and enforce best practices for data engineering, including coding standards, documentation, and quality assurance processes\\', \\'Stay up-to-date with industry trends, emerging technologies, and advancements in data engineering techniques, and assess their potential impact on our business\\', \\'Effectively communicate complex technical concepts to non-technical stakeholders, including executives and other department heads\\', \\'Take ownership of the data engineering budget, including resource allocation, vendor management, and cost optimization\\', \\'Analyze and report on the ROI of data engineering initiatives, identifying areas for improvement and recommending actionable strategies\\', \\'Track team and individual performance metrics, providing constructive feedback and implementing performance improvement plans as necessary\\']}\",\"80\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s degree in computer science, management information systems, information technology, statistics, mathematics, or related discipline\\\\\", \\'Seven years of experience in maintaining business intelligence, data warehouse solutions, or ETL in a Run or Production environment\\', \\'Six years of experience troubleshooting ETL load related issues (SSIS or Data Solutions)\\', \\'Six years of experience with ETL development and maintenance experience in a data warehouse environment\\', \\'Experience with systems engineering (hardware \\\\/ software) capacity\\', \\'Equivalent Education and\\\\/or Experience\\', \\'Five (5) years of experience with healthcare data management in a health plan or managed care organization may be considered in lieu of a degree\\', \\'Epic Cogito or Clarity, SAP Business Objects, Tibco Composite, Microsoft Certified Solutions Engineer (MCSE), Oracle Certified Professional (OCP), etc.)\\', \\'Proficiency with ETL tool Build or Run activities\\', \\'Ability to create reports and\\\\/or build virtual data environments\\', \\'Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy\\', \\'Demonstrated critical thinking and troubleshooting skills accompanied by a high level of detail\\', \\'Demonstrated ability to plan and manage multiple processes and projects simultaneously\\', \\'High level of attention to detail\\', \\'Strong verbal and written communication skills\\', \\'Demonstrated ability to collaborate effectively and work as part of a team\\', \\'Independent worker and self-starter, having the ability to provide internal motivation and drive\\'], \\'Responsibilities\\': [\\\\\"The Parkland Community Health Plan\\'s (PCHP\\'s) Data Engineer is responsible for maintaining the data systems including business intelligence, ETL, and supporting backup strategies in order to provide PCHP with secure, dependable, and accurate data including data transfer, data integrity, and data storage responsibilities\\\\\", \\\\\"The Data Engineer will collaborate with Database Administrators, server team, storage team, and other teams to plan maintenance activities and with PHCP\\'s analytics team for report or universe deployments\\\\\", \\'The Data Engineer will also be involved in dashboard and report development activities\\', \\'Implements and maintains high-value business intelligence environments\\', \\'Has a strong understanding of all the tools within the environment, regardless of vendor, and quickly and efficiently triages, troubleshoots, and restores services during outages or service degradation\\', \\'Responsible for being on-call for Business Intelligence and ETL cycles\\', \\'Works with the Database Analyst and storage teams to ensure proper backups are taken, test back-ups periodically, and ensures that the system can be restored in the time of a disaster\\', \\'Proactively identifies areas for improvement in our Business Intelligence environment\\', \\'Documents all routine processes and cross-trains other team members\\', \\'Improves function, speed, and accuracy of data distribution methods\\', \\'Develops automated reports and dashboards\\', \\'Identifies ways to improve work processes and improve customer satisfaction\\', \\'Makes recommendations to supervisor, implements, and monitors results as appropriate in support of the overall goals of PCHP\\', \\'Stays abreast of the latest developments, advancements, and trends in the field by attending seminars\\\\/workshops, reading professional journals, actively participating in professional organizations, and\\\\/or maintaining certification or licensure\\', \\'Maintains knowledge of applicable rules, regulations, policies, laws, and guidelines that impact the area\\', \\'Develops effective internal controls designed to promote adherence with applicable laws, accreditation agency requirements, and customer requirements\\', \\'Seeks advice and guidance as needed to ensure proper understanding\\']}\",\"81\":\"{\\'Qualifications\\': [\\'8 years of software solution development using Agile, and DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions\\', \\'2 years of cloud development and data lake experience including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Data Lake, Azure Power Apps, and Power BI\\', \\'5 years of experience with Python, PySpark, Spark, Unix, SQL\\', \\'Data Platforms: Teradata, Cassandra, MongoDB, Oracle, SQL Server, ADLS, Snowflake\\', \\'Tools: DataStage, Informatica EDC\\\\/Axon, BigID\\', \\'Expertise with the Azure Technology stack for data management, data ingestion, capture, processing, curation and creating consumption layers\\', \\\\\"Bachelor\\'s degree in Computer Science\\\\/Engineering or related field\\\\\"], \\'Responsibilities\\': [\\'Responsible for migrating an on-premises ETL(IBM DataStage) to Azure Databricks\\', \\'Implement data privacy and data engineering solutions using Azure products and services: (Azure Data Lake Storage, Azure Data Factory, Azure Functions, Event Hub, Azure Stream Analytics, Azure Databricks, etc.) and traditional data warehouse tools\\', \\'Perform multiple aspects involved in the development lifecycle design, cloud engineering (infrastructure, network, security, and administration)\\', \\'Implement batch and streaming data pipelines using cloud technologies\\', \\'Develop and support data privacy and governance related frameworks and help other teams implement them for compliance\\']}\",\"82\":\"{\\'Qualifications\\': [\\'Must have a valid Linkedin profile..\\', \\'Must be based in Dallas or Raleigh- (travel on-site one week per quarter)\\', \\'10+ years in Database Development\\', \\'Experience with Informatica or similar ETL tool\\', \\'Hands-on experience with AWS\\', \\'Experience with Python or similar scripting language\\']}\",\"83\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s degree in Computer Science, Engineering, or related field\\\\\", \\'At least 3 years of experience in big data engineering or related field\\', \\'Strong understanding of data processing fundamentals, including ETL pipelines, data warehousing, and data modeling\\', \\'Proficient in at least one programming language, such as Python or Java\\', \\'Experience with SQL and NoSQL databases\\', \\'Familiarity with distributed computing frameworks, such as Hadoop or Spark\\', \\'Understanding of data security and access control best practices\\', \\'Strong problem-solving skills and ability to work independently and in a team environment\\', \\'Excellent verbal and written communication skills\\', \\'Self-motivated with a strong desire to learn and stay up-to-date with new technologies in the field\\'], \\'Responsibilities\\': [\\'Develop and manage data pipelines, ensuring the smooth flow of data from various sources to our data warehouse\\', \\'Monitor and optimize data processing infrastructure, ensuring fast and reliable ETL pipelines\\', \\'Contribute to the design and implementation of new data-driven solutions, using cutting-edge machine learning and artificial intelligence techniques\\', \\'Collaborate with other members of the engineering team, sharing knowledge and best practices to continuously improve our data processing capabilities\\', \\'Build and maintain data models and ensure data accuracy and consistency\\', \\'Implement and manage data security measures, including backups and access controls\\', \\'Participate in code reviews, providing constructive feedback to ensure code quality\\'], \\'Benefits\\': [\\'Great growth potential at a fast-growing startup, we want you to grow with us!\\', \\'Company-provided laptop and necessary hardware to ensure your setup for success\\', \\'Comprehensive health, dental and vision coverage\\', \\'Company-sponsored 401(k)\\', \\'Inclusive and motivating work culture that values team collaboration\\']}\",\"84\":\"{\\'Qualifications\\': [\\'Expertise in batch and file-based integrations Experience of performing ETL ELT processes from multiple sources\\', \\'Demonstrated application of skills and knowledge across digital technologies (AWS, Cloud, Data, App Architectures (APIs, Event Streaming), Automation) and with Agile delivery methods (Scrum, Kanban, DevOps)\\', \\'Strong Database and SQL Sk ills Required\\', \\'Strong Object-oriented programming Skills Required\\', \\'Strong knowledge in backend frameworks like Spring boot is Required\\', \\'Experience with some backend languages (Java, Python) Experience Required\\', \\'Experience with DevSecOps tools for CI CD Required\\', \\'Experience with Source Code management tools like Git Required\\', \\'Excellent communication skills Required\\'], \\'Responsibilities\\': [\\'(Data Engineer) - 1 open position\\']}\",\"85\":\"{\\'Qualifications\\': [\\'Must Have Firmwide x 2 OR Data x 3\\', \\'Job Description Design and develop data ingest and transform processes Develop data models to provide standardized reporting solutions to the firm Develop automation governance and reporting solutions to provide firm and regulatory mandated controls Work as part of a global team using Agile software methodologies Partner with Marcus risk product acquisition and servicing teams Use Marcus data to drive change throughout the Marcus business Minimum 3 years of relevant professional experience Experience with SQL and relational databases Self-starter motivated and good communication skills Strong sense of ownership and driven to manage tasks to completion\\']}\",\"86\":\"{\\'Qualifications\\': [\\\\\"Bachelor\\'s Degree in Computer Science, Information Systems, or a related discipline\\\\\", \\'At least 3 years of experience as a Data Engineer or related role\\', \\'Strong experience with database environments and (T) SQL\\', \\'Strong programming skills in languages such as Python, PySpark, and\\\\/or Java\\', \\'Experience with ETL tools and processes, such as SSIS or Azure Data Factory\\', \\'Proficiency in data warehousing and data modeling\\', \\'Solid understanding of data management principles, data quality, and data governance\\', \\'Strong problem-solving skills and attention to detail\\', \\'Experience with cloud computing platforms such as Google Cloud Platform, Azure, or AWS is a strong plus\\', \\'Functional Expertise: Possesses the skills and knowledge to perform essential functions efficiently and effectively\\'], \\'Responsibilities\\': [\\'As a Data Engineer, you will be responsible for developing and maintaining the data architecture and infrastructure that support our data-driven applications and processes\\', \\'You will work closely with our data engineering team, IT Ops team, and business partners to ensure that data is appropriately collected, stored, processed, and analyzed\\', \\'Develop and maintain scalable and efficient data pipelines, data warehouses, and databases\\', \\'Build large-scale batch and real-time data pipelines using state-of-the-art cloud technologies, such as Google Cloud, Azure, or AWS\\', \\'Build and manage data infrastructure, including data ingestion, processing, and storage\\', \\'Collaborate with cross-functional teams to ensure the quality and integrity of the data\\', \\'Implement data governance, security, and privacy measures\\', \\'Continuously optimize the data infrastructure to improve performance, reliability, and scalability\\', \\'Conduct data modeling, profiling, and validation to ensure data accuracy and consistency\\', \\'Develop and maintain documentation on the data infrastructure, including data flow diagrams, architecture, and technical specifications\\', \\'Participate in code reviews and collaborate with other data engineers to maintain code quality and best practices\\', \\'Takes ownership and does not misrepresent information\\', \\'Supports colleagues and team efforts\\', \\'Development: Takes an active role in self-development, seeking to grow job-related knowledge and skills\\', \\'Empowers and challenges team members to reach their full potential\\', \\'Analysis and Decision Making: Uses all available resources to make good decisions\\', \\'Managing Change and Continuous Improvement: Demonstrates an entrepreneurial mindset towards change\\']}\",\"87\":\"{\\'Qualifications\\': [\\'Strong problem solver with excellent communication skills\\', \\'Have a growth mindset with a desire to learn and embrace challenges\\', \\'Innovative and passionate about your work\\', \\'\\\\\"Self-starter\\\\\" attitude and the ability to make decisions independently\\', \\'Minimum of 3 years of relevant experience in database design and development\\', \\'Minimum of 2 years of relevant experience in working with Azure PaaS databases\\', \\'Minimum of 1 year of relevant experience working with Azure Data Lakes Gen 2\\', \\'Working knowledge of Azure Synapse\\', \\\\\"Bachelor\\'s in Computer Science, related field, or equivalent work experience\\\\\", \\'Technical Pre-screening test will be required for all candidates\\', \\'The chance to be part of a technology team for a thriving organization that prioritizes accountability, respect, and operational excellence!\\', \\'The opportunity to join a thriving, highly visible organization during its technology transformation!\\'], \\'Responsibilities\\': [\\'You will provide data capabilities and build out a common data model that supports 360 view of our prospect\\\\/resident powered by Azure SQL, Synapse data warehouse, and Microsoft Customer Insights\\', \\'100% hands-on development - develop and unit test database code, including but not limited to T-SQL, stored procedures, functions and views\\', \\'Create and maintain database structures\\', \\'As part of the Scrum team, you will work with BAs, Scrum Master, Leads and engineers to provide data support to our products and build creative solutions and features to move our product roadmap forward\\', \\'Participate in the design of databases, using first, second or third normalized form as needed to support business requirements\\', \\\\\"Create and deploy ADF pipelines, adhering to Greystar\\'s standards and documented best practices\\\\\", \\'Perform analysis of complex data and document findings\\', \\'Prepare data for prescriptive and predictive modeling\\', \\'Combine raw data from different external sources\\', \\'Collaborate with data scientists and architects\\', \\'Play a direct role in the maintenance, technical support, documentation, and administration of databases\\'], \\'Benefits\\': [\\'Competitive pay, benefits, and overall compensation packages\\', \\'Depending on the position offered, regular full-time and part-time team members may be eligible to participate in a bonus program in addition to their salary\\', \\'Team members may also participate in the 401k plan, once eligible\\', \\'Regular, full-time team members are offered a range of medical, financial, and other benefits from which to choose\\', \\'For Union and Prevailing Wage roles compensation and benefits may vary from the listed information above due to Collective Bargaining Agreements and\\\\/or local governing authority\\']}\",\"88\":\"{\\'Qualifications\\': [\\'If you are interested in being part of a collaborative team, working on innovative and challenging projects, then this is the job for you\\', \\'To qualify for this position, you should have more than 5 years of experience with data analysis, data analytics, Excel, HubSpot, CRM, and Redis\\', \\'You should also have excellent problem-solving and communication skills\\', \\'Applicants must be authorized to work in the U.S\\'], \\'Responsibilities\\': [\\'As a Data Engineer, you will be responsible for developing, implementing, and managing data acquisition and data applied models\\', \\'You will also be involved in data analysis and data analytics\\', \\'You will be expected to collaborate with other teams to ensure projects are completed on time and with the highest quality\\'], \\'Benefits\\': [\\'We also provide competitive compensation packages, flexible hours, and generous benefits\\', \\'This position pays between 90000 - 140000 annually\\', \\'In addition to a competitive salary, we offer vacation\\\\/PTO, medical, dental, vision, bonus, and 401k\\']}\"},\"job_naics_name\":{\"0\":\"Employment Placement Agencies\",\"1\":null,\"2\":null,\"3\":null,\"4\":null,\"5\":null,\"6\":null,\"7\":null,\"8\":\"Computer Systems Design Services\",\"9\":null,\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":\"Custom Computer Programming Services\",\"15\":\"Education\",\"16\":null,\"17\":\"Custom Computer Programming Services\",\"18\":null,\"19\":null,\"20\":null,\"21\":\"Portfolio Management\",\"22\":null,\"23\":null,\"24\":null,\"25\":\"Employment Placement Agencies\",\"26\":null,\"27\":\"All Other General Merchandise Stores\",\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":null,\"33\":\"Scheduled Passenger Air Transportation\",\"34\":null,\"35\":\"Supermarkets and Other Grocery (except Convenience) Stores\",\"36\":null,\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":\"Custom Computer Programming Services\",\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":null,\"47\":\"Marketing Consulting Services\",\"48\":null,\"49\":null,\"50\":\"Pharmaceutical Preparation Manufacturing\",\"51\":null,\"52\":\"Other Basic Organic Chemical Manufacturing\",\"53\":null,\"54\":\"Custom Computer Programming Services\",\"55\":null,\"56\":\"Custom Computer Programming Services\",\"57\":null,\"58\":\"Software Publishers\",\"59\":\"Supermarkets and Other Grocery (except Convenience) Stores\",\"60\":\"Internet Publishing and Broadcasting and Web Search Portals\",\"61\":\"Data Processing, Hosting, and Related Services\",\"62\":\"Management Consulting Services\",\"63\":null,\"64\":null,\"65\":\"All Other General Merchandise Stores\",\"66\":null,\"67\":null,\"68\":null,\"69\":\"All Other General Merchandise Stores\",\"70\":\"Marketing Consulting Services\",\"71\":null,\"72\":null,\"73\":null,\"74\":\"Portfolio Management\",\"75\":null,\"76\":null,\"77\":\"Supermarkets and Other Grocery (except Convenience) Stores\",\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":\"Education\",\"83\":null,\"84\":null,\"85\":null,\"86\":null,\"87\":\"Lessors of Residential Buildings and Dwellings\",\"88\":null}}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
